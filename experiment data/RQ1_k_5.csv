requirement,runnable_code,result,success_rate_label1,success_rate_label2
"I'd like to know the latest news, user reviews, and global reach rates for the first three games so I can gauge their popularity and community activity.","import requests

# Constants
API_KEY = ""e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa""
API_HOST = ""steam2.p.rapidapi.com""
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": API_HOST
}

# Function to get news
def get_app_news(app_id, limit=10, content_length=300):
    url = f""https://{API_HOST}/newsForApp/{app_id}/limit/{limit}/{content_length}""
    response = requests.get(url, headers=HEADERS)
    return response.json() if response.status_code == 200 else {""error"": f""Failed to retrieve news for app ID {app_id}""}

# Function to get reviews
def get_app_reviews(app_id, limit=40, cursor=""*""):
    url = f""https://{API_HOST}/appReviews/{app_id}/limit/{limit}/{cursor}""
    response = requests.get(url, headers=HEADERS)
    return response.json() if response.status_code == 200 else {""error"": f""Failed to retrieve reviews for app ID {app_id}""}

# Function to get global achievement percentages
def get_global_achievement_percentages(app_id):
    url = f""https://{API_HOST}/globalAchievementPercentagesForApp/{app_id}""
    response = requests.get(url, headers=HEADERS)
    return response.json() if response.status_code == 200 else {""error"": f""Failed to retrieve achievements for app ID {app_id}""}

# Function to fetch data for multiple apps
def fetch_game_data(app_ids):
    for app_id in app_ids:
        print(f""\n{'=' * 20} App ID: {app_id} {'=' * 20}\n"")

        # Fetch and display news data
        news_data = get_app_news(app_id)
        news_items = news_data.get('appnews', {}).get('newsitems', [])
        if news_items:
            print(""Latest News:"")
            for news in news_items[:3]:  # Show top 3 news items for brevity
                print(f"" - {news.get('title')}: {news.get('contents')[:200]}..."")
        else:
            print(""No news available or failed to fetch news.\n"")

        # Fetch and display review data
        reviews_data = get_app_reviews(app_id)
        reviews = reviews_data.get('reviews', [])
        if reviews:
            print(""\nUser Reviews:"")
            for review in reviews[:3]:  # Show top 3 reviews for brevity
                print(f"" - Review: {review.get('review')[:200]}... | Rating: {review.get('voted_up')}"")
        else:
            print(""No reviews available or failed to fetch reviews.\n"")

        # Fetch and display achievement data
        achievements_data = get_global_achievement_percentages(app_id)
        achievements = achievements_data.get('achievementpercentages', {}).get('achievements', [])
        if achievements:
            print(""\nGlobal Achievement Percentages:"")
            for achievement in achievements[:3]:  # Show top 3 achievements for brevity
                print(f"" - {achievement.get('name')}: {achievement.get('percent')}%"")
        else:
            print(""No achievements data available or failed to fetch achievements.\n"")

if __name__ == '__main__':
    # List of app IDs for the top three games (example: CS:GO, Team Fortress 2, Dota 2)
    app_ids = [730, 440, 570]
    fetch_game_data(app_ids)
","==================== App ID: 730 ====================

Latest News:
 - Release Notes for 10/24/2024: Fixed a bug where grenades were generating headshot sounds. Fixed a bug where grenades were not bouncing off certain dynamic props. Fixed a bug where Copenhagen 2024 winners were not showing up in the...
 - Release Notes for 10/23/2024: Vertigo Fixed a bad collision interaction between grenades and a prop. Fixed a wall bang exploit. Mills Updated to the latest version from the Community Workshop (Update Notes). Added localization str...
 - Release Notes for 10/15/2024: Fixed mixed resolution rendering from erroneously resetting during map changes. Fixed a known crash when compiling lighting in Counter-Strike 2 Workshop Tools. Fixed detail normal UV scaling. Fixed se...

User Reviews:
 - Review: A year has passed. Armory update is a cash grab with no real gameplay value. VAC nowhere to be found, no new maps, the CSGO community has been wiped off the face of the earth. Disgusting company... | Rating: False
 - Review: Counter Strike was once a great game, still good, but too bad its full of cheaters and scammers.

Anti cheat system is a joke. This game would be amazing, if it was free of cheaters and scammers.... | Rating: True
 - Review: 1 Year since release.  Hardly any anti cheat improvements, removed gamemodes, removed live games, removed OverWatch system, no seasons as promised, and now we get the barebones BattlePass with no miss... | Rating: False

Global Achievement Percentages:
 - PLAY_CS2: 26.899999618530273%

==================== App ID: 440 ====================

Latest News:
 - Sacred Scouts 6v6 Cup: {STEAM_CLAN_IMAGE}/554111/cc86e5120ef0bf75bd90c4074018fc55dc7144f3.png Sacred Scouts is back for its second season of 6v6 Scout-only chaos! Sign-ups will open on October 31st and close on November 21s...
 - Sacred Scouts 6v6 Cup: <a href=""https://www.cltf2.com/"" target=""_blank""> </a> (Image credit: <a href=""https://steamcommunity.com/id/metalcloud/"" target=""_blank"">MetalCloud</a>) Sacred Scouts is back for its second season of...
 - Team Fortress 2 Update Released: An update to Team Fortress 2 has been released. The update will be applied automatically when you restart Team Fortress 2. The major changes include: Reverted BLU Scout pants fix because it conflicts ...

User Reviews:
 - Review: Team Fortress 2 is one of my favorite games and hat simulators of all time. It's not really much of a secret, given how many hours I've gotten in it plus the fact I've been playing it for over 10 year... | Rating: True
 - Review: ---{ Graphics }---
ðŸ”² Awful
ðŸ”² Bad
ðŸ”² Will do
âœ… Decent (Nothing special)
ðŸ”² Stunning
ðŸ”² Visual Masterpiece

---{ Gameplay }---
ðŸ”² Bad
ðŸ”² Okay
âœ… Nothing special
ðŸ”² Good
ðŸ”² Very good
ðŸ”² Try not to get addicted

-... | Rating: True
 - Review: They call this ""Team"" Fortress 2, but don't let this deceive you. There is no team. You're teammates are to busy playing with each others dicks to actual play the fucking video game while you get infi... | Rating: True

Global Achievement Percentages:
 - TF_SCOUT_LONG_DISTANCE_RUNNER: 51.099998474121094%
 - TF_HEAVY_DAMAGE_TAKEN: 41.400001525878906%
 - TF_GET_CONSECUTIVEKILLS_NODEATHS: 37.70000076293945%

==================== App ID: 570 ====================

Latest News:
 - 7.37d Gameplay Patch: Patch 7.37d is out now. You can check out the patchnotes here....
 - The International Grand Champions: {STEAM_CLAN_IMAGE}/3703047/5d614dc7454bfde1701834ffc951eb194a3a55af.jpgCongratulations to Team Liquid, Grand Champions at The International 2024. Dropping only three games throughout their championshi...
 - The International is Here: {STEAM_CLAN_IMAGE}/3703047/19deca390e7d27a5d4f6237fea74d1034cd0a29c.jpg For five straight days, sixteen teams have slugged their way through the Playoffs, and eight teams have earned a coveted spot at...
No reviews available or failed to fetch reviews.
",1,1
I want to filter out free games for a specific platform based on multiple tags and sort by release date in order to find the three most recently released games.,"import requests

# Constants
API_KEY = ""e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa""
API_HOST = ""free-to-play-games-database.p.rapidapi.com""
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": API_HOST
}

# Function to filter games by platform and tags, sorted by release date
def get_filtered_sorted_games(platform, tags, top_n=3):
    """"""
    Retrieve games filtered by platform and tags, and sort by release date.
    Only the top_n most recent games are returned.
    """"""
    url = f""https://{API_HOST}/api/games""
    querystring = {""platform"": platform, ""sort-by"": ""release-date""}

    response = requests.get(url, headers=HEADERS, params=querystring)

    # Error handling for API response
    if response.status_code != 200:
        print(f""Error fetching games: {response.status_code}"")
        print(""Response:"", response.json())
        return []

    # Check API response data structure
    games = response.json()
    print(""Total games retrieved:"", len(games))  # Debug: Check the number of games returned

    # Filter games by tags (case insensitive)
    filtered_games = [
        game for game in games
        if all(tag.lower() in game.get('genre', '').lower() for tag in tags)
    ]

    # Check filtered results
    if not filtered_games:
        print(f""No games found for platform '{platform}' with tags {tags}. Please check tags or platform."")
        return []

    # Sort and retrieve the top_n most recent games
    sorted_games = sorted(filtered_games, key=lambda x: x.get(""release_date"", """"), reverse=True)[:top_n]

    return sorted_games

# Main execution
if __name__ == '__main__':
    # Define the platform and tags to filter
    platform = ""pc""  # Example platform
    tags = [""3d"", ""fantasy""]  # Example tags; adjust based on requirements

    # Retrieve and display the top 3 most recent games
    recent_games = get_filtered_sorted_games(platform, tags, top_n=3)

    if recent_games:
        print(f""\nTop {len(recent_games)} recent games for platform '{platform}' with tags {tags}:"")
        for game in recent_games:
            print(f"" - {game.get('title')}: Released on {game.get('release_date')}"")
    else:
        print(""No games found that match the criteria."")
","Top 0 recent games for platform 'pc' with tags ['3d', 'fantasy']:",1,1
"I would like to learn about the first to third evolutionary paths and conditions of Pokemon, including the resources required to strengthen and the number of steps taken. With these three APIs, you have full access to all the information of Pokemon from evolution to enhancement.","import requests

# Constants
API_KEY = ""e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa""
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""pokemon-go1.p.rapidapi.com""
}


# Function to fetch Pokemon evolution data
def pokemon_evolutions():
    url = ""https://pokemon-go1.p.rapidapi.com/pokemon_evolutions.json""
    response = requests.get(url, headers=HEADERS)
    return response.json() if response.status_code == 200 else None


# Function to fetch Pokemon buddy distances
def pokemon_buddy_distances():
    url = ""https://pokemon-go1.p.rapidapi.com/pokemon_buddy_distances.json""
    response = requests.get(url, headers=HEADERS)
    return response.json() if response.status_code == 200 else None


# Function to fetch Pokemon power-up requirements
def pokemon_power_up_requirements():
    url = ""https://pokemon-go1.p.rapidapi.com/pokemon_powerup_requirements.json""
    response = requests.get(url, headers=HEADERS)
    return response.json() if response.status_code == 200 else None


# Main function to display evolution, buddy distance, and power-up requirements
def display_pokemon_evolution_and_requirements(pokemon_name):
    # Fetch all data from APIs
    evolutions_data = pokemon_evolutions()
    buddy_distances_data = pokemon_buddy_distances()
    power_up_data = pokemon_power_up_requirements()

    if not evolutions_data or not buddy_distances_data or not power_up_data:
        print(""Error fetching data from one or more APIs."")
        return

    # Step 1: Find evolution path
    print(f""\nEvolution Path for {pokemon_name.capitalize()}:"")
    for pokemon in evolutions_data:
        if pokemon['pokemon_name'].lower() == pokemon_name.lower():
            evolutions = pokemon['evolutions'][:3]  # Limit to first 3 evolutions
            base_name = pokemon['pokemon_name']
            print(f""{base_name} (Base Form)"")
            for i, evo in enumerate(evolutions, 1):
                evo_name = evo['pokemon_name']
                candies_required = evo.get('candy_required', 'N/A')
                print(f""  -> Stage {i}: {evo_name} (Requires {candies_required} Candies)"")
            break
    else:
        print(""No evolution data found for this Pokemon."")
        return

    # Step 2: Find buddy distance for each form, limiting to the first 3
    print(""\nBuddy Distance Requirements:"")
    count = 0
    for buddy_id, buddy_info_list in buddy_distances_data.items():
        for buddy_info in buddy_info_list:
            if buddy_info['pokemon_name'].lower() == pokemon_name.lower():
                distance = buddy_info.get('distance', 'N/A')
                form = buddy_info.get('form', 'N/A')
                print(f""{buddy_info['pokemon_name']} (Form: {form}) - Distance: {distance} km"")
                count += 1
                if count >= 3:  # Limit output to the first 3 entries
                    break
        if count >= 3:
            break

    # Step 3: Display power-up requirements with data structure check
    print(""\nPower-Up Requirements:"")
    if isinstance(power_up_data, list):
        for requirement in power_up_data:
            if isinstance(requirement, dict) and requirement.get('pokemon_name', '').lower() == pokemon_name.lower():
                current_level = requirement.get('current_level')
                level_after = requirement.get('level_after_powering')
                candy = requirement.get('candy_to_upgrade', 'N/A')
                stardust = requirement.get('stardust_to_upgrade', 'N/A')
                xl_candy = requirement.get('xl_candy_to_upgrade', 'N/A')
                print(
                    f""Level {current_level} -> {level_after}: Candy = {candy}, Stardust = {stardust}, XL Candy = {xl_candy}"")
    else:
        print(""Power-up data is not in the expected format."")


# Main execution
if __name__ == '__main__':
    # Example Pokemon name
    pokemon_name = ""Pikachu""

    # Display evolution path, buddy distance, and power-up requirements
    display_pokemon_evolution_and_requirements(pokemon_name)
","Evolution Path for Pikachu:
Pikachu (Base Form)
  -> Stage 1: Raichu (Requires 50 Candies)

Buddy Distance Requirements:
Pikachu (Form: Adventure_hat_2020) - Distance: 1 km
Pikachu (Form: Copy_2019) - Distance: 1 km
Pikachu (Form: Costume_2020) - Distance: 1 km

Power-Up Requirements:
Power-up data is not in the expected format.",1,1
I'd like to filter out three games by platform and specific tags and generate a list of games that match the criteria.,"import requests

# Constants for API
API_KEY = ""e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa""
API_HOST = ""free-to-play-games-database.p.rapidapi.com""
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": API_HOST
}


# Function to filter games by a single tag and platform
def filter_games_by_single_tag_and_platform(tag, platform=None, limit=3):
    """"""
    Retrieve a list of games filtered by a single tag and platform,
    limiting to a specified number of results.
    """"""
    url = f""https://{API_HOST}/api/filter""
    querystring = {""tag"": tag}
    if platform:
        querystring[""platform""] = platform

    response = requests.get(url, headers=HEADERS, params=querystring)

    # Check if request was successful
    if response.status_code != 200:
        print(f""Error fetching games with tag '{tag}' and platform '{platform}': {response.status_code}"")
        print(""Response Content:"", response.text)  # Debugging output for more details
        return []

    # Return only the top 'limit' number of games
    games = response.json()
    return games[:limit]  # Limit the number of results


# Test different single tags
if __name__ == '__main__':
    print(""\nTesting single-tag search with different tags and platform...\n"")
    test_tags = [""3d"", ""mmorpg"", ""fantasy"", ""pvp""]  # Example single tags
    platform = ""pc""  # Example platform

    for tag in test_tags:
        print(f""\nTesting with tag '{tag}' on platform '{platform}':"")
        games = filter_games_by_single_tag_and_platform(tag, platform)

        # Display results if any games are found
        if games:
            print(f""Top {len(games)} games with tag '{tag}':"")
            for i, game in enumerate(games, 1):
                print(f""{i}. {game.get('title')} - Genre: {game.get('genre')} - Platform: {game.get('platform')}"")
        else:
            print(f""No games found with tag '{tag}' on platform '{platform}'."")
","Testing single-tag search with different tags and platform...


Testing with tag '3d' on platform 'pc':
Top 3 games with tag '3d':
1. Dauntless - Genre: MMORPG - Platform: PC (Windows)
2. World of Tanks - Genre: Shooter - Platform: PC (Windows)
3. Warframe - Genre: Shooter - Platform: PC (Windows)

Testing with tag 'mmorpg' on platform 'pc':
Top 3 games with tag 'mmorpg':
1. Dauntless - Genre: MMORPG - Platform: PC (Windows)
2. Blade and Soul - Genre: MMORPG - Platform: PC (Windows)
3. Trove - Genre: MMORPG - Platform: PC (Windows)

Testing with tag 'fantasy' on platform 'pc':
Top 3 games with tag 'fantasy':
1. Dauntless - Genre: MMORPG - Platform: PC (Windows)
2. Blade and Soul - Genre: MMORPG - Platform: PC (Windows)
3. Trove - Genre: MMORPG - Platform: PC (Windows)

Testing with tag 'pvp' on platform 'pc':
Top 3 games with tag 'pvp':
1. World of Tanks - Genre: Shooter - Platform: PC (Windows)
2. Warframe - Genre: Shooter - Platform: PC (Windows)
3. CRSED: F.O.A.D. - Genre: Shooter - Platform: PC (Windows)",1,1
Three chess puzzles are presented for users to solve and learn.,"import requests

# API Constants
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""chess-puzzles.p.rapidapi.com""
}

# Function to fetch chess puzzles based on themes and difficulty
def fetch_chess_puzzles(themes=""middlegame"", rating=1500, playerMoves=4, count=3):
    """"""
    Fetch a specified number of chess puzzles based on theme, rating, and moves.
    """"""
    url = ""https://chess-puzzles.p.rapidapi.com/""
    querystring = {
        ""themes"": f'[""{themes}""]',
        ""rating"": rating,
        ""playerMoves"": playerMoves,
        ""count"": count
    }
    response = requests.get(url, headers=HEADERS, params=querystring)

    # Check if request was successful
    if response.status_code != 200:
        print(f""Error fetching puzzles: {response.status_code}"")
        return []

    return response.json().get(""puzzles"", [])

# Function to display fetched chess puzzles
def display_chess_puzzles():
    """"""
    Display a list of chess puzzles with details for learning and practice.
    """"""
    puzzles = fetch_chess_puzzles(themes=""middlegame"", rating=1500, playerMoves=4, count=3)

    if not puzzles:
        print(""No puzzles found. Please check your request or API parameters."")
        return

    print(""\nChess Puzzles for Learning:"")
    for i, puzzle in enumerate(puzzles, 1):
        puzzle_id = puzzle.get(""puzzleid"")
        theme = puzzle.get(""themes"", [])
        rating = puzzle.get(""rating"", ""N/A"")
        moves = puzzle.get(""moves"", [])

        print(f""\nPuzzle {i}:"")
        print(f""  - ID: {puzzle_id}"")
        print(f""  - Theme: {', '.join(theme)}"")
        print(f""  - Rating: {rating}"")
        print(f""  - Moves: {moves}"")
        print(""  - Try solving the puzzle on your chess board or an online chess platform!"")

# Main execution
if __name__ == '__main__':
    # Display selected chess puzzles
    display_chess_puzzles()
","Chess Puzzles for Learning:

Puzzle 1:
  - ID: dXxID
  - Theme: kingsideAttack, mate, mateIn4, middlegame, veryLong
  - Rating: 1508
  - Moves: ['e7f8', 'g4g6', 'f7g6', 'h6g6', 'f8g7', 'g6f7', 'g8h8', 'e5g6']
  - Try solving the puzzle on your chess board or an online chess platform!

Puzzle 2:
  - ID: 7Na5Y
  - Theme: attraction, equality, fork, middlegame, veryLong
  - Rating: 1653
  - Moves: ['f8g8', 'g2g8', 'd8g8', 'g1g8', 'f7g8', 'h5f6', 'g8f7', 'f6d7']
  - Try solving the puzzle on your chess board or an online chess platform!

Puzzle 3:
  - ID: pZfBU
  - Theme: advancedPawn, crushing, fork, middlegame, promotion, sacrifice, veryLong
  - Rating: 1790
  - Moves: ['b4c3', 'c2g6', 'h7g6', 'g5g6', 'f7g6', 'f6f7', 'g8h7', 'f7e8q']
  - Try solving the puzzle on your chess board or an online chess platform!

Process finished with exit code 0
",1,1
"I want to get health assessment data, including BMI (body mass index) based on height and weight, BMR (basal metabolic rate) based on information such as age, weight, and height, combined with activity level to get a total daily calorie burn (TDEE) for my diet and fitness program.","import requests

# API Constants
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""gym-fit.p.rapidapi.com""
}

# Function to calculate BMI based on weight and height
def get_bmi(weight, height):
    url = ""https://gym-fit.p.rapidapi.com/v1/calculator/bmi""
    querystring = {""weight"": str(weight), ""height"": str(height)}
    response = requests.get(url, headers=HEADERS, params=querystring)

    # Check for successful response
    if response.status_code == 200:
        bmi_result = response.json().get('bmi')
        return bmi_result
    else:
        print(""Error fetching BMI:"", response.status_code)
        return None

# Function to calculate BMR based on weight, height, gender, and age
def get_bmr(weight, height, gender, age):
    url = ""https://gym-fit.p.rapidapi.com/v1/calculator/bmr""
    querystring = {""weight"": str(weight), ""height"": str(height), ""gender"": gender, ""age"": str(age)}
    response = requests.get(url, headers=HEADERS, params=querystring)

    # Check for successful response
    if response.status_code == 200:
        bmr_result = response.json().get('bmr')
        return bmr_result
    else:
        print(""Error fetching BMR:"", response.status_code)
        return None

# Function to calculate TDEE based on BMR and activity level
def get_tdee(bmr, activity_level):
    activity_multiplier = {
        ""sedentary"": 1.2,
        ""lightly_active"": 1.375,
        ""moderately_active"": 1.55,
        ""active"": 1.725,
        ""very_active"": 1.9
    }
    multiplier = activity_multiplier.get(activity_level.lower())

    if bmr and multiplier:
        tdee = bmr * multiplier
        return round(tdee, 2)
    else:
        print(""Invalid BMR or activity level."")
        return None

# Main function to display health assessment results
def display_health_assessment(weight, height, gender, age, activity_level):
    print(f""Health Assessment for {gender.capitalize()}, Age {age}, Weight {weight}kg, Height {height}cm"")

    # Step 1: Calculate BMI
    bmi = get_bmi(weight, height)
    if bmi:
        print(f""\nBMI: {bmi} - {'Underweight' if bmi < 18.5 else 'Normal' if 18.5 <= bmi < 24.9 else 'Overweight' if 25 <= bmi < 29.9 else 'Obese'}"")

    # Step 2: Calculate BMR
    bmr = get_bmr(weight, height, gender, age)
    if bmr:
        print(f""BMR (Basal Metabolic Rate): {bmr} calories/day"")

    # Step 3: Calculate TDEE based on BMR and activity level
    tdee = get_tdee(bmr, activity_level)
    if tdee:
        print(f""TDEE (Total Daily Energy Expenditure): {tdee} calories/day"")

# Main execution
if __name__ == '__main__':
    # Input data for health assessment
    weight = 78  # kg
    height = 180  # cm
    gender = ""male""
    age = 25
    activity_level = ""active""  # Options: sedentary, lightly_active, moderately_active, active, very_active

    # Display health assessment
    display_health_assessment(weight, height, gender, age, activity_level)
","Health Assessment for Male, Age 25, Weight 78kg, Height 180cm
Invalid BMR or activity level.",1,1
"I want to get appropriate exercise recommendations based on the city's real-time air quality data, including air quality index (AQI) and concentrations of specific pollutants like PM2.5, O3, etc. When the air quality is poor, indoor exercise is recommended.","import requests

# API Key and Headers
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
HEADERS_AIR = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""airvisual1.p.rapidapi.com""
}
HEADERS_GYM = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""gym-fit.p.rapidapi.com""
}

# Function to get air quality data based on city ID
def get_air_quality(city_id):
    """"""
    Retrieves air quality data for the specified city ID.
    """"""
    url = ""https://airvisual1.p.rapidapi.com/cities/v2/get-measurements""
    querystring = {
        ""id"": city_id,
        ""x-user-lang"": ""en-US"",
        ""x-user-timezone"": ""Asia/Singapore"",
        ""x-aqi-index"": ""us""
    }
    response = requests.get(url, headers=HEADERS_AIR, params=querystring)

    # Check if the response is successful
    if response.status_code == 200:
        data = response.json().get(""data"", {}).get(""hourlyMeasurements"", [{}])[0].get(""measurements"", [])
        if not data:
            print(""Warning: No measurement data found."")
        return data
    else:
        print(""Error fetching air quality data:"", response.status_code)
        return []

# Function to get indoor exercise recommendations
def get_indoor_exercises():
    """"""
    Retrieves a list of recommended indoor exercises.
    """"""
    url = ""https://gym-fit.p.rapidapi.com/v1/exercises/search""
    querystring = {""number"": ""50"", ""offset"": ""0""}
    response = requests.get(url, headers=HEADERS_GYM, params=querystring)

    # Filter for indoor exercises
    exercises = response.json().get(""results"", [])
    indoor_exercises = [exercise for exercise in exercises if ""indoor"" in exercise.get(""name"", """").lower()]
    return indoor_exercises

# Function to provide exercise recommendations based on air quality
def exercise_recommendation(city_id):
    """"""
    Provides exercise recommendations based on the AQI and pollution levels in the specified city.
    """"""
    air_quality = get_air_quality(city_id)

    # Print air quality data for debugging
    print(""Air Quality Data:"", air_quality)

    # Extract AQI, PM2.5, and O3 values safely
    aqi = next((item['value'] for item in air_quality if item.get('parameter') == ""AQI""), None)
    pm25 = next((item['value'] for item in air_quality if item.get('parameter') == ""PM2.5""), None)
    o3 = next((item['value'] for item in air_quality if item.get('parameter') == ""O3""), None)

    # Display air quality information
    print(f""\nAir Quality Index (AQI): {aqi}"")
    print(f""PM2.5 Concentration: {pm25} Âµg/mÂ³"")
    print(f""Ozone (O3) Concentration: {o3} Âµg/mÂ³"")

    # Set thresholds for outdoor and indoor exercise recommendations
    if aqi is not None and pm25 is not None:
        if aqi > 100 or pm25 > 35:
            print(""\nAir quality is poor. Recommended indoor exercises:"")
            indoor_exercises = get_indoor_exercises()
            for i, exercise in enumerate(indoor_exercises[:5], 1):  # Display up to 5 indoor exercises
                print(f""{i}. {exercise['name']}"")
        else:
            print(""\nAir quality is good. Outdoor exercises are suitable."")
    else:
        print(""\nAir quality data is incomplete. Unable to provide a recommendation."")

# Main execution
if __name__ == '__main__':
    # Example city ID for air quality data
    city_id = 'hW7vArorRd8cT9h6v'  # Example city ID

    # Provide exercise recommendations based on air quality
    exercise_recommendation(city_id)
","Air Quality Data: [{'value': 15, 'measure': 'pm25', 'color': 'yellow', 'label': 'Moderate'}, {'measure': 'aqius', 'value': 62, 'color': 'yellow', 'label': 'Moderate'}, {'value': 29, 'measure': 'pm10', 'color': 'green', 'label': 'Good'}, {'value': 44, 'measure': 'o3', 'color': 'green', 'label': 'Good'}, {'value': 40, 'measure': 'no2', 'color': 'green', 'label': 'Good'}, {'value': 9, 'measure': 'so2', 'color': 'green', 'label': 'Good'}, {'value': 300, 'measure': 'co', 'color': 'green', 'label': 'Good'}]

Air Quality Index (AQI): None
PM2.5 Concentration: None Âµg/mÂ³
Ozone (O3) Concentration: None Âµg/mÂ³

Air quality data is incomplete. Unable to provide a recommendation.",1,1
"I want to get a training plan that matches my fitness level, including a difficulty level based on the user's current level, appropriate body part training movements, and detailed workout instructions to help increase the intensity of the workout safely and effectively. Now help me optimize and modify the following code to meet the new requirements above
","import requests

# API Key and Headers
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""exercise-db-fitness-workout-gym.p.rapidapi.com""
}

# Static list of valid muscle groups as a backup
VALID_MUSCLE_GROUPS = [
    ""abductors"", ""adductors"", ""biceps"", ""calves"", ""chest"", ""core"", ""glutes"",
    ""hamstrings"", ""lats"", ""lower_back"", ""middle_back"", ""neck"", ""quadriceps"",
    ""shoulders"", ""traps"", ""triceps""
]

# Function to get exercises based on user's difficulty level
def get_exercises_by_level(level):
    url = f""https://exercise-db-fitness-workout-gym.p.rapidapi.com/exercises/level/{level}""
    response = requests.get(url, headers=HEADERS)

    if response.status_code == 200:
        return response.json()
    else:
        print(""Error fetching exercises by level:"", response.status_code)
        return []

# Function to get a list of valid target muscles (with static backup)
def get_valid_muscle_groups():
    """"""
    Attempt to retrieve a list of valid muscle groups from the API.
    If this fails, use a predefined static list as a backup.
    """"""
    url = ""https://exercisedb.p.rapidapi.com/exercises/targetList""
    response = requests.get(url, headers=HEADERS)

    if response.status_code == 200:
        return response.json()
    else:
        print(""Error fetching muscle groups:"", response.status_code)
        print(""Using static backup muscle group list."")
        return VALID_MUSCLE_GROUPS

# Function to get exercises targeting specific muscle group
def get_exercises_for_target_muscle(target_muscle, limit=5):
    url = f""https://exercisedb.p.rapidapi.com/exercises/target/{target_muscle}""
    querystring = {""limit"": str(limit)}
    response = requests.get(url, headers=HEADERS, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(""Error fetching exercises for target muscle:"", response.status_code)
        return []

# Function to display recommended workout plan based on user level and muscle target
def display_workout_plan(level, target_muscle):
    # Step 1: Get exercises based on user's level
    level_exercises = get_exercises_by_level(level)
    print(f""\nRecommended Exercises for Level '{level.capitalize()}':"")
    for i, exercise in enumerate(level_exercises[:5], 1):
        body_part = exercise.get('bodyPart', 'Not specified')
        equipment = exercise.get('equipment', 'Not specified')
        print(f""{i}. {exercise.get('name')} - Body Part: {body_part} - Equipment: {equipment}"")

    # Step 2: Check if target muscle is valid using static or fetched list
    valid_muscles = get_valid_muscle_groups()
    if target_muscle.lower() not in [muscle.lower() for muscle in valid_muscles]:
        print(f""\nError: '{target_muscle}' is not a valid muscle group."")
        print(""Please choose from the following available muscle groups:"")
        print("", "".join(valid_muscles))
        return

    # Step 3: Get exercises for a specific target muscle group
    target_exercises = get_exercises_for_target_muscle(target_muscle)
    print(f""\nExercises Targeting '{target_muscle.capitalize()}':"")
    for i, exercise in enumerate(target_exercises, 1):
        print(f""{i}. {exercise.get('name')}"")
        print(f""   - Body Part: {exercise.get('bodyPart', 'Not specified')}"")
        print(f""   - Equipment: {exercise.get('equipment', 'Not specified')}"")
        print(f""   - Instructions: {exercise.get('instructions', 'No instructions available')}"")

# Main execution
if __name__ == '__main__':
    # Define user level and target muscle for workout plan
    user_level = 'intermediate'  # Example levels: 'beginner', 'intermediate', 'advanced'
    target_muscle = 'abductors'   # Example target muscle

    # Display workout plan based on user's fitness level and target muscle
    display_workout_plan(user_level, target_muscle)
","Recommended Exercises for Level 'Intermediate':
1. Ab Crunch Machine - Body Part: Not specified - Equipment: machine
2. Ab Roller - Body Part: Not specified - Equipment: other
3. Adductor - Body Part: Not specified - Equipment: foam roll
4. Adductor/Groin - Body Part: Not specified - Equipment: None
5. Advanced Kettlebell Windmill - Body Part: Not specified - Equipment: kettlebells
Error fetching muscle groups: 404
Using static backup muscle group list.
Error fetching exercises for target muscle: 404

Exercises Targeting 'Abductors':",1,1
I need to assess their current weight status by BMI (body mass index) and develop a targeted fitness plan based on specific information about the target muscle groups and recommended training movements to better focus the training on the target area.,"import requests

# API Key and Headers
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""gym-fit.p.rapidapi.com""
}

# Function to calculate BMI and assess weight status
def get_bmi(weight, height):
    url = ""https://gym-fit.p.rapidapi.com/v1/calculator/bmi""
    params = {""weight"": weight, ""height"": height}
    response = requests.get(url, headers=HEADERS, params=params)

    # Extract BMI result and calculate weight status
    bmi_value = response.json().get('result')
    if bmi_value:
        if bmi_value < 18.5:
            status = ""Underweight""
        elif 18.5 <= bmi_value < 24.9:
            status = ""Normal weight""
        elif 25 <= bmi_value < 29.9:
            status = ""Overweight""
        else:
            status = ""Obese""
        return bmi_value, status
    else:
        return None, ""Error calculating BMI""

# Function to get exercise details by exercise ID
def get_exercise_by_id(exercise_id):
    url = f""https://gym-fit.p.rapidapi.com/v1/exercises/{exercise_id}""
    response = requests.get(url, headers=HEADERS)
    return response.json()

# Function to get muscle information by muscle ID
def get_muscle_by_id(muscle_id):
    url = f""https://gym-fit.p.rapidapi.com/v1/muscles/{muscle_id}""
    response = requests.get(url, headers=HEADERS)
    return response.json()

# Function to recommend exercises based on BMI and target muscle group
def recommend_exercises(weight, height, target_muscle_id):
    # Step 1: Calculate BMI and determine weight status
    bmi_value, status = get_bmi(weight, height)
    print(f""Your BMI is {bmi_value}, indicating '{status}' status."")

    # Step 2: Get muscle details for the target area
    muscle_details = get_muscle_by_id(target_muscle_id)
    muscle_name = muscle_details.get('name', 'Unknown Muscle')
    print(f""\nTargeting Muscle: {muscle_name}"")

    # Step 3: Get exercises targeting the specified muscle
    exercises = muscle_details.get('exercises', [])
    if not exercises:
        print(f""No exercises found for the muscle '{muscle_name}'."")
        return

    print(f""\nRecommended Exercises for '{muscle_name}':"")
    for i, exercise in enumerate(exercises[:5], 1):  # Limit to top 5 exercises
        exercise_details = get_exercise_by_id(exercise['id'])
        print(f""{i}. {exercise_details.get('name')}"")
        print(f""   - Description: {exercise_details.get('description', 'No description available')}"")
        print(f""   - Equipment: {exercise_details.get('equipment', 'Body weight')}"")
        print(f""   - Instructions: {exercise_details.get('instructions', 'No instructions provided')}"")

# Main execution
if __name__ == '__main__':
    # Example user data
    weight = 78  # User's weight in kilograms
    height = 180  # User's height in centimeters
    target_muscle_id = 'biceps'  # Example target muscle ID

    # Recommend exercises based on user's BMI and target muscle group
    recommend_exercises(weight, height, target_muscle_id)
","Your BMI is 24.1, indicating 'Normal weight' status.

Targeting Muscle: Unknown Muscle
No exercises found for the muscle 'Unknown Muscle'.
",1,1
"I want to get training movements for specific muscle groups to strengthen muscles or shape. The system needs to provide information about the target muscle groups, recommend suitable training movements, and give detailed instructions and steps for each movement to help users train effectively.

","import requests

# API Key and Headers
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""exercisedb.p.rapidapi.com""
}

# Function to get the list of available target muscle groups
def get_muscle_groups():
    url = ""https://exercisedb.p.rapidapi.com/exercises/targetList""
    response = requests.get(url, headers=HEADERS)

    if response.status_code == 200:
        return response.json()
    else:
        print(""Error fetching muscle groups:"", response.status_code)
        return []

# Function to get exercises targeting a specific muscle
def get_exercises_for_muscle(target, limit=10, offset=0):
    url = f""https://exercisedb.p.rapidapi.com/exercises/target/{target}""
    querystring = {""limit"": str(limit), ""offset"": str(offset)}
    response = requests.get(url, headers=HEADERS, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(""Error fetching exercises for target muscle:"", response.status_code)
        return []

# Function to search detailed information about specific muscles
def get_muscle_info(muscle_name, number=1, offset=0):
    """"""
    Attempts to fetch detailed information about a specific muscle.
    If the API returns a string list instead of a dictionary, the function will handle it gracefully.
    """"""
    url = ""https://gym-fit.p.rapidapi.com/v1/muscles/search""
    querystring = {
        ""query"": muscle_name,
        ""number"": str(number),
        ""offset"": str(offset)
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""gym-fit.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        data = response.json()
        # Ensure data is a list of dictionaries, not a list of strings
        if isinstance(data, list) and all(isinstance(item, dict) for item in data):
            return data
        else:
            print(""Unexpected data format received for muscle information."")
            return []
    else:
        print(""Error fetching muscle information:"", response.status_code)
        return []

# Main function to provide a workout plan focused on a specific muscle group
def display_muscle_workout_plan(target_muscle):
    # Step 1: Get muscle group info
    muscle_info = get_muscle_info(target_muscle)
    if muscle_info:
        print(f""\nMuscle Information for '{target_muscle.capitalize()}':"")
        for muscle in muscle_info:
            print(f"" - Name: {muscle.get('name', 'N/A')}"")
            print(f"" - Description: {muscle.get('description', 'No description available')}"")
    else:
        print(f""No detailed information available for '{target_muscle}'."")

    # Step 2: Get exercises for the selected muscle group
    exercises = get_exercises_for_muscle(target_muscle)
    if exercises:
        print(f""\nRecommended Exercises for '{target_muscle.capitalize()}':"")
        for i, exercise in enumerate(exercises, 1):
            print(f""\n{i}. {exercise.get('name')}"")
            print(f""   - Body Part: {exercise.get('bodyPart', 'Not specified')}"")
            print(f""   - Equipment: {exercise.get('equipment', 'Body weight')}"")
            print(f""   - Instructions: {exercise.get('instructions', 'No instructions provided')}"")
            print(f""   - Steps: {exercise.get('steps', 'No step-by-step guide available')}"")
    else:
        print(f""No exercises found for muscle '{target_muscle}'."")

# Main execution
if __name__ == '__main__':
    # Step 1: List available muscle groups
    muscle_groups = get_muscle_groups()
    print(""Available Muscle Groups:"", "", "".join(muscle_groups))

    # Example: Choose a target muscle group for focused training (e.g., 'biceps')
    target_muscle = ""biceps""  # This can be dynamic based on user choice

    # Step 2: Display workout plan based on the chosen muscle group
    display_muscle_workout_plan(target_muscle)
",Fail,-1,-1
"By entering the postcode, I want to get additional postcode and location data within a 5km radius of the location, which can be used to explore nearby areas or find adjacent locations.","import requests

# API Key and Headers
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
ZIP_API_HOST = ""zip-code-distance-radius.p.rapidapi.com""
GEO_API_HOST = ""navitime-geocoding.p.rapidapi.com""

# Function to get zip codes within a radius
def get_zipcodes_within_radius(zip_code, radius):
    url = f""https://{ZIP_API_HOST}/api/zipCodesWithinRadius""
    querystring = {""zipCode"": zip_code, ""radius"": str(radius)}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ZIP_API_HOST
    }
    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching zip codes within radius: {response.status_code}"")
        return []

# Function to get detailed location data based on postal code
def get_location_data(postal_code):
    """"""
    Retrieves location data for a given postal code. Returns None if an error occurs.
    """"""
    url = f""https://{GEO_API_HOST}/address/postal_code""
    querystring = {""datum"": ""wgs84"", ""coord_unit"": ""degree"", ""limit"": ""1"", ""postal_code"": postal_code}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": GEO_API_HOST
    }
    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching location data for postal code {postal_code}: {response.status_code}"")
        return None

# Main function to display nearby zip codes with location data
def display_nearby_zipcode_data(zip_code, radius_km):
    # Step 1: Get nearby zip codes within the specified radius
    zip_codes_data = get_zipcodes_within_radius(zip_code, radius_km)

    if not zip_codes_data:
        print(""No nearby zip codes found."")
        return

    # Step 2: Retrieve and display location data for each nearby zip code
    print(f""Nearby Locations within {radius_km} km of {zip_code}:"")
    for item in zip_codes_data:
        # Try to fetch location details; if unavailable, mark as N/A
        zip_code_detail = get_location_data(item[""zipCode""])

        location_name = zip_code_detail.get(""location"", {}).get(""name"", ""N/A"") if zip_code_detail else ""N/A""
        coordinates = zip_code_detail.get(""location"", {}).get(""coordinates"", ""N/A"") if zip_code_detail else ""N/A""

        print(f""\nZip Code: {item['zipCode']}"")
        print(f""  - Distance: {item['distance']} km"")
        print(f""  - Location: {location_name}"")
        print(f""  - Coordinates: {coordinates}"")

# Main execution
if __name__ == '__main__':
    # Input parameters
    zip_code = ""48195""  # Example zip code
    radius_km = 5       # Radius in km

    # Display nearby zip code data with location details
    display_nearby_zipcode_data(zip_code, radius_km)
","Nearby Locations within 5 km of 48195:
Error fetching location data for postal code 48195: 400

Zip Code: 48195
  - Distance: 0.0 km
  - Location: N/A
  - Coordinates: N/A
Error fetching location data for postal code 48193: 400

Zip Code: 48193
  - Distance: 2.1363509674204266 km
  - Location: N/A
  - Coordinates: N/A
Error fetching location data for postal code 48192: 400

Zip Code: 48192
  - Distance: 2.5145184538923644 km
  - Location: N/A
  - Coordinates: N/A
Error fetching location data for postal code 48146: 400

Zip Code: 48146
  - Distance: 2.9437606650577206 km
  - Location: N/A
  - Coordinates: N/A
Error fetching location data for postal code 48180: 400

Zip Code: 48180
  - Distance: 3.476270389701451 km
  - Location: N/A
  - Coordinates: N/A
Error fetching location data for postal code 48101: 400

Zip Code: 48101
  - Distance: 3.790888925723768 km
  - Location: N/A
  - Coordinates: N/A
Error fetching location data for postal code 48229: 400

Zip Code: 48229
  - Distance: 4.555562331497093 km
  - Location: N/A
  - Coordinates: N/A
",1,1
"I would like to query a country and a subset of cities in a specific state to get geographic information about each city for display on a map or for regional analysis.

","import requests

# API Key and Headers
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
BASE_URL = ""https://rwanda.p.rapidapi.com""
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""rwanda.p.rapidapi.com""
}

# Function to get provinces
def get_provinces():
    url = f""{BASE_URL}/provinces""
    response = requests.get(url, headers=HEADERS)

    if response.status_code == 200:
        return response.json().get(""data"", [])
    else:
        print(f""Error fetching provinces: {response.status_code}"")
        return []

# Function to get districts in a given province
def get_districts(province):
    url = f""{BASE_URL}/districts""
    params = {""p"": province}
    response = requests.get(url, headers=HEADERS, params=params)

    if response.status_code == 200:
        return response.json().get(""data"", [])
    else:
        print(f""Error fetching districts for province {province}: {response.status_code}"")
        return []

# Function to get villages in a given province and district
def get_villages(province, district):
    url = f""{BASE_URL}/villages""
    params = {""p"": province, ""d"": district}
    response = requests.get(url, headers=HEADERS, params=params)

    if response.status_code == 200:
        return response.json().get(""data"", [])
    else:
        print(f""Error fetching villages for district {district} in province {province}: {response.status_code}"")
        return []

# Main function to collect administrative data
def collect_administrative_data():
    all_data = {}

    # Step 1: Get all provinces
    provinces = get_provinces()
    for province in provinces:
        province_name = province.get('name', 'Unknown Province')
        province_data = {}

        # Step 2: Get districts for each province
        districts = get_districts(province_name)
        for district in districts:
            district_name = district.get('name', 'Unknown District')

            # Step 3: Get villages for each district
            villages = get_villages(province_name, district_name)
            province_data[district_name] = villages

        all_data[province_name] = province_data

    return all_data

if __name__ == '__main__':
    # Collect and print administrative data
    administrative_data = collect_administrative_data()
    print(""Administrative Data Structure:"")
    for province, districts in administrative_data.items():
        print(f""\nProvince: {province}"")
        for district, villages in districts.items():
            print(f""  District: {district}"")
            for village in villages:
                print(f""    Village: {village}"")
","Top 10 Cities in Florida, United States:
 - City: Aberdeen, Latitude: 26.55063000, Longitude: -80.14866000
 - City: Alachua, Latitude: 29.75163000, Longitude: -82.42483000
 - City: Alachua County, Latitude: 29.67476000, Longitude: -82.35770000
 - City: Alafaya, Latitude: 28.56410000, Longitude: -81.21140000
 - City: Allapattah, Latitude: 25.81454000, Longitude: -80.22394000
 - City: Altamonte Springs, Latitude: 28.66111000, Longitude: -81.36562000
 - City: Alturas, Latitude: 27.87169000, Longitude: -81.71508000
 - City: Alva, Latitude: 26.71562000, Longitude: -81.61008000
 - City: Andover, Latitude: 25.96843000, Longitude: -80.21283000
 - City: Anna Maria, Latitude: 27.53115000, Longitude: -82.73343000",1,1
"I need to obtain the information of some of the ten administrative units (such as provinces, regions, townships, and villages) in a specified country/region for administrative data collation or geographic classification display.

","import requests

# API Key and Headers
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
BASE_URL = ""https://rwanda.p.rapidapi.com""
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""rwanda.p.rapidapi.com""
}

# Function to get provinces (limit to first 10)
def get_provinces():
    print(""Fetching provinces..."")
    url = f""{BASE_URL}/provinces""
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        if response.status_code == 200:
            return response.json().get(""data"", [])[:10]  # Limit to 10 provinces
        else:
            print(f""Error fetching provinces: {response.status_code}"")
    except requests.exceptions.Timeout:
        print(""Request timed out while fetching provinces."")
    return []

# Function to get districts in a given province (limit to first 10)
def get_districts(province):
    print(f""Fetching districts for province: {province}"")
    url = f""{BASE_URL}/districts""
    params = {""p"": province}
    try:
        response = requests.get(url, headers=HEADERS, params=params, timeout=10)
        if response.status_code == 200:
            return response.json().get(""data"", [])[:10]  # Limit to 10 districts
        else:
            print(f""Error fetching districts for province {province}: {response.status_code}"")
    except requests.exceptions.Timeout:
        print(f""Request timed out while fetching districts for province: {province}"")
    return []

# Function to get villages in a given province and district (limit to first 10)
def get_villages(province, district):
    print(f""Fetching villages for district: {district} in province: {province}"")
    url = f""{BASE_URL}/villages""
    params = {""p"": province, ""d"": district}
    try:
        response = requests.get(url, headers=HEADERS, params=params, timeout=10)
        if response.status_code == 200:
            return response.json().get(""data"", [])[:10]  # Limit to 10 villages
        else:
            print(f""Error fetching villages for district {district} in province {province}: {response.status_code}"")
    except requests.exceptions.Timeout:
        print(f""Request timed out while fetching villages for district: {district} in province: {province}"")
    return []

# Main function to collect administrative data
def collect_administrative_data():
    all_data = {}

    # Step 1: Get all provinces (limited to 10)
    provinces = get_provinces()
    for province in provinces:
        if isinstance(province, dict):
            province_name = province.get('name', 'Unknown Province')
        elif isinstance(province, str):
            province_name = province
        else:
            print(f""Unexpected data format for province: {province}"")
            continue

        province_data = {}

        # Step 2: Get districts for each province (limited to 10)
        districts = get_districts(province_name)
        for district in districts:
            if isinstance(district, dict):
                district_name = district.get('name', 'Unknown District')
            elif isinstance(district, str):
                district_name = district
            else:
                print(f""Unexpected data format for district: {district}"")
                continue

            # Step 3: Get villages for each district (limited to 10)
            villages = get_villages(province_name, district_name)
            province_data[district_name] = villages

        all_data[province_name] = province_data

    return all_data

if __name__ == '__main__':
    # Collect and print administrative data
    print(""Starting data collection..."")
    administrative_data = collect_administrative_data()
    print(""Data collection completed."")
    print(""Administrative Data Structure:"")
    for province, districts in administrative_data.items():
        print(f""\nProvince: {province}"")
        for district, villages in districts.items():
            print(f""  District: {district}"")
            for village in villages:
                print(f""    Village: {village}"")
",Fail,-1,-1
Enter an address and the system will automatically suggest completions to quickly find the correct address and get relevant geographic information.,"import requests

# API Key and Headers
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""navitime-geocoding.p.rapidapi.com""
}

def address_autocomplete(word):
    url = ""https://navitime-geocoding.p.rapidapi.com/address/autocomplete""
    querystring = {""datum"": ""wgs84"", ""word"": word, ""coord_unit"": ""degree""}
    response = requests.get(url, headers=HEADERS, params=querystring)

    if response.status_code == 200:
        data = response.json()
        return data.get(""items"", [])
    else:
        print(f""Error fetching autocomplete suggestions: {response.status_code}"")
        return []

def get_address_details(code):
    url = ""https://navitime-geocoding.p.rapidapi.com/address""
    querystring = {""coord_unit"": ""degree"", ""datum"": ""wgs84"", ""code"": code}
    response = requests.get(url, headers=HEADERS, params=querystring)

    if response.status_code == 200:
        data = response.json()
        return data.get(""items"", [])
    else:
        print(f""Error fetching address details: {response.status_code}"")
        return []

if __name__ == '__main__':
    # ä½¿ç”¨é»˜è®¤åœ°å€å…³é”®å­— 'ä»£ã€…æœ¨' è¿›è¡Œè¡¥å…¨
    user_input = ""ä»£ã€…æœ¨""
    autocomplete_results = address_autocomplete(user_input)

    if autocomplete_results:
        print(""Autocomplete suggestions:"")
        for idx, suggestion in enumerate(autocomplete_results):
            print(f""{idx + 1}. {suggestion['name']} - {suggestion.get('address', 'N/A')}"")

        # å‡è®¾ç”¨æˆ·é€‰æ‹©ç¬¬ä¸€ä¸ªç»“æžœ
        selected_index = 0
        selected_code = autocomplete_results[selected_index][""code""]

        # èŽ·å–è¯¦ç»†çš„åœ°ç†ä¿¡æ¯
        detailed_info = get_address_details(selected_code)
        if detailed_info:
            print(""\nDetailed geographic information:"")
            for item in detailed_info:
                print(f""Name: {item.get('name', 'N/A')}"")
                print(f""Address: {item.get('address', 'N/A')}"")
                print(f""Latitude: {item.get('latitude', 'N/A')}"")
                print(f""Longitude: {item.get('longitude', 'N/A')}"")
                print(""\n---\n"")
        else:
            print(""No detailed information found for the selected address."")
    else:
        print(""No autocomplete suggestions available."")
",Fail,-1,-1
"I enter the starting point and destination, and I need to provide the best driving route, show the driving path, and estimate the arrival time.

","import requests
from datetime import datetime, timedelta

# API Key and Headers
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def FindDrivingRoute(start, destination):
    url = ""https://trueway-directions2.p.rapidapi.com/FindDrivingRoute""
    querystring = {
        ""stops"": f""{start[0]},{start[1]};{destination[0]},{destination[1]}"",
        ""optimize"": ""true"",
        ""geometry_format"": ""latlng""
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""trueway-directions2.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching driving route: {response.status_code}"")
        return None

def GetDistance(start, destination):
    url = ""https://geocodeapi.p.rapidapi.com/GetDistance""
    querystring = {
        ""lat1"": start[0],
        ""lon1"": start[1],
        ""lat2"": destination[0],
        ""lon2"": destination[1]
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""geocodeapi.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching distance: {response.status_code}"")
        return None

def GetTimezone(lat, lon):
    url = ""https://geocodeapi.p.rapidapi.com/GetTimezone""
    querystring = {""latitude"": lat, ""longitude"": lon}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""geocodeapi.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching timezone: {response.status_code}"")
        return None

if __name__ == '__main__':
    # é»˜è®¤çš„èµ·ç‚¹å’Œç›®çš„åœ°åæ ‡ï¼ˆçº½çº¦å¸‚ Times Square å’Œ Brooklyn Bridgeï¼‰
    start = (40.758896, -73.985130)  # Times Square
    destination = (40.706086, -73.996864)  # Brooklyn Bridge

    # èŽ·å–è¡Œè½¦è·¯çº¿è¯¦æƒ…
    route_data = FindDrivingRoute(start, destination)
    if route_data and 'route' in route_data:
        route_duration_seconds = route_data['route']['duration']
        route_distance = route_data['route']['distance']
        route_path = route_data['route']['geometry']['coordinates']

        # èŽ·å–è·ç¦»å’Œæ–¹ä½ä¿¡æ¯
        distance_data = GetDistance(start, destination)

        # èŽ·å–ç›®çš„åœ°çš„æ—¶åŒºï¼Œä»¥ä¾¿è®¡ç®—æœ¬åœ°åˆ°è¾¾æ—¶é—´
        timezone_data = GetTimezone(destination[0], destination[1])
        if timezone_data and 'LocalTime_Now' in timezone_data:
            local_time_now = datetime.strptime(timezone_data['LocalTime_Now'], '%I:%M:%S %p')
            arrival_time = local_time_now + timedelta(seconds=route_duration_seconds)

            # æ˜¾ç¤ºç»“æžœ
            print(""Driving route details:"")
            print(""Distance:"", route_distance, ""meters"")
            print(""Estimated Duration:"", route_duration_seconds // 60, ""minutes"")
            print(""Arrival Time (local at destination):"", arrival_time.strftime('%I:%M:%S %p'))
            print(""Route Path:"", route_path)
        else:
            print(""Failed to retrieve local time at destination."")
    else:
        print(""Failed to retrieve driving route details."")
","Driving route details:
Distance: 9777 meters
Estimated Duration: 21 minutes
Arrival Time (local at destination): 10:19:25 AM
Route Path: [[40.758773, -73.984914], [40.758553, -73.985072], [40.758314, -73.98525], [40.758274, -73.985279], [40.758245, -73.985309], [40.757937, -73.985536], [40.757769, -73.985655], [40.757542, -73.985824], [40.757532, -73.985824], [40.757424, -73.985903], [40.757325, -73.985972], [40.75702, -73.986201], [40.756696, -73.98643], [40.75652, -73.98657], [40.756451, -73.98662], [40.756049, -73.98692], [40.755685, -73.987201], [40.755409, -73.987412], [40.755162, -73.987573], [40.755013, -73.987704], [40.754775, -73.987866], [40.754616, -73.987977], [40.754457, -73.988098], [40.754407, -73.988139], [40.754307, -73.988209], [40.754297, -73.98822], [40.754257, -73.98824], [40.754177, -73.988301], [40.753544, -73.988787], [40.752916, -73.989233], [40.752763, -73.989345], [40.752438, -73.989578], [40.752285, -73.989689], [40.752132, -73.989801], [40.751817, -73.990033], [40.751766, -73.990064], [40.751654, -73.990145], [40.751369, -73.990367], [40.751135, -73.990538], [40.750983, -73.990639], [40.750588, -73.989679], [40.750487, -73.989436], [40.750416, -73.989254], [40.750235, -73.988858], [40.750054, -73.988402], [40.749933, -73.988139], [40.749853, -73.987957], [40.749813, -73.987855], [40.749753, -73.987734], [40.749663, -73.987502], [40.749523, -73.98714], [40.749404, -73.98689], [40.749394, -73.98685], [40.749215, -73.98644], [40.749175, -73.98634], [40.749105, -73.986181], [40.748858, -73.985576], [40.748848, -73.985546], [40.74864, -73.985062], [40.748433, -73.984559], [40.748128, -73.983819], [40.747981, -73.983503], [40.747843, -73.983186], [40.747765, -73.982978], [40.747539, -73.98245], [40.747431, -73.98218], [40.747343, -73.98198], [40.747294, -73.98187], [40.747225, -73.981709], [40.747206, -73.981659], [40.747127, -73.981467], [40.747088, -73.981367], [40.747058, -73.981276], [40.747009, -73.981165], [40.746931, -73.980963], [40.746891, -73.980861], [40.746576, -73.980091], [40.746557, -73.98003], [40.746429, -73.979726], [40.74634, -73.979513], [40.74629, -73.979392], [40.746251, -73.979301], [40.745984, -73.978644], [40.745904, -73.978462], [40.745765, -73.9781], [40.745556, -73.9776], [40.745437, -73.97732], [40.745257, -73.976933], [40.745157, -73.976695], [40.745097, -73.976536], [40.744806, -73.975884], [40.744735, -73.975707], [40.744725, -73.975697], [40.744614, -73.975421], [40.744533, -73.975224], [40.744483, -73.975096], [40.744301, -73.974701], [40.744169, -73.974394], [40.744017, -73.974038], [40.743885, -73.973729], [40.743824, -73.97353], [40.743723, -73.97337], [40.743601, -73.97308], [40.74357, -73.973], [40.743458, -73.972739], [40.743306, -73.972377], [40.743285, -73.972326], [40.743234, -73.972195], [40.743214, -73.972155], [40.743194, -73.972185], [40.743163, -73.972225], [40.743143, -73.972256], [40.743132, -73.972286], [40.743112, -73.972316], [40.743031, -73.972336], [40.742959, -73.972367], [40.742786, -73.972447], [40.742766, -73.972467], [40.742755, -73.972467], [40.742725, -73.972477], [40.742674, -73.972508], [40.742613, -73.972528], [40.742542, -73.972558], [40.74246, -73.972588], [40.742369, -73.972628], [40.742297, -73.972649], [40.742226, -73.972679], [40.742145, -73.972709], [40.742054, -73.972729], [40.741973, -73.972749], [40.741881, -73.972779], [40.74179, -73.972799], [40.74175, -73.972799], [40.741679, -73.972819], [40.741547, -73.97285], [40.741366, -73.97288], [40.741335, -73.97289], [40.741094, -73.97292], [40.740943, -73.97294], [40.740923, -73.97295], [40.740863, -73.97288], [40.740703, -73.97292], [40.740643, -73.97293], [40.740573, -73.97294], [40.740394, -73.97298], [40.740334, -73.97299], [40.740145, -73.97303], [40.740036, -73.97305], [40.739947, -73.97307], [40.739917, -73.97307], [40.739868, -73.97308], [40.739828, -73.97309], [40.73967, -73.97313], [40.739621, -73.97315], [40.739512, -73.97318], [40.739414, -73.97321], [40.739305, -73.97326], [40.739207, -73.9733], [40.73906, -73.97337], [40.738971, -73.97341], [40.738951, -73.97342], [40.738893, -73.97346], [40.738804, -73.97351], [40.738716, -73.97356], [40.738618, -73.97363], [40.738529, -73.97369], [40.738451, -73.973749], [40.738353, -73.973819], [40.738255, -73.973889], [40.738166, -73.973958], [40.738068, -73.974028], [40.73797, -73.974097], [40.737842, -73.974196], [40.737636, -73.974345], [40.737616, -73.974365], [40.737547, -73.974414], [40.737448, -73.974493], [40.737438, -73.974493], [40.73735, -73.974563], [40.737211, -73.974661], [40.737172, -73.974691], [40.737063, -73.97476], [40.737043, -73.97477], [40.736974, -73.97481], [40.736924, -73.974839], [40.736904, -73.974849], [40.736875, -73.974869], [40.736825, -73.974889], [40.736775, -73.974908], [40.736686, -73.974948], [40.736676, -73.974948], [40.736586, -73.974977], [40.736477, -73.975007], [40.736447, -73.975017], [40.736367, -73.975037], [40.736277, -73.975056], [40.736147, -73.975066], [40.736097, -73.975066], [40.736057, -73.975066], [40.736047, -73.975066], [40.735996, -73.975066], [40.735956, -73.975066], [40.735916, -73.975066], [40.735876, -73.975066], [40.735826, -73.975066], [40.735775, -73.975056], [40.735715, -73.975046], [40.735705, -73.975046], [40.735655, -73.975037], [40.735574, -73.975017], [40.735473, -73.974987], [40.735362, -73.974968], [40.735352, -73.974958], [40.73525, -73.974938], [40.735169, -73.974918], [40.735159, -73.974918], [40.735027, -73.974879], [40.734763, -73.974819], [40.73454, -73.97476], [40.734489, -73.97475], [40.734316, -73.974711], [40.734163, -73.974671], [40.73398, -73.974622], [40.733623, -73.974533], [40.73345, -73.974493], [40.733338, -73.974464], [40.733003, -73.974385], [40.732922, -73.974365], [40.732739, -73.974315], [40.732568, -73.974276], [40.732446, -73.974246], [40.732325, -73.974206], [40.732154, -73.974167], [40.732014, -73.974137], [40.731903, -73.974107], [40.731793, -73.974077], [40.731743, -73.974067], [40.731673, -73.974038], [40.731583, -73.974008], [40.731563, -73.973998], [40.731414, -73.973938], [40.731374, -73.973908], [40.731264, -73.973859], [40.731205, -73.973819], [40.731155, -73.973789], [40.731046, -73.97371], [40.730947, -73.97363], [40.730917, -73.9736], [40.730897, -73.97358], [40.730828, -73.97351], [40.730739, -73.97342], [40.73068, -73.97335], [40.73064, -73.97331], [40.730571, -73.97322], [40.730355, -73.97296], [40.730325, -73.97291], [40.730315, -73.9729], [40.730217, -73.972779], [40.729961, -73.972467], [40.729843, -73.972356], [40.729696, -73.972225], [40.729598, -73.972155], [40.72949, -73.972074], [40.729333, -73.971983], [40.729323, -73.971983], [40.729196, -73.971922], [40.729098, -73.971892], [40.729019, -73.971861], [40.728901, -73.971821], [40.728891, -73.971821], [40.728813, -73.971811], [40.728704, -73.971791], [40.728616, -73.97178], [40.728498, -73.97178], [40.72834, -73.971791], [40.727964, -73.971841], [40.727706, -73.971872], [40.727317, -73.971932], [40.727087, -73.971963], [40.726886, -73.971993], [40.726574, -73.972074], [40.726402, -73.972134], [40.72625, -73.972195], [40.726048, -73.972286], [40.725906, -73.972356], [40.725885, -73.972377], [40.725845, -73.972397], [40.725652, -73.972498], [40.72557, -73.972548], [40.72556, -73.972548], [40.72553, -73.972568], [40.725336, -73.972679], [40.725234, -73.972739], [40.725183, -73.972769], [40.725132, -73.972809], [40.725071, -73.97284], [40.72499, -73.9729], [40.724888, -73.97295], [40.724725, -73.97305], [40.724633, -73.97311], [40.724603, -73.97313], [40.724491, -73.9732], [40.72443, -73.97325], [40.724399, -73.97326], [40.724277, -73.97334], [40.724135, -73.97343], [40.724104, -73.97344], [40.723658, -73.973729], [40.723456, -73.973849], [40.723295, -73.973948], [40.723164, -73.974028], [40.722863, -73.974216], [40.722663, -73.974325], [40.722563, -73.974375], [40.722463, -73.974424], [40.722364, -73.974474], [40.722254, -73.974513], [40.722145, -73.974543], [40.722105, -73.974563], [40.722046, -73.974582], [40.721927, -73.974602], [40.721828, -73.974632], [40.721719, -73.974652], [40.721621, -73.974661], [40.721621, -73.974671], [40.721296, -73.974701], [40.720853, -73.97476], [40.720696, -73.97477], [40.720657, -73.97478], [40.720333, -73.974819], [40.720166, -73.974839], [40.720157, -73.974839], [40.720009, -73.974859], [40.719616, -73.974908], [40.719241, -73.974958], [40.719103, -73.974977], [40.719043, -73.974987], [40.718895, -73.974997], [40.718686, -73.975027], [40.718556, -73.975046], [40.718427, -73.975066], [40.718377, -73.975076], [40.718207, -73.975116], [40.717996, -73.975165], [40.717946, -73.975175], [40.717896, -73.975185], [40.717866, -73.975194], [40.717584, -73.975283], [40.717372, -73.975342], [40.717291, -73.975372], [40.716835, -73.975549], [40.716631, -73.975628], [40.716204, -73.975845], [40.716112, -73.975894], [40.715908, -73.975993], [40.715755, -73.976072], [40.715633, -73.976131], [40.715531, -73.976181], [40.715409, -73.97625], [40.715044, -73.976428], [40.714891, -73.976507], [40.71477, -73.976566], [40.714709, -73.976596], [40.714568, -73.976665], [40.714537, -73.976685], [40.714315, -73.976794], [40.714235, -73.976833], [40.714205, -73.976853], [40.713983, -73.976972], [40.713344, -73.97729], [40.713245, -73.97734], [40.713195, -73.97737], [40.712967, -73.97749], [40.712957, -73.97749], [40.712818, -73.97757], [40.712571, -73.97769], [40.712325, -73.97784], [40.712276, -73.97787], [40.712227, -73.9779], [40.712138, -73.97799], [40.711932, -73.97816], [40.711755, -73.978372], [40.711608, -73.978573], [40.711461, -73.978815], [40.711441, -73.978845], [40.711274, -73.979179], [40.711157, -73.979463], [40.711137, -73.979534], [40.711068, -73.979757], [40.711039, -73.979899], [40.711009, -73.98003], [40.710999, -73.980101], [40.71096, -73.980294], [40.71096, -73.980304], [40.710931, -73.980588], [40.710921, -73.980699], [40.710881, -73.981306], [40.710862, -73.981618], [40.710832, -73.98202], [40.710832, -73.98205], [40.710793, -73.98256], [40.710783, -73.982719], [40.710763, -73.982958], [40.710714, -73.98376], [40.710665, -73.98449], [40.710645, -73.984677], [40.710626, -73.984875], [40.710527, -73.985764], [40.710507, -73.985873], [40.710468, -73.986231], [40.710458, -73.98635], [40.710438, -73.98646], [40.710419, -73.98669], [40.710281, -73.987845], [40.710281, -73.987876], [40.710251, -73.988108], [40.710231, -73.988351], [40.710122, -73.989254], [40.710093, -73.989497], [40.710063, -73.98975], [40.709994, -73.990215], [40.709964, -73.990457], [40.709944, -73.990618], [40.709924, -73.990689], [40.709895, -73.99092], [40.709865, -73.99116], [40.709825, -73.99139], [40.709815, -73.99147], [40.709775, -73.99171], [40.709746, -73.991918], [40.709746, -73.991938], [40.709706, -73.992167], [40.709686, -73.992355], [40.709666, -73.992454], [40.709646, -73.992592], [40.709606, -73.992819], [40.709596, -73.992918], [40.709556, -73.993145], [40.709537, -73.993283], [40.709527, -73.993372], [40.709497, -73.993559], [40.709497, -73.993608], [40.709467, -73.993835], [40.709437, -73.994072], [40.709427, -73.994181], [40.709387, -73.994507], [40.709337, -73.994824], [40.709297, -73.995161], [40.709247, -73.99549], [40.709207, -73.99578], [40.709187, -73.99594], [40.709177, -73.99601], [40.709147, -73.996201], [40.709127, -73.996382], [40.709067, -73.996815], [40.709057, -73.996855], [40.709027, -73.997088], [40.709027, -73.997128], [40.708996, -73.997321], [40.708976, -73.997442], [40.708956, -73.997544], [40.708956, -73.997564], [40.708916, -73.997726], [40.708916, -73.997736], [40.708856, -73.997959], [40.708856, -73.99797], [40.708806, -73.998152], [40.708765, -73.998264], [40.708735, -73.998365], [40.708725, -73.998406], [40.708695, -73.998487], [40.708644, -73.998598], [40.708604, -73.99871], [40.708564, -73.998821], [40.708554, -73.998851], [40.708533, -73.998952], [40.708523, -73.999023], [40.708513, -73.999074], [40.708513, -73.999175], [40.708513, -73.999276], [40.708523, -73.999356], [40.708533, -73.999437], [40.708544, -73.999498], [40.708564, -73.999588], [40.708634, -73.99999], [40.708665, -74.0001], [40.708665, -74.00012], [40.708675, -74.00015], [40.708705, -74.00023], [40.708715, -74.00027], [40.708735, -74.00032], [40.708745, -74.00037], [40.708755, -74.0004], [40.708795, -74.00048], [40.708816, -74.00051], [40.708826, -74.00053], [40.708886, -74.00063], [40.708926, -74.00069], [40.708946, -74.000739], [40.709007, -74.000869], [40.709107, -74.000998], [40.709207, -74.001127], [40.709287, -74.001236], [40.709377, -74.001355], [40.709437, -74.001434], [40.709566, -74.001602], [40.709606, -74.001661], [40.709676, -74.00174], [40.709934, -74.002076], [40.710112, -74.002293], [40.710419, -74.002687], [40.710468, -74.002756], [40.710517, -74.002815], [40.710557, -74.002875], [40.710645, -74.002983], [40.710675, -74.003023], [40.710714, -74.003082], [40.710744, -74.003131], [40.710773, -74.003171], [40.710862, -74.003309], [40.710881, -74.003378], [40.710901, -74.003477], [40.710911, -74.003536], [40.710921, -74.003586], [40.710931, -74.003635], [40.710931, -74.003685], [40.71094, -74.003764], [40.71095, -74.003784], [40.71096, -74.003853], [40.71097, -74.003933], [40.71097, -74.003992], [40.71098, -74.004022], [40.71099, -74.004111], [40.710999, -74.004171], [40.711019, -74.004221], [40.711039, -74.004241], [40.711039, -74.004251], [40.711068, -74.00429], [40.711098, -74.00432], [40.711127, -74.00434], [40.711166, -74.00435], [40.711196, -74.00436], [40.711225, -74.00436], [40.711265, -74.00436], [40.711294, -74.00435], [40.711323, -74.00434], [40.711353, -74.00432], [40.711392, -74.00429], [40.711402, -74.004271], [40.711431, -74.004241], [40.711451, -74.004181], [40.711461, -74.004151], [40.711471, -74.004131], [40.711471, -74.004101], [40.71148, -74.004072], [40.71148, -74.004042], [40.71148, -74.004012], [40.71148, -74.003982], [40.711471, -74.003952], [40.711461, -74.003903], [40.711441, -74.003863], [40.711431, -74.003824], [40.711392, -74.003764], [40.711353, -74.003705], [40.711304, -74.003635], [40.711265, -74.003586], [40.711235, -74.003546], [40.711206, -74.003497], [40.711176, -74.003457], [40.711127, -74.003388], [40.711088, -74.003319], [40.710881, -74.003023], [40.710793, -74.002894], [40.710675, -74.002717], [40.710419, -74.002392], [40.71029, -74.002234], [40.709904, -74.00175], [40.709736, -74.001543], [40.709566, -74.001335], [40.709377, -74.001097], [40.709347, -74.001057], [40.709117, -74.000769], [40.709017, -74.00064], [40.708906, -74.00051], [40.708806, -74.00038], [40.708685, -74.00023], [40.708655, -74.00019], [40.708614, -74.00015], [40.708554, -74.00007], [40.708473, -73.99998], [40.708362, -73.99984], [40.70825, -73.999689], [40.708119, -73.999518], [40.707967, -73.999326], [40.707896, -73.999245], [40.707824, -73.999155], [40.707723, -73.999023], [40.707631, -73.998922], [40.707275, -73.998466], [40.707132, -73.998304], [40.70643, -73.997422], [40.706297, -73.99725], [40.706023, -73.996906]]
",1,1
"I enter GPS coordinates to determine if the location is on a body of water (such as a lake, river, or ocean) and provide information about the nearest city so I know the geographic location.

","import requests

# API Key and Headers
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""geocodeapi.p.rapidapi.com""
}

def is_on_water(lat, lon):
    url = ""https://geocodeapi.p.rapidapi.com/isonwater""
    querystring = {""lat"": str(lat), ""lon"": str(lon)}
    response = requests.get(url, headers=HEADERS, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching water information: {response.status_code}"")
        return None

def get_nearest_cities(latitude, longitude, range_meters):
    url = ""https://geocodeapi.p.rapidapi.com/GetNearestCities""
    querystring = {""latitude"": str(latitude), ""longitude"": str(longitude), ""range"": str(range_meters)}
    response = requests.get(url, headers=HEADERS, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching nearest cities information: {response.status_code}"")
        return None

def get_timezone(latitude, longitude):
    url = ""https://geocodeapi.p.rapidapi.com/GetTimezone""
    querystring = {""latitude"": str(latitude), ""longitude"": str(longitude)}
    response = requests.get(url, headers=HEADERS, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching timezone information: {response.status_code}"")
        return None

if __name__ == '__main__':
    # é»˜è®¤åæ ‡ï¼ˆçº½çº¦å¸‚ä¸­å¿ƒï¼‰
    latitude = 40.7128
    longitude = -74.0060

    # æ£€æŸ¥è¯¥ä½ç½®æ˜¯å¦åœ¨æ°´ä¸Š
    water_check = is_on_water(latitude, longitude)
    if water_check and 'isOnWater' in water_check:
        if water_check['isOnWater']:
            water_type = ""sea"" if water_check.get('sea') else ""lake"" if water_check.get('lake') else ""other body of water""
            print(f""The location is on a {water_type}."")
        else:
            print(""The location is on land."")
    else:
        print(""Unable to determine if the location is on water."")

    # èŽ·å–æœ€è¿‘åŸŽå¸‚ä¿¡æ¯
    nearest_cities_info = get_nearest_cities(latitude, longitude, range_meters=50000)  # 50 å…¬é‡ŒåŠå¾„
    if nearest_cities_info:
        print(""\nNearest cities within 50 km:"")
        for city in nearest_cities_info:
            print(f""- {city.get('name', 'Unknown City')} ({city.get('country', 'N/A')}), Distance: {city.get('distance', 'N/A')} meters"")
    else:
        print(""No nearest cities information available."")

    # èŽ·å–è¯¥ä½ç½®çš„æ—¶åŒºä¿¡æ¯
    timezone_info = get_timezone(latitude, longitude)
    if timezone_info:
        print(""\nTimezone information:"")
        print(f""Time Zone ID: {timezone_info.get('TimezoneId', 'N/A')}"")
        print(f""Local Time Now: {timezone_info.get('LocalTime_Now', 'N/A')}"")
    else:
        print(""No timezone information available."")
","The location is on land.

Nearest cities within 50 km:
- Unknown City (N/A), Distance: N/A meters
- Unknown City (N/A), Distance: N/A meters
- Unknown City (N/A), Distance: N/A meters
- Unknown City (N/A), Distance: N/A meters
- Unknown City (N/A), Distance: N/A meters

Timezone information:
Time Zone ID: N/A
Local Time Now: 10:00:08 AM",1,1
"I enter the geographic coordinates of multiple locations, and the system determines if they're in the same time zone and provides specific time zone information for each location to plan events or communications across regions.","import requests

# API Key and Headers
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""geocodeapi.p.rapidapi.com""
}

def get_timezone(latitude, longitude):
    url = ""https://geocodeapi.p.rapidapi.com/GetTimezone""
    querystring = {""latitude"": latitude, ""longitude"": longitude}
    response = requests.get(url, headers=HEADERS, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching timezone for ({latitude}, {longitude}): {response.status_code}"")
        return None

def check_timezones(locations):
    # èŽ·å–æ¯ä¸ªä½ç½®çš„æ—¶åŒºæ•°æ®
    timezone_data = []
    for lat, lon in locations:
        tz_info = get_timezone(lat, lon)
        if tz_info:
            timezone_data.append({
                ""latitude"": lat,
                ""longitude"": lon,
                ""TimeZoneId"": tz_info.get(""TimeZoneId"", ""N/A""),
                ""LocalTime_Now"": tz_info.get(""LocalTime_Now"", ""N/A""),
                ""GMT_offset"": tz_info.get(""GMT_offset"", ""N/A"")
            })

    # èŽ·å–å”¯ä¸€çš„æ—¶åŒº ID
    unique_timezones = set([tz[""TimeZoneId""] for tz in timezone_data])

    if len(unique_timezones) == 1:
        print(""All locations are in the same time zone:"", timezone_data[0][""TimeZoneId""])
    else:
        print(""Locations are in different time zones:"")
        for tz in timezone_data:
            print(f""Location ({tz['latitude']}, {tz['longitude']}):"")
            print(f"" - Time Zone: {tz['TimeZoneId']}"")
            print(f"" - Local Time: {tz['LocalTime_Now']}"")
            print(f"" - GMT Offset: {tz['GMT_offset']}"")
            print()

if __name__ == '__main__':
    # ç¤ºä¾‹ä½ç½®çš„åæ ‡
    locations = [
        (40.712776, -74.005974),  # New York, USA
        (34.052235, -118.243683), # Los Angeles, USA
        (51.507351, -0.127758)    # London, UK
    ]
    check_timezones(locations)
","Locations are in different time zones:
Location (40.712776, -74.005974):
 - Time Zone: America/New_York
 - Local Time: 10:02:11 AM
 - GMT Offset: -5

Location (34.052235, -118.243683):
 - Time Zone: America/Los_Angeles
 - Local Time: 7:02:12 AM
 - GMT Offset: -8

Location (51.507351, -0.127758):
 - Time Zone: Europe/London
 - Local Time: 3:02:13 PM
 - GMT Offset: 0",1,1
"I enter two addresses, convert them to geographic coordinates, and calculate the straight-line distance between them for distance measurement and navigation.

","import requests

# API Key and Headers
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""forward-reverse-geocoding.p.rapidapi.com""
}

def forward_geocode(address):
    url = ""https://forward-reverse-geocoding.p.rapidapi.com/v1/forward""
    querystring = {
        ""format"": ""json"",
        ""street"": address.get(""street"", """"),
        ""city"": address.get(""city"", """"),
        ""state"": address.get(""state"", """"),
        ""postalcode"": address.get(""postalcode"", """"),
        ""country"": address.get(""country"", """"),
        ""addressdetails"": ""1"",
        ""limit"": ""1""
    }
    response = requests.get(url, headers=HEADERS, params=querystring)

    if response.status_code == 200 and response.json():
        data = response.json()[0]
        return float(data[""lat""]), float(data[""lon""])
    else:
        print(f""Error fetching coordinates for address: {address}"")
        return None, None

def get_distance(lat1, lon1, lat2, lon2):
    url = ""https://geocodeapi.p.rapidapi.com/GetDistance""
    querystring = {
        ""lat1"": str(lat1),
        ""lon1"": str(lon1),
        ""lat2"": str(lat2),
        ""lon2"": str(lon2)
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""geocodeapi.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error calculating distance: {response.status_code}"")
        return None

if __name__ == '__main__':
    # å®šä¹‰ä¸¤ä¸ªåœ°å€
    address1 = {
        ""street"": ""34 West 13th Street"",
        ""city"": ""New York City"",
        ""state"": ""NY"",
        ""postalcode"": ""10011"",
        ""country"": ""USA""
    }
    address2 = {
        ""street"": ""1600 Amphitheatre Parkway"",
        ""city"": ""Mountain View"",
        ""state"": ""CA"",
        ""postalcode"": ""94043"",
        ""country"": ""USA""
    }

    # èŽ·å–ç¬¬ä¸€ä¸ªåœ°å€çš„åæ ‡
    lat1, lon1 = forward_geocode(address1)
    # èŽ·å–ç¬¬äºŒä¸ªåœ°å€çš„åæ ‡
    lat2, lon2 = forward_geocode(address2)

    if lat1 is not None and lat2 is not None:
        # è®¡ç®—ä¸¤ç‚¹ä¹‹é—´çš„è·ç¦»
        distance_info = get_distance(lat1, lon1, lat2, lon2)

        # æå–å¹¶æ˜¾ç¤ºå¤šç§è·ç¦»å•ä½
        if distance_info and 'Distance' in distance_info:
            print(f""Distance between the addresses:"")
            print(f""- Meters: {distance_info['Distance']} meters"")
            print(f""- Kilometers: {distance_info['DistanceInKm']} km"")
            print(f""- Miles: {distance_info['DistanceInMiles']} miles"")
        else:
            print(""Failed to retrieve distance information or 'Distance' key not found in response."")
    else:
        print(""Unable to retrieve coordinates for one or both addresses."")
","Distance between the addresses:
- Meters: 4118914.760087681 meters
- Kilometers: 4118.914760087681 km
- Miles: 2559.3749751996347 miles",1,1
"I hope that by entering a keyword or song name, the system will return ten related songs and provide similar song recommendations to expand the music playlist.

","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def search_song(query, limit=10):
    url = ""https://shazam-api7.p.rapidapi.com/search""
    querystring = {""term"": query, ""limit"": str(limit)}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""shazam-api7.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)

    # Debugging output to check full API response
    data = response.json()
    print(""Debug: Full API Response for search:"", data)

    if response.status_code == 200:
        return data
    else:
        print(f""Error searching song: {response.status_code}"")
        return None

if __name__ == '__main__':
    # Use default song name ""Blinding Lights"" for testing
    song_query = ""Blinding Lights""
    search_results = search_song(song_query)

    # Verify and parse the results
    if search_results and 'tracks' in search_results['data'] and 'hits' in search_results['data']['tracks']:
        for idx, track in enumerate(search_results['data']['tracks']['hits'][:10], start=1):
            title = track['heading']['title']
            artist = track['heading']['subtitle']
            print(f""{idx}. {title} by {artist}"")
    else:
        print(""No songs found for the provided query."")
","1. Blinding Lights by The Weeknd
2. Blinding Lights by Main-de-Gloire & Î›EVE
3. Blinding Lights by Teddy Swims
4. Blinding Lights by Jack Savoretti
5. Blinding Lights by KIDZ BOP Kids
6. Blinding Lights by Loi
7. Blinding Lights (Remix) by The Weeknd & ROSALÃA
8. Blinding Lights (Country Version) by Tebey
",1,1
"I want to know the details of an album and see all the tracks in the album and their playback data (e.g. number of plays, duration, etc.).

","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def get_album_details(album_id):
    """"""Retrieve and format album details.""""""
    url = ""https://spotify23.p.rapidapi.com/albums/""
    querystring = {""ids"": album_id}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""spotify23.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    data = response.json()

    album_data = data['albums'][0]
    name = album_data['name']
    artists = "", "".join([artist['name'] for artist in album_data['artists']])
    release_date = album_data['release_date']
    total_tracks = album_data['total_tracks']
    label = album_data['label']

    print(f""Album Name: {name}"")
    print(f""Artist(s): {artists}"")
    print(f""Release Date: {release_date}"")
    print(f""Total Tracks: {total_tracks}"")
    print(f""Label: {label}"")
    print()


def get_album_tracks(album_id):
    """"""Retrieve and format track details for a specific album.""""""
    url = ""https://spotify23.p.rapidapi.com/album_tracks/""
    querystring = {""id"": album_id, ""offset"": ""0"", ""limit"": ""50""}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""spotify23.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    data = response.json()

    # æ£€æŸ¥ 'items' é”®æ˜¯å¦å­˜åœ¨äºŽæ•°æ®ä¸­
    if 'items' not in data:
        print(""No track data available for this album."")
        return

    print(""Track List:"")
    for index, track in enumerate(data['items'], start=1):
        track_name = track.get('name', 'Unknown Track')
        duration_ms = track.get('duration_ms', 0)
        duration_sec = duration_ms // 1000
        print(f""{index}. {track_name} - Duration: {duration_sec} seconds"")
    print()


def get_album_metadata(album_id):
    """"""Retrieve and format metadata for a specific album.""""""
    url = ""https://spotify23.p.rapidapi.com/album_metadata/""
    querystring = {""id"": album_id}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""spotify23.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    data = response.json()

    album_data = data['data']['album']
    cover_art_url = album_data['coverArt']['sources'][0]['url']
    spotify_url = album_data['sharingInfo']['shareUrl']

    print(""Additional Metadata:"")
    print(f""Cover Art URL: {cover_art_url}"")
    print(f""Spotify Link: {spotify_url}"")
    print()


if __name__ == '__main__':
    album_id = ""3IBcauSj5M2A6lTeffJzdv""  # example album ID
    get_album_details(album_id)
    get_album_tracks(album_id)
    get_album_metadata(album_id)
","Album Name: Kan
Artist(s): UZI
Release Date: 2021-03-19
Total Tracks: 10
Label: M.O.B. Entertainment

No track data available for this album.
Additional Metadata:
Cover Art URL: https://i.scdn.co/image/ab67616d00001e0267c738a703dc979f5c3c52ef
Spotify Link: https://open.spotify.com/album/3IBcauSj5M2A6lTeffJzdv?si=OCZfOkEpRp6sx5C7sin6uQ",1,1
"I want to get a list of news by topic and click on the first one to see the full content of the article, the system needs to display news by popularity and provide detailed pages for different topics.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

# Fetch trending news topics
def get_news_topics():
    url = ""https://biztoc.p.rapidapi.com/news/topics""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""biztoc.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        return response.json()
    else:
        print(""Error fetching news topics:"", response.status_code)
        return []

# Get detailed articles sorted by popularity/heat
def get_articles_by_topic(topic, page_num=1, limit=10):
    url = f""https://climate-news-feed.p.rapidapi.com/page/{page_num}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""climate-news-feed.p.rapidapi.com""
    }
    querystring = {""limit"": str(limit), ""topic"": topic}
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        try:
            articles = response.json()
            if isinstance(articles, list):
                return sorted(articles, key=lambda x: x.get('heat', 0), reverse=True)  # Sort articles by 'heat' (popularity)
            else:
                print(""Unexpected response format: expected a list of articles."")
                return []
        except json.JSONDecodeError:
            print(""Error decoding JSON response."")
            return []
    else:
        print(""Error fetching articles:"", response.status_code)
        return []

# View the complete content of an article
def get_article_details(article_id):
    url = f""https://climate-news-feed.p.rapidapi.com/articles/{article_id}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""climate-news-feed.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        article = response.json()
        if 'title' in article and 'body' in article:
            return {
                'title': article['title'],
                'body': article['body']
            }
        else:
            print(""Unexpected response format: missing 'title' or 'body'."")
            return {}
    else:
        print(""Error fetching article details:"", response.status_code)
        return {}

if __name__ == '__main__':
    # Step 1: Fetch trending news topics
    news_topics = get_news_topics()
    if news_topics:
        print(""Trending News Topics:"")
        for idx, topic in enumerate(news_topics):
            print(f""{idx + 1}. {topic}"")

        # Step 2: Choose a topic to fetch articles
        selected_topic = news_topics[0]  # For simplicity, we select the first topic
        print(f""\nFetching articles for topic: {selected_topic}\n"")
        articles = get_articles_by_topic(selected_topic)

        if articles:
            for idx, article in enumerate(articles):
                print(f""{idx + 1}. {article['title']} (Heat: {article.get('heat', 'N/A')})"")

            # Step 3: Click on the first article to view details
            first_article_id = articles[0]['id']
            article_details = get_article_details(first_article_id)
            if article_details:
                print(""\nFirst Article Details:"")
                print(f""Title: {article_details['title']}\n"")
                print(f""Body: {article_details['body']}\n"")
        else:
            print(""No articles found for the selected topic."")
    else:
        print(""No trending topics found."")","Trending News Topics:
1. {'stories': [{'body_preview': '', 'domain': 'barrons.com', 'id': '1d75e4eb9f06825e', 'img': {'s': '1d75e4eb9f06825e_s.webp', 'sq': '1d75e4eb9f06825e_sq.webp'}, 'published': 'Sat, 26 Oct 2024 06:00:00 GMT', 'title': 'Boeing Workers Hate Their 401(k)s. They Have a Point', 'topic': ['boeing', 'spirit'], 'url': 'https://www.barrons.com/articles/boeing-workers-401k-pensions-cad43b34'}, {'body_preview': '', 'domain': 'reuters.com', 'id': '139086abaf85da7f', 'img': None, 'published': 'Fri, 25 Oct 2024 23:49:22 GMT', 'title': 'Delta sues CrowdStrike, saying the July 19 outage caused mass flight cancellations, disrupted travel plans of 1.3M customers, and cost the carrier over $500M', 'topic': ['boeing', 'spirit'], 'url': 'https://www.reuters.com/legal/delta-sues-crowdstrike-over-software-update-that-prompted-mass-flight-2024-10-25/'}, {'body_preview': 'Boeing workers rejected a contract offer this week and refused to end their six-week strike, digging in their heels over a demand the embattled planemaker calls a non-starter: bringing back traditional pensions.\nThe company discontinued guaranteed retirement payments starting in 2014, when workersâ€¦', 'domain': 'morningbrew.com', 'id': '546d5c5fbd94ae98', 'img': {'s': '546d5c5fbd94ae98_s.webp', 'sq': '546d5c5fbd94ae98_sq.webp'}, 'published': 'Fri, 25 Oct 2024 20:59:00 GMT', 'title': 'Boeing strikersâ€™ demand for pensions is a throwback', 'topic': ['boeing', 'spirit'], 'url': 'https://www.morningbrew.com/daily/stories/2024/10/25/boeing-strikers-demand-for-pensions-is-a-throwback?utm_source='}, {'body_preview': 'A sale would include the troubled Starliner space vehicle and operations supporting the International Space Station, the Wall Street Journal reported.', 'domain': 'marketwatch.com', 'id': 'f514dbb476eac96a', 'img': None, 'published': 'Fri, 25 Oct 2024 18:00:00 GMT', 'title': 'Boeing considers selling storied space business, WSJ reports', 'topic': ['boeing', 'spirit'], 'url': 'https://www.marketwatch.com/story/boeing-considers-selling-storied-space-business-wsj-reports-9149dccd'}, {'body_preview': ""New Boeing CEO May Clean House With Sale Of Space Unit\n \nBoeing is unlikely to recover from its botched Starliner spacecraft mission to the International Space Station this past summer, leaving two astronauts stranded on the ISS that Elon Musk's SpaceX has to rescue with a Crew Dragon spacecraftâ€¦"", 'domain': 'zerohedge.com', 'id': '8e72c810abbe3158', 'img': {'s': '8e72c810abbe3158_s.webp', 'sq': '8e72c810abbe3158_sq.webp'}, 'published': 'Fri, 25 Oct 2024 17:25:00 GMT', 'title': 'New Boeing CEO May Clean House With Sale Of Space Unit', 'topic': ['boeing', 'spirit'], 'url': 'https://www.zerohedge.com/technology/new-boeing-ceo-may-clean-house-sale-space-unit'}, {'body_preview': 'Striking workers will take at least a 44,000-job bite out of the October jobs report, according to new data released Friday.\nThatâ€™s the biggest monthly employment knock in a year, according to the Bureau of Labor Statisticsâ€™ latest strike report.\nThe largest chunk of that total comes from theâ€¦', 'domain': 'edition.cnn.com', 'id': '779e945060ad8a2e', 'img': {'s': '779e945060ad8a2e_s.webp', 'sq': '779e945060ad8a2e_sq.webp'}, 'published': 'Fri, 25 Oct 2024 15:41:27 GMT', 'title': 'Boeing and other strikes will take at least a 44,000-job bite out of the October jobs report', 'topic': ['boeing', 'spirit'], 'url': 'https://edition.cnn.com/2024/10/25/economy/bls-strike-report-boeing/index.html'}, {'body_preview': ""Boeing union to striking workers: 'Bullying' each other over strike votes is 'vile' Yahoo! Voices\nDoes that 2% pay raise mean federal employees are chumps? Federal News Network\nAs Machinists strike extends, Boeing is running out of runway The Seattle Times\nStriking Boeing workers make unexpectedâ€¦"", 'domain': 'news.google.com', 'id': '2caf030058cb477c', 'img': None, 'published': 'Fri, 25 Oct 2024 14:24:07 GMT', 'title': ""Boeing union to striking workers: 'Bullying' each other over strike votes is 'vile'"", 'topic': ['boeing', 'spirit'], 'url': 'https://news.google.com/rss/articles/CBMihwFBVV95cUxObWNodURibmJGeUhmU1BjVXRrQ0ZqX1NXZm9YbVNIa2RjdFdzX0RiS1I1cEZQM281OFhoSWNQZVF3cnJtQlZoTUc0MjR1WUJ6V3JHN3FkWS14dDRXanloM0dQcWp0NzF1T25pTXdYX2ZHQmNGLUQ4X1pFM1ZkQzVQWkRWZEs5RXM?oc=5'}, {'body_preview': '', 'domain': 'finance.yahoo.com', 'id': 'fe134bf864ff8750', 'img': None, 'published': 'Fri, 25 Oct 2024 13:33:17 GMT', 'title': ""What's Going On With Spirit Airlines Stock Friday?"", 'topic': ['boeing', 'spirit'], 'url': 'https://finance.yahoo.com/news/whats-going-spirit-airlines-stock-133317831.html'}, {'body_preview': 'Boeing machinists want a return to traditional retirement plans, but defined-benefit pensions are rare in the private sector these days.', 'domain': 'washingtonpost.com', 'id': '54fd176bf768efd3', 'img': None, 'published': 'Fri, 25 Oct 2024 13:05:16 GMT', 'title': 'Striking workersâ€™ pension demand reflects longing for a bygone era', 'topic': ['boeing', 'spirit'], 'url': 'https://www.washingtonpost.com/business/2024/10/25/pensions-boeing/'}, {'body_preview': 'Spirit Airlines shares rose after the struggling carrier said it will cut jobs and sell planes.', 'domain': 'cnbc.com', 'id': '15f2a3ed66dc2ba3', 'img': None, 'published': 'Fri, 25 Oct 2024 14:55:43 GMT', 'title': 'Spirit Airlines stock jumps 25% after struggling budget carrier said it will sell planes, cut jobs', 'topic': ['boeing', 'spirit'], 'url': 'https://www.cnbc.com/2024/10/25/spirit-airlines-stock-sell-planes-cut-jobs.html'}], 'topic': ['boeing', 'spirit']}
2. {'stories': [{'body_preview': 'More than 6.1 million malaria cases, and 1,038 deaths, have been recorded in the country this year through the end of September, compared with 4.5 million cases, and 469 deaths, for all of 2023. Worse, cases are likely to soar far higher in the next couple of months because peak malaria season,â€¦', 'domain': 'marginalrevolution.com', 'id': '187daf45bbc49998', 'img': None, 'published': 'Sat, 26 Oct 2024 06:59:35 GMT', 'title': 'Ethiopia fact of the day', 'topic': ['coli', 'mcdonald'], 'url': 'https://marginalrevolution.com/marginalrevolution/2024/10/ethiopia-fact-of-the-day-2.html'}, {'body_preview': ""In our news wrap Friday, more people have been reported sick from the E. coli outbreak tied to McDonald's, the Biden administration is proposing a new path for student loan forgiveness, Hezbollah militants and the Israeli army traded fire across the Lebanese border and Ukrainian Presidentâ€¦"", 'domain': 'pbs.org', 'id': '5eb8e6046991dbe1', 'img': {'s': '5eb8e6046991dbe1_s.webp', 'sq': '5eb8e6046991dbe1_sq.webp'}, 'published': 'Fri, 25 Oct 2024 22:50:35 GMT', 'title': ""News Wrap: More E. coli cases linked to McDonald's outbreak"", 'topic': ['coli', 'mcdonald'], 'url': 'https://www.pbs.org/newshour/show/news-wrap-more-e-coli-cases-linked-to-mcdonalds-outbreak'}, {'body_preview': 'Fast food restaurants are holding onions as the E. coli outbreak likely traces its roots back to the vegetable.', 'domain': 'inc.com', 'id': 'f93cb02fe03c47be', 'img': {'s': 'f93cb02fe03c47be_s.webp', 'sq': 'f93cb02fe03c47be_sq.webp'}, 'published': 'Fri, 25 Oct 2024 21:53:03 GMT', 'title': 'Onion Identified as McDonaldâ€™s E. Coli Culprit', 'topic': ['coli', 'mcdonald'], 'url': 'https://www.inc.com/reuters/onion-identified-as-mcdonalds-e-coli-culprit/90994689'}, {'body_preview': 'A deadly outbreak of E. coli poisoning tied to McDonaldâ€™s Quarter Pounders has expanded, with at least 75 people sick in 13 states, federal health officials said Friday.\nA total of 22 people have now been hospitalized, and two have developed a dangerous kidney disease complication, the Centers forâ€¦', 'domain': 'theglobeandmail.com', 'id': 'bb89656acb195ee6', 'img': None, 'published': 'Fri, 25 Oct 2024 20:09:36 GMT', 'title': 'At least 75 people are sickened as the deadly McDonaldâ€™s E. coli outbreak expands', 'topic': ['coli', 'mcdonald'], 'url': 'https://www.theglobeandmail.com/business/article-at-least-75-people-are-sickened-as-the-deadly-mcdonalds-e-coli/'}, {'body_preview': 'TOKYO -- Watami is taking over the Japan operations of sandwich chain Subway, it announced on Friday, with plans to more than double locations in the country as it looks to transform from an izakaya pub company into a comprehensive restaurant operator.\n""We want to change the Watami brand,"" saidâ€¦', 'domain': 'asia.nikkei.com', 'id': '2ee2ca5c42d837df', 'img': {'s': '2ee2ca5c42d837df_s.webp', 'sq': '2ee2ca5c42d837df_sq.webp'}, 'published': 'Fri, 25 Oct 2024 20:07:39 GMT', 'title': ""Watami acquires Subway's Japan operations in fast-food pivot"", 'topic': ['coli', 'mcdonald'], 'url': 'https://asia.nikkei.com/Business/Food-Beverage/Watami-acquires-Subway-s-Japan-operations-in-fast-food-pivot'}, {'body_preview': ""This New Restaurant Is Banking on One Dish â€” Because It's the Only EntrÃ©e on the Menu The new hotspot is gaining buzz on social media for its innovative yet super simple concept.\nKey Takeaways\nRestaurants in 2024 tend to be filled with Instagramable spots and a robust menu that can appeal to aâ€¦"", 'domain': 'entrepreneur.com', 'id': '527b042f03a80bb3', 'img': {'s': '527b042f03a80bb3_s.webp', 'sq': '527b042f03a80bb3_sq.webp'}, 'published': 'Fri, 25 Oct 2024 19:43:00 GMT', 'title': ""This New Restaurant Is Banking on One Dish â€” Because It's the Only EntrÃ©e on the Menu"", 'topic': ['coli', 'mcdonald'], 'url': 'https://www.entrepreneur.com/business-news/new-miami-restaurant-cotoletta-only-serves-one-entree/481911'}, {'body_preview': ""Costco Has Issued a Recall On a Fan-Favorite Itemâ€”Here's What to Know Yahoo! Voices\nBreaking: Costco Is Recalling Chicken & Salmon Products Over Listeria Concerns Delish\nMajor grocery chain recalls popular food item due to contamination: Do not eat, return ASAP MassLive.com\nVoluntary recall issuedâ€¦"", 'domain': 'news.google.com', 'id': '5a2fc48ba772ad09', 'img': None, 'published': 'Fri, 25 Oct 2024 16:59:22 GMT', 'title': ""Costco Has Issued a Recall On a Fan-Favorite Itemâ€”Here's What to Know"", 'topic': ['coli', 'mcdonald'], 'url': 'https://news.google.com/rss/articles/CBMigAFBVV95cUxORmpxblU3SGp4ZkFFWUVsUnR0cmtXdG9udE1KaDNQTUM0RmFzeEJrRm9HcXp6bXI3R0tGRU91VGU0RlhVMFFfTVhpOHdXeWRWUmhXcmpKbkkwSkhSbk9DRFBZakY0Nk5VbGlkTjhSWGpsXzhzUW9rQzQ1SXdhQV9QVA?oc=5'}, {'body_preview': 'Federal health officials announced Thursday that a Missouri resident infected with bird flu did not spread the virus to others, alleviating concerns about potential human-to-human transmission during the current nationwide outbreak affecting poultry and dairy farms.\nThe Missouri case had raisedâ€¦', 'domain': 'newsweek.com', 'id': 'f238bd2759844071', 'img': {'s': 'f238bd2759844071_s.webp', 'sq': 'f238bd2759844071_sq.webp'}, 'published': 'Fri, 25 Oct 2024 16:16:48 GMT', 'title': 'Bird Flu Cases Not Spreading Between Humans, Missouri Tests Show', 'topic': ['coli', 'mcdonald'], 'url': 'https://www.newsweek.com/bird-flu-cases-not-spreading-between-humans-missouri-1974983'}, {'body_preview': 'Update by Food and Drug Administration signals escalation in food poisoning outbreak affecting US west and midwest\nWhat is E Coli and how can you avoid getting it?\nThe US Food and Drug Administration said on Friday that 75 people had now been infected by the E coli outbreak linked to McDonaldâ€™sâ€¦', 'domain': 'theguardian.com', 'id': 'b0370823f436e382', 'img': {'s': 'b0370823f436e382_s.webp', 'sq': 'b0370823f436e382_sq.webp'}, 'published': 'Fri, 25 Oct 2024 15:56:23 GMT', 'title': 'Seventy-five infected as cases rise in US E coli outbreak linked to McDonaldâ€™s', 'topic': ['coli', 'mcdonald'], 'url': 'https://www.theguardian.com/business/2024/oct/25/mcdonalds-e-coli-outbreak'}, {'body_preview': ""A deadly outbreak of E. coli poisoning tied to McDonald's Quarter Pounders has expanded, with at least 75 people sick in 13 states, federal health officials said Friday.\nTwenty-two people have been hospitalized, the Centers for Disease Control and Prevention said. Two people developed a dangerousâ€¦"", 'domain': 'apnews.com', 'id': '55c1e5535aa940bb', 'img': {'s': '55c1e5535aa940bb_s.webp', 'sq': '55c1e5535aa940bb_sq.webp'}, 'published': 'Fri, 25 Oct 2024 15:49:52 GMT', 'title': ""At least 75 sickened as deadly McDonald's E. coli outbreak expands"", 'topic': ['coli', 'mcdonald'], 'url': 'https://apnews.com/article/mcdonalds-e-coli-outbreak-quarter-pounder-a1ec3dc14636f5e713b15b32aedcd524'}], 'topic': ['coli', 'mcdonald']}
3. {'stories': [{'body_preview': '', 'domain': 'barrons.com', 'id': '0c6bf3169febfb11', 'img': {'s': '0c6bf3169febfb11_s.webp', 'sq': '0c6bf3169febfb11_sq.webp'}, 'published': 'Sat, 26 Oct 2024 00:09:00 GMT', 'title': 'Israeli Retaliation on Iran Begins', 'topic': ['israel', 'iran'], 'url': 'https://www.barrons.com/articles/israel-attack-iran-oil-65242828'}, {'body_preview': '', 'domain': 'bloomberg.com', 'id': 'afdc995b0052230b', 'img': {'s': 'afdc995b0052230b_s.webp', 'sq': 'afdc995b0052230b_sq.webp'}, 'published': 'Fri, 25 Oct 2024 23:35:39 GMT', 'title': 'Israel Says Started Precise Strikes on Military Targets in Iran', 'topic': ['israel', 'iran'], 'url': 'https://www.bloomberg.com/news/articles/2024-10-25/israel-says-started-precise-strikes-on-military-targets-in-iran-m2pdehzz'}, {'body_preview': 'Israel began its attack against Iran early Saturday morning local time in retaliation for its massive ballistic missile attack on Oct. 1, two sources with knowledge told Axios.\nState of play: Iranian state TV reported explosions in several areas of the country, including Tehran.\nThe big picture:â€¦', 'domain': 'axios.com', 'id': 'bf59e064f5e92054', 'img': {'s': 'bf59e064f5e92054_s.webp', 'sq': 'bf59e064f5e92054_sq.webp'}, 'published': 'Fri, 25 Oct 2024 23:17:22 GMT', 'title': 'Israel launches retaliatory attack against Iran', 'topic': ['israel', 'iran'], 'url': 'https://www.axios.com/2024/10/25/israel-attacks-iran-retaliation'}, {'body_preview': ""There are reports of explosions in Iran's capital believed to be an Israeli strike in response to Iran's ballistic missile attack earlier this month. Israeli and U.S. officials have indicated that Israel was expected to strike Iranian intelligence and military targets, and not Iranian nuclear andâ€¦"", 'domain': 'pbs.org', 'id': '799fa445422acdbd', 'img': {'s': '799fa445422acdbd_s.webp', 'sq': '799fa445422acdbd_sq.webp'}, 'published': 'Fri, 25 Oct 2024 23:15:01 GMT', 'title': 'Israel launches retaliatory strikes on military targets in Iran', 'topic': ['israel', 'iran'], 'url': 'https://www.pbs.org/newshour/show/israel-launches-retaliatory-strikes-on-military-targets-in-iran'}, {'body_preview': 'Israeli military said it targeted military sites after explosions were heard in Tehran', 'domain': 'ft.com', 'id': 'c430b5d82b987cc6', 'img': {'s': 'c430b5d82b987cc6_s.webp', 'sq': 'c430b5d82b987cc6_sq.webp'}, 'published': 'Sat, 26 Oct 2024 00:02:44 GMT', 'title': 'Israel launches retaliatory strike on Iran', 'topic': ['israel', 'iran'], 'url': 'https://www.ft.com/content/d2bf147d-4cbc-4022-bdf8-e4386cc5be14'}, {'body_preview': 'Israel is conducting retaliatory attacks in Iran. These retaliatory strikes are in response to previous attacks from Iran, according to the IDF.', 'domain': 'nbcnews.com', 'id': '9f0cfaf4080391ef', 'img': {'s': '9f0cfaf4080391ef_s.webp', 'sq': '9f0cfaf4080391ef_sq.webp'}, 'published': 'Fri, 25 Oct 2024 23:55:43 GMT', 'title': 'Special Report: Israel launches retaliatory strikes on Iran', 'topic': ['israel', 'iran'], 'url': 'https://www.nbcnews.com/video/special-report-israel-launches-retaliatory-strikes-on-iran-222736453969'}], 'topic': ['israel', 'iran']}
4. {'stories': [{'body_preview': 'The Biden administration believes that some North Korean troops are on their way to the Kursk region to help Russia fend off Ukrainian forces who mounted an offensive there in August, according to a U.S. official.', 'domain': 'wsj.com', 'id': '21e5ec13379df038', 'img': None, 'published': 'Fri, 25 Oct 2024 19:02:00 GMT', 'title': ""U.S. Says North Korean Troops Heading to Russia's Kursk Region"", 'topic': ['russia', 'ukraine'], 'url': 'https://www.wsj.com/articles/u-s-says-north-korean-troops-heading-to-russias-kursk-region-f36312db'}, {'body_preview': ""Russian President Vladimir Putin and North Korean leader Kim Jong Un toast in June 2024, during Putin's first visit to North Korea in 24 years.\nVLADIMIR SMIRNOV/POOL/AFP via Getty Images\nNorth Korea is solidifying ties with Russia, a change from its usual focus on China.\nAnalysts told Businessâ€¦"", 'domain': 'businessinsider.com', 'id': '96b530917092fcd6', 'img': None, 'published': 'Fri, 25 Oct 2024 15:32:51 GMT', 'title': 'North Korea is cozying up to Russia. For China, that may be no bad thing', 'topic': ['russia', 'ukraine'], 'url': 'https://www.businessinsider.com/north-korea-cozying-russia-suits-china-xi-putin-kim-alliance-2024-10'}, {'body_preview': ""Russia's central bank raises interest rate to 21% to fight inflation boosted by military spending The Associated Press\nRussia Raises Interest Rate to 21 Percent, Its Highest in Decades The New York Times\nRussia's Nabiullina on raising rates to 21% Reuters\nRussian arms group warns it may haltâ€¦"", 'domain': 'news.google.com', 'id': '1b4d005163d14481', 'img': None, 'published': 'Fri, 25 Oct 2024 15:30:31 GMT', 'title': ""Russia's central bank raises interest rate to 21% to fight inflation boosted by military spending"", 'topic': ['russia', 'ukraine'], 'url': 'https://news.google.com/rss/articles/CBMiiAFBVV95cUxQbVFFLXBoTmJyejZFOWdKaVNTZHpXQUxVTFdKWElYRUgxT1UxNm1RRElETFdfckh0X21VbGZUZXIteXFGcVRWMjQwWi1HR0xTdTA2c2IzWXdBbHpKZUVQTGVrSkpvZWVrNWl2MW1PM1M1RWd3QU9pTW05YWx5UWlkbkVyV18wa1VM?ceid=US%3Aen&gl=US&hl=en-US&oc=5'}, {'body_preview': 'North Koreaâ€™s Kim Jong-un has realised that Russiaâ€™s invasion of Ukraine could bring him benefits.', 'domain': 'euractiv.com', 'id': 'fbefa79813c0f38a', 'img': None, 'published': 'Fri, 25 Oct 2024 14:49:39 GMT', 'title': 'The Brief â€“ Putinâ€™s North Korea experiment', 'topic': ['russia', 'ukraine'], 'url': 'https://www.euractiv.com/section/politics/opinion/the-brief-putins-north-korea-experiment/'}, {'body_preview': 'Military spending and recruitment are causing the countryâ€™s economy to overheat, leaving regulators in a struggle to rein in rising prices.', 'domain': 'nytimes.com', 'id': 'a8ac97ce9a7460e8', 'img': {'s': 'a8ac97ce9a7460e8_s.webp', 'sq': 'a8ac97ce9a7460e8_sq.webp'}, 'published': 'Fri, 25 Oct 2024 14:43:35 GMT', 'title': 'Russia Raises Interest Rate to 21 Percent, Its Highest in Decades', 'topic': ['russia', 'ukraine'], 'url': 'https://www.nytimes.com/2024/10/25/world/europe/russia-interest-rate.html'}, {'body_preview': 'Ukrainian President Volodymyr Zelensky claimed on Friday that North Korean troops are poised to be deployed by Russia on the battlefield in Ukraine as early as this weekend.\nWestern officials cautioned that this development could intensify the nearly three-year conflict and produce geopoliticalâ€¦', 'domain': 'newsweek.com', 'id': '84441803a3dea5f9', 'img': {'s': '84441803a3dea5f9_s.webp', 'sq': '84441803a3dea5f9_sq.webp'}, 'published': 'Fri, 25 Oct 2024 14:41:52 GMT', 'title': 'Zelensky Says North Korean Troops on Frontlines in Days', 'topic': ['russia', 'ukraine'], 'url': 'https://www.newsweek.com/zelensky-claims-north-korean-soldiers-frontlines-within-days-1975007'}, {'body_preview': ""GOP Chair: US Must Consider 'Direct Military Action' If N.Korean Troops Fight In Ukraine\n \nPerhaps as expected, some hawks in Washington are reacting loudly and angrily to continuing reports which say North Korean troops are in Russia preparing for greater involvement in Ukraine. South Koreanâ€¦"", 'domain': 'zerohedge.com', 'id': '80e86e7d7f5cafe8', 'img': {'s': '80e86e7d7f5cafe8_s.webp', 'sq': '80e86e7d7f5cafe8_sq.webp'}, 'published': 'Fri, 25 Oct 2024 14:35:00 GMT', 'title': ""GOP Chair: US Must Consider 'Direct Military Action' If N.Korean Troops Fight In Ukraine"", 'topic': ['russia', 'ukraine'], 'url': 'https://www.zerohedge.com/geopolitical/gop-chair-us-must-consider-direct-military-action-against-nkorea-if-their-troops-fight'}], 'topic': ['russia', 'ukraine']}
5. {'stories': [{'body_preview': 'Yves here. Many of our very rational readers may feel that taking climate change remediation or containment steps at the local level is too modest an action to make any difference. But there are many reasons to do so. First, it is better to do what one can to be part of the solution rather thanâ€¦', 'domain': 'nakedcapitalism.com', 'id': '67d304264531243a', 'img': {'s': '67d304264531243a_s.webp', 'sq': '67d304264531243a_sq.webp'}, 'published': 'Sat, 26 Oct 2024 05:27:51 GMT', 'title': 'Guide to Climate Action in Your Local Community', 'topic': ['uk', 'climate'], 'url': 'https://www.nakedcapitalism.com/2024/10/guide-to-climate-action-in-your-local-community.html'}, {'body_preview': '', 'domain': 'bloomberg.com', 'id': '925dbf973121d5c1', 'img': {'s': '925dbf973121d5c1_s.webp', 'sq': '925dbf973121d5c1_sq.webp'}, 'published': 'Fri, 25 Oct 2024 15:52:15 GMT', 'title': 'Ailbhe Rea: What Wonâ€™t Be in Next Weekâ€™s UK Budget', 'topic': ['uk', 'climate'], 'url': 'https://www.bloomberg.com/news/newsletters/2024-10-25/ailbhe-rea-what-won-t-be-in-next-week-s-uk-budget'}, {'body_preview': 'A massive funding gap is endangering the future of global climate talks at a time when the world is facing the â€˜catastrophicâ€™ possibility of temperatures warming by more than 3 degrees Celsius by the end of the century.\nThe UN Framework Convention on Climate Change (UNFCCC) â€” the agency leadingâ€¦', 'domain': 'asiafinancial.com', 'id': '5a5a18a6d5b9b1b1', 'img': None, 'published': 'Fri, 25 Oct 2024 14:26:34 GMT', 'title': 'Funding Gap Hurts Climate Talks While World Faces 3.1C Warming', 'topic': ['uk', 'climate'], 'url': 'https://www.asiafinancial.com/funding-gap-hurts-climate-talks-while-world-faces-3-1c-warming'}, {'body_preview': 'The phrase is commonly used by all parties but its meaning has come under scrutiny before next weekâ€™s budget\nIt is a phrase that crops up 21 times in the Labour manifesto, 12 times in the Conservative manifesto, and has been used in the House of Commons more than 3,500 times in the last decade.â€¦', 'domain': 'theguardian.com', 'id': '87ef05c78e2a9b87', 'img': {'s': '87ef05c78e2a9b87_s.webp', 'sq': '87ef05c78e2a9b87_sq.webp'}, 'published': 'Fri, 25 Oct 2024 13:57:33 GMT', 'title': 'â€˜Working peopleâ€™ definition is work in progress for No 10', 'topic': ['uk', 'climate'], 'url': 'https://www.theguardian.com/uk-news/2024/oct/25/working-people-definition-labour-starmer-budget'}, {'body_preview': 'As officials from nearly 200 countries convened in Colombia this week to discuss implementing a global pact to reverse biodiversity loss, and prepare for crunch climate talks in Azerbaijan in November, yet another major environmental threat â€“ plastic pollution â€“ is the focus of a separate UNâ€¦', 'domain': 'euronews.com', 'id': '66a4cbdce7ba1988', 'img': {'s': '66a4cbdce7ba1988_s.webp', 'sq': '66a4cbdce7ba1988_sq.webp'}, 'published': 'Fri, 25 Oct 2024 15:25:18 GMT', 'title': 'Microplastics trigger major concern as EU governments prepare for UN treaty talks', 'topic': ['uk', 'climate'], 'url': 'https://www.euronews.com/green/2024/10/25/microplastics-trigger-major-concern-as-eu-governments-prepare-for-un-treaty-talks'}], 'topic': ['uk', 'climate']}
6. {'stories': [{'body_preview': ""Mark Walter, the principal owner of the Los Angeles Dodgers, and Hal Steinbrenner, the New York Yankees' chairman and one of its owners, are facing off in the 124 World Series.\nKevin Winter/Getty Images; Jim McIsaac/Getty Images\nThe Dodgers and Yankees will face off in the MLB 2024 Worldâ€¦"", 'domain': 'businessinsider.com', 'id': 'ce0c13ea596bb7b4', 'img': {'s': 'ce0c13ea596bb7b4_s.webp', 'sq': 'ce0c13ea596bb7b4_sq.webp'}, 'published': 'Sat, 26 Oct 2024 11:24:01 GMT', 'title': 'Meet the billionaires battling it out in the 2024 World Series', 'topic': ['series', 'yankees'], 'url': 'https://www.businessinsider.com/billionaire-owners-world-series-yankees-dodgers-baseball-mark-walter-steinbrenner-2024-10'}, {'body_preview': ""Dave Roberts had a bold prediction.\nâ€œMore eyeballs are going to be watching this World Series than any other Series in history,â€ the Los Angeles Dodgers manager said.\nMajor League Baseball's dream matchup is the Dodgers and the New York Yankees. Massive markets, expansive fan bases and luminousâ€¦"", 'domain': 'apnews.com', 'id': '3895c198ee8ed9ea', 'img': {'s': '3895c198ee8ed9ea_s.webp', 'sq': '3895c198ee8ed9ea_sq.webp'}, 'published': 'Sat, 26 Oct 2024 10:10:08 GMT', 'title': ""Yankees-Dodgers is MLB's marketing Dream Series"", 'topic': ['series', 'yankees'], 'url': 'https://apnews.com/article/world-series-yankees-dodgers-marketing-17043d088b4f9330cb816f81de0d09c5'}, {'body_preview': 'After doing research for his job as an NBC News anchor, Tom Llamas goes to the backyard of his home in Westchester County, N.Y., to play baseball with his son, Tomas.', 'domain': 'nytimes.com', 'id': 'dabb27aaa72d7b5e', 'img': {'s': 'dabb27aaa72d7b5e_s.webp', 'sq': 'dabb27aaa72d7b5e_sq.webp'}, 'published': 'Sat, 26 Oct 2024 07:00:13 GMT', 'title': 'How Tom Llamas, an NBC Anchor, Spends His Sundays', 'topic': ['series', 'yankees'], 'url': 'https://www.nytimes.com/2024/10/26/nyregion/tom-llamas-nbc.html'}, {'body_preview': 'The World Series, MLBâ€™s biggest showcase, kicks off on Friday with arguably its greatest matchup in a generation: the New York Yankees and Los Angeles Dodgers.', 'domain': 'msnbc.com', 'id': '4fb355d4a1fa3cee', 'img': {'s': '4fb355d4a1fa3cee_s.webp', 'sq': '4fb355d4a1fa3cee_sq.webp'}, 'published': 'Fri, 25 Oct 2024 21:16:09 GMT', 'title': 'This Dodgers-Yankees World Series matchup is everything a baseball fan could want', 'topic': ['series', 'yankees'], 'url': 'https://www.msnbc.com/opinion/msnbc-opinion/dodgers-yankees-world-series-mlb-viewers-dream-rcna177330'}, {'body_preview': ""This Is the Goal-Setting Method Shohei Ohtani Learned in High School That Helped Get Him to the World Series The MLB superstar's success did not come as a surprise to him. He had a plan.\nKey Takeaways\nThe 2024 World Series is Major League Baseball's dream come true: two franchises in the biggestâ€¦"", 'domain': 'entrepreneur.com', 'id': '96343a0f3d834791', 'img': {'s': '96343a0f3d834791_s.webp', 'sq': '96343a0f3d834791_sq.webp'}, 'published': 'Fri, 25 Oct 2024 15:06:49 GMT', 'title': 'This Is the Goal-Setting Method Shohei Ohtani Learned in High School That Helped Get Him to the World Series', 'topic': ['series', 'yankees'], 'url': 'https://www.entrepreneur.com/business-news/inside-shohei-ohtanis-goal-setting-method/481915'}, {'body_preview': 'Week 9 of the 2024 college football season has already delivered one of the biggest upsets in recent memory -- on a Wednesday night, no less!\nUndefeated Liberty traveled to Kennesaw, Georgia, on Wednesday as 27.5-point favorites over Kennesaw State, but lost to the previously winless Owls,â€¦', 'domain': 'newsweek.com', 'id': '5466f107bc19f1ee', 'img': {'s': '5466f107bc19f1ee_s.webp', 'sq': '5466f107bc19f1ee_sq.webp'}, 'published': 'Fri, 25 Oct 2024 15:00:01 GMT', 'title': ""College Football Week 9 Upset Picks: Navy, BYU, LSU Among Tempting 'Dogs"", 'topic': ['series', 'yankees'], 'url': 'https://www.newsweek.com/college-football-week-9-upset-picks-navy-byu-lsu-among-tempting-dogs-1974507'}, {'body_preview': 'EasyBet is a joint-venture with Triplebet that had its gambling licence temporarily suspended in 2020\nStelios Haji-Ioannou has stamped the Easy brand on everything from aeroplanes to pizza joints, with varying degrees of success.\nNow the billionaire entrepreneur is taking a punt on online sportsâ€¦', 'domain': 'theguardian.com', 'id': 'cac5f539095875aa', 'img': {'s': 'cac5f539095875aa_s.webp', 'sq': 'cac5f539095875aa_sq.webp'}, 'published': 'Fri, 25 Oct 2024 14:21:30 GMT', 'title': 'EasyGroup partners with offshore firm in punt on online sports betting', 'topic': ['series', 'yankees'], 'url': 'https://www.theguardian.com/business/2024/oct/25/easygroup-partners-with-offshore-firm-in-punt-on-online-sports-betting'}, {'body_preview': 'This yearâ€™s world series is sure to strengthen the East Coast-versus-West Coast rivalry. The Los Angeles Dodgers will face off against the New York Yankees in a best-of-seven battle for the 2024 World Series. \nThis marks the 12th time that these two teams have gone head-to-head to win it all. Theâ€¦', 'domain': 'fastcompany.com', 'id': '0111321e0f58acd3', 'img': {'s': '0111321e0f58acd3_s.webp', 'sq': '0111321e0f58acd3_sq.webp'}, 'published': 'Fri, 25 Oct 2024 16:51:00 GMT', 'title': 'World Series 2024: How to watch Yankees-Dodgers game 1 live online, on TV, or on a phone, including free options', 'topic': ['series', 'yankees'], 'url': 'https://www.fastcompany.com/91216623/world-series-2024-live-stream-watch-yankees-dodgers-online-tv-free'}], 'topic': ['series', 'yankees']}
7. {'stories': [{'body_preview': ""The demand for cruises is still going strong â€” and it doesn't appear to be letting up anytime soon.\nThe industry was the last to recover from the Covid pandemic, but once it did, it has been enjoying strong pricing and booking momentum. While pricing growth is starting to normalize somewhat, it isâ€¦"", 'domain': 'cnbc.com', 'id': 'bdd01eabb4081bcb', 'img': {'s': 'bdd01eabb4081bcb_s.webp', 'sq': 'bdd01eabb4081bcb_sq.webp'}, 'published': 'Sat, 26 Oct 2024 12:34:43 GMT', 'title': 'Cruise lines are having a moment as a popular â€” and cheaper â€” alternative to hotels', 'topic': ['cruise', 'gate'], 'url': 'https://www.cnbc.com/2024/10/26/cruise-lines-are-having-a-moment-as-a-cheaper-alternative-to-hotels.html'}, {'body_preview': ""Your cruise experience varies greatly based on the size of your ship. That's obvious when someone considers a small luxury yacht versus a traditional cruise ship, but the difference between ships that can accommodate 2,000-3,000 passengers and ones that serve closer to 6,000 are significant.\nSomeâ€¦"", 'domain': 'thestreet.com', 'id': '78d0a454dd1a74b0', 'img': {'s': '78d0a454dd1a74b0_s.webp', 'sq': '78d0a454dd1a74b0_sq.webp'}, 'published': 'Sat, 26 Oct 2024 12:00:00 GMT', 'title': 'These are the best mega and large ship cruise lines passengers say', 'topic': ['cruise', 'gate'], 'url': 'https://www.thestreet.com/travel/these-are-the-best-mega-and-large-ship-cruise-lines-passengers-say'}, {'body_preview': 'Are you the kind of traveller who skips ahead in line during boarding calls? One major airline has a message for you: Not so fast. American Airlines is experimenting with new technology that triggers a signal to gate agents when a passenger tries to board ahead of their group.', 'domain': 'cbc.ca', 'id': 'd83acf6bb303cb4b', 'img': {'s': 'd83acf6bb303cb4b_s.webp', 'sq': 'd83acf6bb303cb4b_sq.webp'}, 'published': 'Sat, 26 Oct 2024 08:00:00 GMT', 'title': 'Nice try, but this major airline is testing tech that shames you for boarding too early', 'topic': ['cruise', 'gate'], 'url': 'https://www.cbc.ca/news/business/american-airlines-gate-control-tech-1.7363343'}, {'body_preview': 'As if there werenâ€™t enough ways to be humiliated during air travel. American Airlines is testing a new technology that audibly alerts passengers if someone tries to board before their group is allowed.\nAmerican brings out the medicated shampoo. The term â€œgate liceâ€ refers to passengers who try toâ€¦', 'domain': 'morningbrew.com', 'id': '63b05279f30344b2', 'img': {'s': '63b05279f30344b2_s.webp', 'sq': '63b05279f30344b2_sq.webp'}, 'published': 'Sat, 26 Oct 2024 05:03:00 GMT', 'title': 'American Airlines will publicly embarrass â€˜gate liceâ€™', 'topic': ['cruise', 'gate'], 'url': 'https://www.morningbrew.com/daily/stories/american-airlines-publicly-embarrass-gate-lice?utm_source='}, {'body_preview': 'travel 101\nAirline Lost Your Bag? Hereâ€™s How to Reunite With Your Luggage.\nOr at least be reimbursed if itâ€™s gone for good.\nWeâ€™ve all been there, waiting impatiently at the baggage carousel, anxiously hoping for that first glimpse of our checked luggage. Then we breathe a sigh of relief, collectâ€¦', 'domain': 'nytimes.com', 'id': '036bbfaf8b20c32c', 'img': {'s': '036bbfaf8b20c32c_s.webp', 'sq': '036bbfaf8b20c32c_sq.webp'}, 'published': 'Fri, 25 Oct 2024 19:54:29 GMT', 'title': 'Airline Lost Your Bag? Hereâ€™s How to Reunite With Your Luggage', 'topic': ['cruise', 'gate'], 'url': 'https://www.nytimes.com/2024/10/25/travel/lost-luggage-customer-service.html'}, {'body_preview': 'Facial recognition and iris biometrics are allowing Singapore Changi Airport to process passengers through immigration in as little as 10 seconds as it fully embraces futuristic technology. Read More', 'domain': 'fortune.com', 'id': '405fae83f338ad9b', 'img': {'s': '405fae83f338ad9b_s.webp', 'sq': '405fae83f338ad9b_sq.webp'}, 'published': 'Fri, 25 Oct 2024 17:33:32 GMT', 'title': 'Travelers clear Singapore airport immigration in just 10 seconds using new tech', 'topic': ['cruise', 'gate'], 'url': 'https://fortune.com/2024/10/25/singapore-airport-immigration-tech-facial-recognition-iris-biometrics/'}, {'body_preview': ""Why is American Airlines removing â€˜gate liceâ€™ at airports? What it means for DFW travelers Fort Worth Star-Telegram\nAmerican Airlines tests new boarding platform in some airports to stop 'gate lice' USA TODAY\nAmerican Airlines tests new 'shaming tools' to deter boarding frauds SFGATE\nAmericanâ€¦"", 'domain': 'news.google.com', 'id': '93c70a50b55ca6d8', 'img': None, 'published': 'Fri, 25 Oct 2024 15:08:04 GMT', 'title': 'Why is American Airlines removing â€˜gate liceâ€™ at airports? What it means for DFW travelers', 'topic': ['cruise', 'gate'], 'url': 'https://news.google.com/rss/articles/CBMifkFVX3lxTFBtQ25pU1VrSDdNR3ZUd1FEWS1HMUZqenNWRlB1dUVYamh4a3dQVWNLYktMbEhyaTY2X2FDTnRXQWFob2lhbVR6eE5vbWNOQ2JjWC00bG1XTjd6QlcyQzB1MWphQUlWTFd0bVdhMW5ydVdqN050Q0QtVm4zUDVRZw?oc=5'}], 'topic': ['cruise', 'gate']}
8. {'stories': [{'body_preview': 'Las Vegas Strip guests look forward to seeing some of the most popular performers in the music business whenever they pay Sin City a visit.\nThe Strip features long-running, classic Vegas headliners who plan to continue their residencies many months or years into the future, including Mr. Las Vegasâ€¦', 'domain': 'thestreet.com', 'id': 'b05c78e3e265c8f9', 'img': {'s': 'b05c78e3e265c8f9_s.webp', 'sq': 'b05c78e3e265c8f9_sq.webp'}, 'published': 'Sat, 26 Oct 2024 12:03:00 GMT', 'title': 'Las Vegas Strip casino closes popular rock starâ€™s residency', 'topic': ['hallmark', 'ladies'], 'url': 'https://www.thestreet.com/travel/las-vegas-strip-casino-closes-popular-rock-stars-residency'}, {'body_preview': 'The actor, who has voiced many a memorable character on the long-running animated series, is now heading up a Bruce Springsteen tribute band.', 'domain': 'marketwatch.com', 'id': 'f511ec809c7efcee', 'img': None, 'published': 'Sat, 26 Oct 2024 11:00:00 GMT', 'title': 'Hank Azaria of â€˜Simpsonsâ€™ fame shares what the show has meant to him financially', 'topic': ['hallmark', 'ladies'], 'url': 'https://www.marketwatch.com/story/hank-azaria-of-simpsons-fame-shares-what-the-show-has-meant-to-him-financially-98f6c112'}, {'body_preview': 'Lesh was the oldest and one of the longest surviving members of the band that came to define the acid rock sound emanating from San Francisco in the 1960s.', 'domain': 'pbs.org', 'id': '19f51f055414da14', 'img': {'s': '19f51f055414da14_s.webp', 'sq': '19f51f055414da14_sq.webp'}, 'published': 'Sat, 26 Oct 2024 00:19:22 GMT', 'title': 'Phil Lesh, influential bassist and Grateful Dead founding member, dies at 84', 'topic': ['hallmark', 'ladies'], 'url': 'https://www.pbs.org/newshour/arts/phil-lesh-influential-bassist-and-grateful-dead-founding-member-dies-at-84'}, {'body_preview': 'Hallmark Media executive vice president of programming Lisa Hamilton Daly instructed a former employee not to cast â€œold peopleâ€ for Hallmark roles, saying that â€œour leading ladies are aging out,â€ according to a lawsuit filed against the network this month and obtained by The Associated Press.', 'domain': 'seattletimes.com', 'id': '57da3abd06f8a975', 'img': {'s': '57da3abd06f8a975_s.webp', 'sq': '57da3abd06f8a975_sq.webp'}, 'published': 'Fri, 25 Oct 2024 20:55:05 GMT', 'title': 'Hallmark exec says leading ladies Lacey Chabert, Holly Robinson Peete are â€˜aging outâ€™: lawsuit', 'topic': ['hallmark', 'ladies'], 'url': 'https://www.seattletimes.com/business/hallmark-exec-says-leading-ladies-lacey-chabert-holly-robinson-peete-are-aging-out-lawsuit/'}, {'body_preview': 'Hey yâ€™all,\nA few months ago, I interviewed a listener who said something Iâ€™m still thinking about. After Amanda Petterutiâ€™s dad passed away, she discovered his secret coin collection â€” something that left her not just frustrated, but angry. She couldnâ€™t understand why her dad sank thousands ofâ€¦', 'domain': 'marketplace.org', 'id': 'ac847ca6a81a6b41', 'img': None, 'published': 'Fri, 25 Oct 2024 19:48:06 GMT', 'title': 'This is the end of our 10th season', 'topic': ['hallmark', 'ladies'], 'url': 'https://www.marketplace.org/2024/10/25/this-is-the-end-of-our-10th-season/'}, {'body_preview': 'Captain Sandy Yawn spoke to Newsweek about how she handles firing crew members while filming ""Below Deck.""', 'domain': 'newsweek.com', 'id': '33771d88322f45c4', 'img': None, 'published': 'Fri, 25 Oct 2024 17:00:06 GMT', 'title': ""'Below Deck' Star Captain Sandy on Firing Crewâ€”'Defer to My Lawyer'"", 'topic': ['hallmark', 'ladies'], 'url': 'https://www.newsweek.com/below-deck-captain-sandy-firing-crew-1975030'}, {'body_preview': '""Gilmore Girls"" had a few plot holes you may have never noticed.\nGilmore Girls\n""Gilmore Girls"" is 24 years old, but it still has dedicated fans who are eager to point out errors.\nFrom actors doing double duty to coffee cups changing color, the show had a few continuity errors.\nThe Netflix revival,â€¦', 'domain': 'businessinsider.com', 'id': 'bc716e19be33295d', 'img': None, 'published': 'Fri, 25 Oct 2024 15:26:34 GMT', 'title': ""12 plot holes and errors you probably never noticed on 'Gilmore Girls'"", 'topic': ['hallmark', 'ladies'], 'url': 'https://www.businessinsider.com/plot-holes-you-never-noticed-on-gilmore-girls-2019-4'}], 'topic': ['hallmark', 'ladies']}
9. {'stories': [{'body_preview': 'The Scout brand, originally known for making the International Harvester back in the 60s and 70s, was purchased by Volkswagen (VWAGY) in 2021 and reborn as an electric adventure brand â€” and its two latest vehicles are here: the Terra pickup and the Traveler SUV.\nThe two EV trucks both useâ€¦', 'domain': 'aol.com', 'id': 'cb262fc413f47cba', 'img': {'s': 'cb262fc413f47cba_s.webp', 'sq': 'cb262fc413f47cba_sq.webp'}, 'published': 'Fri, 25 Oct 2024 20:18:49 GMT', 'title': ""VW's Scout Motors debuts 2 new EVs and a surprise range extender hybrid option"", 'topic': ['scout', 'motors'], 'url': 'https://www.aol.com/vws-scout-motors-debuts-2-200133284.html'}, {'body_preview': ""New Scout EVs will be available with backup gasoline engine Axios\nSee Photos of the 2028 Scout Traveler Car and Driver\nScout's Direct-Sales Plans Are Making Volkswagen Dealers Sad Jalopnik\nVW revives Scout SUVs as off-roading EVs in billion-dollar bid for US sales Detroit Free Press\nScout Motorsâ€¦"", 'domain': 'news.google.com', 'id': '4b62b5f729b010d4', 'img': None, 'published': 'Fri, 25 Oct 2024 17:32:37 GMT', 'title': 'New Scout EVs will be available with backup gasoline engine', 'topic': ['scout', 'motors'], 'url': 'https://news.google.com/rss/articles/CBMiekFVX3lxTFBMZnF3Y0pDbTNCa3Z2T01LUUhabnBPZ1AtcVBIODZuZ3p6NDV2d2dQUklpOWViUXIwUmVnUFFTS2w4TjRpakJRR2k0emx4b3p5S3BhUDY3VHVMUEJra2FwN1B1c24xRWJvQkpnblpqVTB5RjY2SENOb3B3?oc=5'}], 'topic': ['scout', 'motors']}
10. {'stories': [{'body_preview': 'The Pentagon has denied claims running rampant online around an updated Defense Department (DOD) document outlining how military intelligence can be shared with local and federal authorities, after numerous sources claimed the directive opened the door for the use of force against civilians aheadâ€¦', 'domain': 'newsweek.com', 'id': 'd300e39176c00a34', 'img': {'s': 'd300e39176c00a34_s.webp', 'sq': 'd300e39176c00a34_sq.webp'}, 'published': 'Fri, 25 Oct 2024 15:07:16 GMT', 'title': ""What is Department of Defense Directive 5240.01? Pentagon's Military Update"", 'topic': ['military', 'defense'], 'url': 'https://www.newsweek.com/pentagon-directive-5240-01-conspiracy-theories-1974219'}, {'body_preview': 'The United States has been the only country in the world with more guns than people for more than a decade. According to the latest estimates from the World Bank and the Small Arms Survey, a Swiss-based independent research organization, the U.S. is home to 45% of the worldâ€™s civilian-ownedâ€¦', 'domain': '247wallst.com', 'id': 'beda4bdf748bb736', 'img': None, 'published': 'Fri, 25 Oct 2024 13:56:08 GMT', 'title': 'How Many Guns Americans Have Bought Every Year Since 1986', 'topic': ['military', 'defense'], 'url': 'https://247wallst.com/guns-and-hunting/2024/10/25/how-many-guns-americans-have-bought-every-year-since-1986/'}], 'topic': ['military', 'defense']}
11. {'stories': [{'body_preview': 'AI helps philanthropic organization provide neediest families direct cash payments, fast.', 'domain': 'cbsnews.com', 'id': 'b39717439066c161', 'img': None, 'published': 'Fri, 25 Oct 2024 23:00:33 GMT', 'title': 'AI helps organization send poorest households impacted by Helene and Milton $1,000', 'topic': ['hurricane', 'helene'], 'url': 'https://www.cbsnews.com/news/ai-hurricane-helene-milton-cash-payments/'}, {'body_preview': 'The nonprofit GiveDirectly intends to send direct payments of $1,000 to households impacted by Hurricanes Helene and Milton.', 'domain': 'inc.com', 'id': '7062597d3a502bb6', 'img': {'s': '7062597d3a502bb6_s.webp', 'sq': '7062597d3a502bb6_sq.webp'}, 'published': 'Fri, 25 Oct 2024 22:46:24 GMT', 'title': 'Some Storm Victims Seeing $1,000 Disaster Aid Through AI', 'topic': ['hurricane', 'helene'], 'url': 'https://www.inc.com/associated-press/some-storm-victims-seeing-1000-disaster-aid-through-ai/90995037'}, {'body_preview': 'Hurricane Kristy strengthened to a Category 5 hurricane on Thursday, making it the most powerful storm on the planet.\nThe storm became a major hurricane after it underwent rapid intensification earlier this week, in which it strengthened from a tropical storm to a Category 3 hurricane in less thanâ€¦', 'domain': 'newsweek.com', 'id': '40f59a9d23279b19', 'img': {'s': '40f59a9d23279b19_s.webp', 'sq': '40f59a9d23279b19_sq.webp'}, 'published': 'Fri, 25 Oct 2024 15:40:30 GMT', 'title': ""Hurricane Kristy Is 'Most Powerful Storm on the Planet'"", 'topic': ['hurricane', 'helene'], 'url': 'https://www.newsweek.com/hurricane-kristy-strongest-storm-planet-1975042'}, {'body_preview': 'It was a bone-chilling report. As North Carolinians reeled from the aftermath of Hurricane Helene, the Federal Emergency Management Agency (FEMA) suddenly ordered emergency workers ""to stand down and evacuate"" Rutherford County due to reports of ""trucks of armed militias saying they were outâ€¦', 'domain': 'reason.com', 'id': '65c7f4da61efaca1', 'img': {'s': '65c7f4da61efaca1_s.webp', 'sq': '65c7f4da61efaca1_sq.webp'}, 'published': 'Fri, 25 Oct 2024 14:00:56 GMT', 'title': ""The Panic Over an Imaginary Militia 'Hunting FEMA' Did More Damage Than the Actual Threat"", 'topic': ['hurricane', 'helene'], 'url': 'https://reason.com/2024/10/25/the-panic-over-an-imaginary-militia-hunting-fema-did-more-damage-than-the-actual-threat/'}, {'body_preview': 'Some hurricane-impacted households in North Carolina and Florida will receive one-time, direct cash payments of up to $1,500 on Friday', 'domain': 'abcnews.go.com', 'id': '2aa27190b2873b9b', 'img': {'s': '2aa27190b2873b9b_s.webp', 'sq': '2aa27190b2873b9b_sq.webp'}, 'published': 'Fri, 25 Oct 2024 13:02:23 GMT', 'title': 'AI being used to send households hit by Helene and Milton $1,000 cash relief payments', 'topic': ['hurricane', 'helene'], 'url': 'https://abcnews.go.com/Business/wireStory/ai-send-households-impacted-helene-milton-1000-cash-115141057'}], 'topic': ['hurricane', 'helene']}
12. {'stories': [{'body_preview': 'Immigration, particularly by illegal aliens at the southern US border, is a contentious issue in 21st century America. Federal Reserve Chairman Jerome Powell has expressed concern about immigrationâ€™s probable impact on the unemployment rate. Homebuyers and renters are concerned about its impact onâ€¦', 'domain': 'mises.org', 'id': '6d69f3dddbb2ca69', 'img': {'s': '6d69f3dddbb2ca69_s.webp', 'sq': '6d69f3dddbb2ca69_sq.webp'}, 'published': 'Sat, 26 Oct 2024 06:00:00 GMT', 'title': 'Does Mass Immigration Adversely Impact the Natural Environment?', 'topic': ['immigrants', 'immigration'], 'url': 'https://mises.org/mises-wire/does-mass-immigration-adversely-impact-natural-environment'}, {'body_preview': ""These Are The Most Common Origins Of Immigrants\n \nA world map showing the most common origins of immigrants by country reveals some prolific immigrant nations as well as the influence that conflict and economic collapse can have on migration patterns.\nAs Statista's Katharina Buchholz reports, theâ€¦"", 'domain': 'zerohedge.com', 'id': 'b74c77b7b53faf7b', 'img': {'s': 'b74c77b7b53faf7b_s.webp', 'sq': 'b74c77b7b53faf7b_sq.webp'}, 'published': 'Sat, 26 Oct 2024 02:10:00 GMT', 'title': 'These Are The Most Common Origins Of Immigrants', 'topic': ['immigrants', 'immigration'], 'url': 'https://www.zerohedge.com/geopolitical/these-are-most-common-origins-immigrants'}, {'body_preview': 'NEW YORK -- Since the administration of U.S. President Joe Biden began, a flood of people have illegally crossed the border into the United States from Mexico. Many of these immigrants have gone to major cities like New York and Chicago, and the entire country is now grappling with the issue ofâ€¦', 'domain': 'asia.nikkei.com', 'id': '24e611d583ae752c', 'img': {'s': '24e611d583ae752c_s.webp', 'sq': '24e611d583ae752c_sq.webp'}, 'published': 'Fri, 25 Oct 2024 22:53:55 GMT', 'title': 'NIKKEI Film: Immigration shakes the foundations of the U.S', 'topic': ['immigrants', 'immigration'], 'url': 'https://asia.nikkei.com/Politics/U.S.-elections-2024/NIKKEI-Film-Immigration-shakes-the-foundations-of-the-U.S'}, {'body_preview': 'undefined', 'domain': 'projects.propublica.org', 'id': '038f24c955b9b5d0', 'img': {'s': '038f24c955b9b5d0_s.webp', 'sq': '038f24c955b9b5d0_sq.webp'}, 'published': 'Fri, 25 Oct 2024 18:50:26 GMT', 'title': 'The New Effects Of Immigration', 'topic': ['immigrants', 'immigration'], 'url': 'https://projects.propublica.org/new-effects-of-immigration/'}, {'body_preview': '', 'domain': 'bloomberg.com', 'id': 'b4fd96b11d0cd63a', 'img': {'s': 'b4fd96b11d0cd63a_s.webp', 'sq': 'b4fd96b11d0cd63a_sq.webp'}, 'published': 'Fri, 25 Oct 2024 18:30:00 GMT', 'title': 'Macklem Sees Little Inflation Impact From Immigration Reform', 'topic': ['immigrants', 'immigration'], 'url': 'https://www.bloomberg.com/news/articles/2024-10-25/macklem-sees-little-inflation-impact-from-immigration-reform'}, {'body_preview': '24/7 Wall St. Insights\nEvery year, millions of immigrants move to the United States. \nOf the 2.1 million people who moved to the U.S. from abroad in 2022, hundreds of thousands now live in California â€” a state with a nation-leading foreign-born population. \nAlso: 2 Dividend Legends to Holdâ€¦', 'domain': '247wallst.com', 'id': 'e7d1667c0c21c2e0', 'img': None, 'published': 'Sat, 26 Oct 2024 11:23:42 GMT', 'title': 'Immigrants Are Flocking to These Counties in This Blue State', 'topic': ['immigrants', 'immigration'], 'url': 'https://247wallst.com/population-and-social-characteristics/2024/10/26/immigrants-are-flocking-to-these-counties-in-this-blue-state/'}], 'topic': ['immigrants', 'immigration']}
13. {'stories': [{'body_preview': '', 'domain': 'bloomberg.com', 'id': 'bf8caf876a730037', 'img': {'s': 'bf8caf876a730037_s.webp', 'sq': 'bf8caf876a730037_sq.webp'}, 'published': 'Fri, 25 Oct 2024 18:21:50 GMT', 'title': 'Biden Issues Rare Presidential Apology Over Indian Boarding Schools', 'topic': ['boarding', 'apology'], 'url': 'https://www.bloomberg.com/news/articles/2024-10-25/biden-issues-rare-presidential-apology-over-indian-boarding-schools'}, {'body_preview': ""King Charles III has told a summit of Commonwealth countries in Samoa that the past could not be changed as he indirectly acknowledged calls from some of Britain's former colonies for a reckoning over its role in the trans-Atlantic slave trade."", 'domain': 'pbs.org', 'id': '4f769277fac135d9', 'img': {'s': '4f769277fac135d9_s.webp', 'sq': '4f769277fac135d9_sq.webp'}, 'published': 'Fri, 25 Oct 2024 16:14:25 GMT', 'title': ""As Britain's former colonies ask for slavery reparations, King Charles says past can't be changed"", 'topic': ['boarding', 'apology'], 'url': 'https://www.pbs.org/newshour/world/as-britains-former-colonies-ask-for-slavery-reparations-king-charles-says-past-cant-be-changed'}, {'body_preview': 'Watch live coverage as President Joe Biden formally apologizes for a U.S. government boarding school system that forcibly separated generations of Native American children from their families starting in 1819 and through at least 1969. Biden will speak at the Gila River Indian Community in Arizona.', 'domain': 'nbcnews.com', 'id': '823ca4d6fcfb0d7a', 'img': {'s': '823ca4d6fcfb0d7a_s.webp', 'sq': '823ca4d6fcfb0d7a_sq.webp'}, 'published': 'Fri, 25 Oct 2024 15:55:09 GMT', 'title': 'Biden formally apologizes for Indian boarding school system', 'topic': ['boarding', 'apology'], 'url': 'https://www.nbcnews.com/video/watch-live-biden-formally-apologizes-for-indian-boarding-schools-222680645645'}], 'topic': ['boarding', 'apology']}
14. {'stories': [{'body_preview': 'In late January, I visited Lincoln Center on the Upper West Side of Manhattan to watch an infamous, practically unstreamable film. The Lincoln is the hub of old-guard, uptown prestige and culture and the headquarters of the New York Film Festival. On Saturday night at 8:45, prime time forâ€¦', 'domain': 'theringer.com', 'id': '8e5547e8608eefb8', 'img': {'s': '8e5547e8608eefb8_s.webp', 'sq': '8e5547e8608eefb8_sq.webp'}, 'published': 'Sat, 26 Oct 2024 08:16:25 GMT', 'title': 'The Future of Film May Just Be Old Movies', 'topic': ['movies', 'marvel'], 'url': 'https://www.theringer.com/movies/2024/10/23/24270869/repertory-revival-cinema-old-movie-screenings-vidiots-film-at-lincoln'}, {'body_preview': 'The director of â€˜The Florida Projectâ€™ and â€˜Tangerineâ€™ is tipped for Oscar success but says â€˜Iâ€™m staying in realityâ€™', 'domain': 'ft.com', 'id': '1456f7126427c0a6', 'img': {'s': '1456f7126427c0a6_s.webp', 'sq': '1456f7126427c0a6_sq.webp'}, 'published': 'Sat, 26 Oct 2024 04:00:52 GMT', 'title': 'Filmmaker Sean Baker on his sex-worker story â€˜Anoraâ€™ and refusing to play it safe', 'topic': ['movies', 'marvel'], 'url': 'https://www.ft.com/content/2ef4d4d9-4cd8-43e1-8d51-186e5964824e'}, {'body_preview': 'Anna Kendrick starred in and directed Netflix\'s ""Woman of the Hour.""\nMatt Winkelmeyer/Getty Images\nAnna Kendrick made her film debut in 2003\'s ""Camp"" and has appeared in dozens of films since.\nHer directorial debut, ""Woman of the Hour,"" was released on Netflix this month.\nHer 2011 movie ""50/50"" isâ€¦', 'domain': 'businessinsider.com', 'id': 'a4e63f8e8378f221', 'img': {'s': 'a4e63f8e8378f221_s.webp', 'sq': 'a4e63f8e8378f221_sq.webp'}, 'published': 'Fri, 25 Oct 2024 14:57:00 GMT', 'title': ""Anna Kendrick's 10 best and 10 worst movies, according to critics"", 'topic': ['movies', 'marvel'], 'url': 'https://www.businessinsider.com/anna-kendrick-movies-ranked-according-to-critics-2020-8'}, {'body_preview': 'Who says the rom-com is dead? Venom: The Last Dance is a slapstick, silly, oddly sweet romp about a mismatched couple on a zany road trip across the American West. It just happens that the couple is a disgruntled journalist with a rap sheet and a brain-eating alien symbiote that latched onto him,â€¦', 'domain': 'reason.com', 'id': '6c96898e596c36b0', 'img': {'s': '6c96898e596c36b0_s.webp', 'sq': '6c96898e596c36b0_sq.webp'}, 'published': 'Fri, 25 Oct 2024 14:31:45 GMT', 'title': 'Venom: The Last Dance Is a Camp Buddy Comedy Disguised as a Comic Book Movie', 'topic': ['movies', 'marvel'], 'url': 'https://reason.com/2024/10/25/venom-the-last-dance-is-a-camp-buddy-comedy-disguised-as-a-comic-book-movie/'}], 'topic': ['movies', 'marvel']}
15. {'stories': [{'body_preview': ""Marcus's vision of Bitcoin as a large and neutral intermediary at the center of global money transfers is an intriguing one. Read More"", 'domain': 'fortune.com', 'id': 'bc137065e25dcd93', 'img': {'s': 'bc137065e25dcd93_s.webp', 'sq': 'bc137065e25dcd93_sq.webp'}, 'published': 'Fri, 25 Oct 2024 19:27:44 GMT', 'title': 'PayPal veteran David Marcus has bold ambitions for his $175 million Bitcoin startup Lightsparkâ€”but says it might take the rest of his life', 'topic': ['marcus', 'getnet'], 'url': 'https://fortune.com/crypto/2024/10/25/bitcoin-lightspark-david-marcus-uma/'}, {'body_preview': 'Getnet launched a regional eCommerce solution that provides a single entry point (SEP) for payments in Argentina, Brazil, Chile and Mexico.\nThe new Getnet SEP gives merchants in these countries access to Getnetâ€™s payment services through one integration, the company said in a Friday (Oct. 25)â€¦', 'domain': 'pymnts.com', 'id': '870b990c2403e371', 'img': None, 'published': 'Fri, 25 Oct 2024 17:19:46 GMT', 'title': 'Santanderâ€™s Getnet Launches Single Entry Point for Payments in Latin America', 'topic': ['marcus', 'getnet'], 'url': 'https://www.pymnts.com/news/ecommerce/2024/santander-getnet-launches-single-entry-point-payments-latin-america/'}, {'body_preview': 'Trusted by over 20,000 satisfied clients, merchant services provider Slice is reshaping the financial landscape with proprietary software and a commitment to individualized service. In response to recent amendments to credit card surcharging laws, the company has become a beacon for affordable,â€¦', 'domain': 'benzinga.com', 'id': '2968bc16340e398d', 'img': {'s': '2968bc16340e398d_s.webp', 'sq': '2968bc16340e398d_sq.webp'}, 'published': 'Fri, 25 Oct 2024 15:30:34 GMT', 'title': 'Merchant Services Provider Slice Celebrates 20,000 Happy Clients Enjoying Zero-Fee Processing', 'topic': ['marcus', 'getnet'], 'url': 'https://www.benzinga.com/partner/general/24/10/41549411/merchant-services-provider-slice-celebrates-20-000-happy-clients-enjoying-zero-fee-processing'}], 'topic': ['marcus', 'getnet']}
16. {'stories': [{'body_preview': 'Look at what youâ€™ve done.\nToday, the NYSE announced a plan to allow trading on its Arca electronic exchange 22 hours a dayâ€”from 1:30 am to 11:30 pm on weekdaysâ€”subject to regulatory approval. Right now, after-hours trading extends from 4 am to 8 pm.\nThe move to make equity trading available at allâ€¦', 'domain': 'morningbrew.com', 'id': '9b5c2b3d29e43a81', 'img': {'s': '9b5c2b3d29e43a81_s.webp', 'sq': '9b5c2b3d29e43a81_sq.webp'}, 'published': 'Fri, 25 Oct 2024 19:58:43 GMT', 'title': '22 hours of degeneracy', 'topic': ['hours', 'arca'], 'url': 'https://www.morningbrew.com/brew-markets/stories/2024/10/25/nyse-extended-trading-hours?utm_source='}, {'body_preview': 'In a game-changing move, the New York Stock Exchange (NYSE) announced on Friday it will be extending after-hours trading on its Arca electronic exchange.\nThe changes, which are subject to regulatory approval, would extend Arca electronic exchange hours from 1:30 a.m. ET to 11:30 p.m. ET Mondayâ€¦', 'domain': 'fastcompany.com', 'id': '674b34785e5d2680', 'img': {'s': '674b34785e5d2680_s.webp', 'sq': '674b34785e5d2680_sq.webp'}, 'published': 'Fri, 25 Oct 2024 19:00:00 GMT', 'title': 'New York Stock Exchange is extending its trading hours to meet growing demand in the Robinhood era', 'topic': ['hours', 'arca'], 'url': 'https://www.fastcompany.com/91216900/new-york-stock-exchange-nyse-extended-trading-after-hours-market-close-weekdays-robinhood-all-day'}, {'body_preview': 'NYSE Extending Trading On ARCA Equities Exchange To 22 Hours A Day\n \nSo much for Robinhood having a monopoly on overnight trading...\nThe New York Stock Exchange said in a press release today that it plans on extending weekday trading on its NYSE ARCA Equities Exchange to 22 hours a day. \n""NYSEâ€¦', 'domain': 'zerohedge.com', 'id': '9955cad1845bda28', 'img': {'s': '9955cad1845bda28_s.webp', 'sq': '9955cad1845bda28_sq.webp'}, 'published': 'Fri, 25 Oct 2024 18:40:00 GMT', 'title': 'NYSE Extending Trading On ARCA Equities Exchange To 22 Hours A Day', 'topic': ['hours', 'arca'], 'url': 'https://www.zerohedge.com/markets/nyse-extending-trading-arca-equities-exchange-22-hours-day'}, {'body_preview': 'Prospect of longer session comes in response to increased interest in buying and selling US stocks around the clock', 'domain': 'ft.com', 'id': '57d0d8e2c96cd5ef', 'img': None, 'published': 'Fri, 25 Oct 2024 16:14:55 GMT', 'title': 'NYSE plans to extend daily trading to 22 hours on its Arca exchange', 'topic': ['hours', 'arca'], 'url': 'https://www.ft.com/content/59dc109d-d385-412a-bcc6-b671db9ce680'}, {'body_preview': 'The New York Stock Exchange (ICE) on Friday announced plans to extend its weekday trading on its U.S. equities market to 22 hours a day.\nThat would mean the NYSE Arca â€” the top electronic exchange in the United States â€” will allow trading from 1:30 a.m. to 11:30 p.m. Eastern Time Monday throughâ€¦', 'domain': 'yahoo.com', 'id': 'e35bd0320a1facb9', 'img': None, 'published': 'Fri, 25 Oct 2024 15:45:00 GMT', 'title': 'U.S. stocks could soon trade for almost 24 hours a day', 'topic': ['hours', 'arca'], 'url': 'https://www.yahoo.com/tech/u-stocks-could-soon-trade-154500009.html'}, {'body_preview': 'Trading would open at 1:30 a.m. and end at 11:30 pm ET on weekdays on the Arca electronics exchange, if approved by regulators.', 'domain': 'cnbc.com', 'id': '1d4036c09e0e022c', 'img': None, 'published': 'Fri, 25 Oct 2024 14:29:13 GMT', 'title': 'New York Stock Exchange to extend after-hours trading', 'topic': ['hours', 'arca'], 'url': 'https://www.cnbc.com/2024/10/25/new-york-stock-exchange-to-extend-after-hours-trading.html'}], 'topic': ['hours', 'arca']}
17. {'stories': [{'body_preview': 'Defamed South Carolina lawyer Alex Murdaugh\'s Lowcountry hunting estate in Islandton has been sold to an ambitious new buyer.\nColleton County property records show Alexander Blair paid $1.16 million in May for 48 acres of land on the estate known as Moselle.\n""Moselle will be a completely differentâ€¦', 'domain': 'aol.com', 'id': '29bc7abe27be1339', 'img': {'s': '29bc7abe27be1339_s.webp', 'sq': '29bc7abe27be1339_sq.webp'}, 'published': 'Fri, 25 Oct 2024 17:21:18 GMT', 'title': ""Murdaugh hunting estate buyer says it will look 'completely' different after renovations"", 'topic': ['infowars', 'auction'], 'url': 'https://www.aol.com/murdaugh-hunting-estate-buyer-says-150942828.html'}, {'body_preview': 'Conspiracy theorist Alex Jones is seeking to protect his personal social-media accounts from being sold in the coming auction of his Infowars media platform to pay more than $1-billion he owes relatives of the Sandy Hook Elementary School shooting, claiming selling those accounts would violate hisâ€¦', 'domain': 'theglobeandmail.com', 'id': '715a8fd9c367b711', 'img': None, 'published': 'Fri, 25 Oct 2024 17:15:18 GMT', 'title': 'Alex Jones fighting attempt to sell his social-media account rights in Infowars auction', 'topic': ['infowars', 'auction'], 'url': 'https://www.theglobeandmail.com/business/article-alex-jones-fighting-attempt-to-sell-his-social-media-account-rights-in/'}, {'body_preview': 'Conspiracy theorist Alex Jones is seeking to protect his personal social media accounts from being sold in the upcoming auction of his Infowars media platform to pay more than $1 billion he owes relatives of the Sandy Hook Elementary School shooting', 'domain': 'marketbeat.com', 'id': '5390e18b04b311a2', 'img': None, 'published': 'Fri, 25 Oct 2024 16:44:27 GMT', 'title': 'Alex Jones fighting attempt to sell his social media account rights in Infowars auction', 'topic': ['infowars', 'auction'], 'url': 'https://www.marketbeat.com/articles/alex-jones-fighting-attempt-to-sell-his-social-media-account-rights-in-infowars-auction-2024-10-25/'}], 'topic': ['infowars', 'auction']}
18. {'stories': [{'body_preview': 'Candidates: Sen. Tammy Baldwin (D-Wis.) and businessman Eric Hovde (R). \nAd spending since Labor Day: $55.8 million for Democrats; $48.5 million for Republicans. \nPast results: Baldwin won reelection in 2018 with 55.4 percent of the vote against then-challenger and state Sen. Leah Vukmir (R), whoâ€¦', 'domain': 'politico.com', 'id': 'b17155e0dabc91ae', 'img': None, 'published': 'Fri, 25 Oct 2024 18:00:00 GMT', 'title': 'The POLITICO race to watch: Wisconsin Senate', 'topic': ['senate', 'hovde'], 'url': 'https://www.politico.com/live-updates/2024/10/25/2024-elections-live-coverage-updates-analysis/politico-race-to-watch-00185532'}, {'body_preview': 'Wisconsin Republican Senate candidate Eric Hovde has repeatedly defended his self-professed lack of knowledge about the U.S. farm bill in the campaignâ€™s closing days, after first saying in a debate that he is â€œnot an expertâ€ on the legislation that is vital to farmers in the state known asâ€¦', 'domain': 'seattletimes.com', 'id': '7b3657c246350a11', 'img': {'s': '7b3657c246350a11_s.webp', 'sq': '7b3657c246350a11_sq.webp'}, 'published': 'Fri, 25 Oct 2024 15:22:08 GMT', 'title': 'Wisconsinâ€™s GOP Senate candidate Hovde defends not knowing much about the farm bill in a dairy state', 'topic': ['senate', 'hovde'], 'url': 'https://www.seattletimes.com/business/wisconsins-gop-senate-candidate-hovde-defends-not-knowing-much-about-the-farm-bill-in-a-dairy-state/'}, {'body_preview': 'Wisconsin Republican Senate candidate Eric Hovde has repeatedly defended his self-professed lack of knowledge about the U.S. farm bill in the campaignâ€™s closing days, after first saying in a debate that he is â€œnot an expertâ€ on the legislation that is vital to farmers in the state known asâ€¦', 'domain': 'marketbeat.com', 'id': '7ba6e77f6ab12cdf', 'img': None, 'published': 'Fri, 25 Oct 2024 15:21:27 GMT', 'title': ""Wisconsin's GOP Senate candidate Hovde defends not knowing much about the farm bill in a dairy state"", 'topic': ['senate', 'hovde'], 'url': 'https://www.marketbeat.com/articles/wisconsins-gop-senate-candidate-hovde-defends-not-knowing-much-about-the-farm-bill-in-a-dairy-state-2024-10-25/'}, {'body_preview': 'Bernie Moreno, the GOP Senate nominee in Ohio, has been using an anti-recording devices against ""trackers"" affiliated with the state\'s Democratic Party.\nAP Photo/David Dermer\nBernie Moreno is hoping to unseat Sen. Sherrod Brown in a race that will help decide Senate control.\nHe\'s invited eventâ€¦', 'domain': 'businessinsider.com', 'id': 'f6cafc8df02c9960', 'img': {'s': 'f6cafc8df02c9960_s.webp', 'sq': 'f6cafc8df02c9960_sq.webp'}, 'published': 'Fri, 25 Oct 2024 14:48:09 GMT', 'title': ""Ohio GOP candidate in key Senate race uses anti-recording tech to combat 'trackers'"", 'topic': ['senate', 'hovde'], 'url': 'https://www.businessinsider.com/bernie-moreno-senate-ohio-ultrasonic-tech-blocks-recording-events-2024-10'}, {'body_preview': ""A Republican lawmaker in South Carolina attempted to claim credit for an infrastructure win for the state, but it didn't go well.\nCongresswoman Nancy Mace, the representative for South Carolina's 1st congressional district since 2021, posted on X, formerly Twitter, on October 22, a link to anâ€¦"", 'domain': 'newsweek.com', 'id': '3ab1d556cfad0486', 'img': {'s': '3ab1d556cfad0486_s.webp', 'sq': '3ab1d556cfad0486_sq.webp'}, 'published': 'Fri, 25 Oct 2024 14:47:00 GMT', 'title': ""GOP Lawmaker's Attempt To Claim Credit for Infrastructure Funding Backfires"", 'topic': ['senate', 'hovde'], 'url': 'https://www.newsweek.com/south-carolina-representative-infrastructure-bill-1974939'}], 'topic': ['senate', 'hovde']}
",0,0
"When I want to access environmental and climate-related news stories, I need to display the title, URL, publication date, and media source of the news so that I can quickly understand what is relevant to it.

","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
API_HOST = ""climate-news-feed.p.rapidapi.com""

headers = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": API_HOST
}

def get_climate_articles(limit=10, source=None, exclude=None):
    """"""
    Fetch climate-related news articles with specified options.
    Args:
        limit (int): The number of articles to retrieve.
        source (str): The specific source to filter articles from.
        exclude (str): A term to exclude from the results.
    Returns:
        None: Prints article details such as title, URL, published date, and source.
    """"""
    url = f""https://{API_HOST}/""
    querystring = {""limit"": str(limit)}

    if source:
        querystring[""source""] = source
    if exclude:
        querystring[""exclude""] = exclude

    try:
        response = requests.get(url, headers=headers, params=querystring)
        response.raise_for_status()
        articles = response.json().get(""articles"", [])

        if articles:
            for article in articles:
                print(f""Title: {article.get('title')}"")
                print(f""URL: {article.get('url')}"")
                print(f""Published: {article.get('published')}"")
                print(f""Source: {article.get('source')}\n"")
        else:
            print(""No articles found."")

    except requests.exceptions.RequestException as e:
        print(f""Error fetching articles: {e}"")

if __name__ == '__main__':
    # Example call to the optimized function
    print(""Fetching climate-related articles:"")
    get_climate_articles(limit=5, source=""Nasa Climate"")
","Fetching climate-related articles:
Title: Google's â€˜A Passage of Water' Brings NASA's Water Data to Life
URL: https://climate.nasa.gov/news/3293/
Published: 2023-11-30T17:50:23.000Z
Source: Nasa Climate

Title: NASA to Showcase Earth Science Data at COP28
URL: https://climate.nasa.gov/news/3292/
Published: 2023-11-28T00:05:44.000Z
Source: Nasa Climate

Title: NASA Analysis Finds Strong El NiÃ±o Could Bring Extra Floods This Winter
URL: https://climate.nasa.gov/news/3291/
Published: 2023-11-08T19:46:30.000Z
Source: Nasa Climate

Title: NASA Flights Link Methane Plumes to Tundra Fires in Western Alaska
URL: https://climate.nasa.gov/news/3289/
Published: 2023-11-02T15:57:00.000Z
Source: Nasa Climate

Title: See SWOT Mission's Unprecedented View of Global Sea Levels
URL: https://climate.nasa.gov/news/3288/
Published: 2023-10-30T16:15:38.000Z
Source: Nasa Climate",1,1
"I want to see the main news feeds for three countries and regions and get the latest news from each source, and the system needs to show a list of the latest news by source or country.

","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def fetch_data(url, headers, params=None):
    try:
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f""Error fetching data from {url}: {e}"")
        return None


def get_sources():
    url = ""https://biztoc.p.rapidapi.com/sources""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""biztoc.p.rapidapi.com""
    }
    return fetch_data(url, headers)


def get_articles_by_source(source_id, limit=1):
    url = ""https://climate-news-feed.p.rapidapi.com/""
    querystring = {""source"": source_id, ""limit"": str(limit)}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""climate-news-feed.p.rapidapi.com""
    }
    return fetch_data(url, headers, params=querystring)


def get_latest_news(lang):
    url = ""https://bbc-api2.p.rapidapi.com/latest""
    querystring = {""lang"": lang}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""bbc-api2.p.rapidapi.com""
    }
    return fetch_data(url, headers, params=querystring)


def get_news_by_country(country_list):
    # Step 1: Get sources
    sources = get_sources()
    if not sources:
        print(""Failed to fetch sources."")
        return

    country_news = {}
    for country in country_list:
        country_news[country] = []
        for source in sources:
            if source.get('country') == country:
                # Step 2: Get latest articles from each source
                articles = get_articles_by_source(source_id=source['id'], limit=1)
                if articles and 'articles' in articles:
                    country_news[country].extend(articles['articles'])

    # Step 3: Display news grouped by country
    for country, articles in country_news.items():
        print(f""\nLatest news from {country}:"")
        if articles:
            for article in articles:
                print(f""- {article['title']} (URL: {article['url']}, Published: {article['published']})"")
        else:
            print(""No articles available."")

    if not any(country_news[country] for country in country_list):
        print(""\nNo news available for any of the specified countries."")


def main():
    # Define the list of countries you want to get news for
    country_list = [""USA"", ""UK"", ""Australia""]

    # Get news by country
    get_news_by_country(country_list)

    # Step 4: Get global latest news in English
    latest_news = get_latest_news(lang=""en"")
    if latest_news and 'Latest' in latest_news:
        print(""\nGlobal Latest News:"")
        for news in latest_news.get('Latest', []):
            print(f""News Title: {news['title']}, Summary: {news['summary']}, URL: {news['news_link']}"")
    else:
        print(""\nFailed to fetch global latest news or no news available."")


if __name__ == '__main__':
    main()
","Latest news from USA:
No articles available.

Latest news from UK:
No articles available.

Latest news from Australia:
No articles available.

No news available for any of the specified countries.
Error fetching data from https://bbc-api2.p.rapidapi.com/latest: 400 Client Error: Bad Request for url: https://bbc-api2.p.rapidapi.com/latest?lang=en

Failed to fetch global latest news or no news available.",1,1
"I would like to perform lexical and structural analysis on three specific texts to obtain information such as unique lexical statistics, chapter counts, and word counts to better understand the complexity and style of the text.

","import requests
from collections import Counter
import re

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""al-quran1.p.rapidapi.com""
}

def get_base_information():
    """"""Retrieve general information about the text for overview analysis.""""""
    url = ""https://al-quran1.p.rapidapi.com/""
    response = requests.get(url, headers=HEADERS)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Failed to retrieve base information: {response.status_code}"")
        return {}

def get_entire_surah(chapter_id):
    """"""Retrieve the text of a specific surah.""""""
    url = f""https://al-quran1.p.rapidapi.com/{chapter_id}""
    response = requests.get(url, headers=HEADERS)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Failed to retrieve surah information: {response.status_code}"")
        return {}

def search_for_word(search_term):
    """"""Search for occurrences of a specific word to analyze its usage and context.""""""
    url = f""https://al-quran1.p.rapidapi.com/corpus/{search_term}""
    response = requests.get(url, headers=HEADERS)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Failed to search for word '{search_term}': {response.status_code}"")
        return {}

def analyze_text(text):
    """"""Perform lexical and structural analysis of the text.""""""
    words = re.findall(r'\b\w+\b', text.lower())
    unique_words = set(words)
    word_count = len(words)
    unique_word_count = len(unique_words)
    word_freq = Counter(words)

    print(f""Total Words: {word_count}"")
    print(f""Unique Words: {unique_word_count}"")
    print(""Top 10 Most Frequent Words:"", word_freq.most_common(10))

if __name__ == '__main__':
    # Step 1: Retrieve base information for lexical and structural analysis
    base_info = get_base_information()
    if base_info:
        print(""Base Information Retrieved."")

    # Step 2: Retrieve specific surah details for analysis
    chapter_id = 1  # Example Surah (Al-Fatihah)
    surah_info = get_entire_surah(chapter_id)

    # Check if surah_info has 'data' and 'ayahs' keys
    if 'data' in surah_info and 'ayahs' in surah_info['data']:
        surah_text = ' '.join([ayah['text'] for ayah in surah_info['data']['ayahs']])
        analyze_text(surah_text)
    else:
        print(""Surah data not in expected format or missing keys."")

    # Step 3: Search for a specific word in the text to analyze usage and context
    search_term = ""mercy""  # Example search term
    search_results = search_for_word(search_term)
    if search_results:
        print(""Search Results:"", search_results)
","Base Information Retrieved.
Surah data not in expected format or missing keys.
Search Results: [{'total_matches ': 139}, {'surah_no': '2', 'verse_no': '64', 'content': ' ""then after that you turned away;and were it not for allahâ€™s grace on youand his mercy,you would surely have been among the losers."",...'}, {'surah_no': '2', 'verse_no': '105', 'content': ' ""neither the faithless from among the people of the book, nor the idolaters, like that any good be showered on you from your lord; but allah singles out for his mercy whomever he wishes, and allah is dispenser of a mighty grace."",...'}, {'surah_no': '2', 'verse_no': '157', 'content': ' ""it is they who receive the blessings of their lord and [his] mercy, and it is they who are the [rightly] guided."",...'}, {'surah_no': '2', 'verse_no': '178', 'content': ' ""o you who have faith! retribution is prescribed for you regarding the slain...'}, {'surah_no': '2', 'verse_no': '218', 'content': ' ""indeed those who have become faithful and those who have migrated and waged jihÄd in the way of allah â€”it is they who expect allahâ€™s mercy, and allah is all-forgiving, all-merciful."",...'}, {'surah_no': '3', 'verse_no': '8', 'content': ' ""[they say,] â€˜our lord! do not make our hearts swerve after you have guided us, and bestow your mercy on us. indeed you are the all-munificent."",...'}, {'surah_no': '3', 'verse_no': '74', 'content': ' ""he singles out for his mercy whomever he wishes, and allah is dispenser of a great grace.â€™"",...'}, {'surah_no': '3', 'verse_no': '107', 'content': ' ""but as for those whose faces become white, they shall dwell in allahâ€™s mercy, and they will remain in it [forever]."",...'}, {'surah_no': '3', 'verse_no': '132', 'content': ' ""and obey allah and the apostle so that you may be granted [his] mercy."",...'}, {'surah_no': '3', 'verse_no': '157', 'content': ' ""if you are slain in the way of allah, or die, surely forgiveness and mercy from allah are better than what they amass."",...'}, {'surah_no': '3', 'verse_no': '159', 'content': ' ""it is by allahâ€™s mercy that you are gentle to them; and had you been harsh and hardhearted, surely they would have scattered from around you. so excuse them, and plead for forgiveness for them, and consult them in the affairs, and once you are resolved, put your trust in allah. indeed allah loves those who trust in him."",...'}, {'surah_no': '4', 'verse_no': '83', 'content': ' ""when a report of safety or alarm comes to them, they immediately broadcast it; but had they referred it to the apostle or to those vested with authority among them, those of them who investigate would have ascertained it. and were it not for allahâ€™s grace upon you and his mercy, you would have surely followed satan, [all] except a few."",...'}, {'surah_no': '4', 'verse_no': '96', 'content': ' ""ranks from him, forgiveness, and mercy, and allah is all-forgiving, all-merciful."",...'}, {'surah_no': '4', 'verse_no': '113', 'content': ' ""were it not for allahâ€™s grace and his mercy on you, a group of them were bent on leading you astray; but they do not mislead anyone except themselves, and they cannot do you any harm. allah has sent down to you the book and wisdom, and he has taught you what you did not know, and great is allahâ€™s grace upon you."",...'}, {'surah_no': '4', 'verse_no': '175', 'content': ' ""as for those who have faith in allah, and hold fast to him, he will admit them to his mercy and grace, and he will guide them on a straight path to him."",...'}, {'surah_no': '6', 'verse_no': '12', 'content': ' ""say, â€˜to whom belongs whatever is in the heavens and the earth?â€™ say, â€˜to allah. he has made mercy incumbent upon himself. he will surely gather you on the day of resurrection, in which there is no doubt. those who have ruined their souls1 will not have faith.â€™or â€˜themselves.â€™"",...'}, {'surah_no': '6', 'verse_no': '54', 'content': ' ""when those who have faith in our signs come to you, say, â€˜peace to you! your lord has made mercy incumbent upon himself...'}, {'surah_no': '6', 'verse_no': '133', 'content': ' ""your lord is the all-sufficient dispenser of mercy. if he wishes, he will take you away, and make whomever he wishes succeed you, just as he produced you from the descendants of another people."",...'}, {'surah_no': '6', 'verse_no': '147', 'content': ' ""but if they deny you, say, â€˜your lord is dispenser of an all-embracing mercy, but his punishment will not be averted from the guilty lot.â€™"",...'}, {'surah_no': '6', 'verse_no': '154', 'content': ' ""then we gave moses the book, completing [our blessing] on him who is virtuous, and as an elaboration1 of all things, and as a guidance and mercy, so that they may believe in the encounter with their lord.or â€˜articulation.â€™"",...'}, {'surah_no': '6', 'verse_no': '155', 'content': ' ""and this book that we have sent down is a blessed one; so follow it, and be godwary so that you may receive [his] mercy."",...'}, {'surah_no': '6', 'verse_no': '157', 'content': ' ""or [lest] you should say, â€˜if the book had been sent down to us, surely we would have been better-guided than them.â€™ there has already come to you a manifest proof from your lord and a guidance and mercy. so who is a greater wrongdoer than him who denies the signs of allah, and turns away from them? soon we shall requite those who turn away from our signs with a terrible punishment because of what they used to evade."",...'}, {'surah_no': '7', 'verse_no': '23', 'content': ' ""they said, â€˜our lord, we have wronged ourselves! if you do not forgive us and have mercy upon us, we will surely be among the losers.â€™"",...'}, {'surah_no': '7', 'verse_no': '49', 'content': ' ""are these1 the ones concerning whom you swore that allah will not extend them any mercy?â€™ â€˜enter paradise!2 you shall have no fear, nor shall you grieve.â€™that is, the people who will be about to enter paradise.addressed to the people about to enter paradise."",...'}, {'surah_no': '7', 'verse_no': '52', 'content': ' ""certainly we have brought them a book, which we have elaborated with knowledge, as a guidance and mercy for a people who have faith."",...'}, {'surah_no': '7', 'verse_no': '56', 'content': ' ""and do not cause corruption on the earth after its restoration, and supplicate him with fear and hope...'}, {'surah_no': '7', 'verse_no': '57', 'content': ' ""it is he who sends forth the winds as harbingers of his mercy. when they bear [rain-]laden clouds, we drive them toward a dead land and send down water on it, and with it we bring forth all kinds of crops. thus shall we raise the dead; maybe you will take admonition."",...'}, {'surah_no': '7', 'verse_no': '63', 'content': ' ""do you consider it odd that a reminder from your lord should come to you through a man from among yourselves, to warn you so that you may be godwary and so that you may receive his mercy?â€™"",...'}, {'surah_no': '7', 'verse_no': '72', 'content': ' ""then we delivered him and those who were with him by a mercy from us, and we rooted out those who denied our signs and were not faithful."",...'}, {'surah_no': '7', 'verse_no': '149', 'content': ' ""but when they became remorseful and realised they had gone astray, they said, â€˜should our lord have no mercy on us, and forgive us, we will be surely among the losers.â€™"",...'}, {'surah_no': '7', 'verse_no': '151', 'content': ' ""he said, â€˜my lord, forgive me and my brother, and admit us into your mercy, for you are the most merciful of the merciful."",...'}, {'surah_no': '7', 'verse_no': '154', 'content': ' ""and when mosesâ€™ indignation abated, he picked up the tablets whose inscriptions contained guidance and mercy for those who are in awe of their lord."",...'}, {'surah_no': '7', 'verse_no': '155', 'content': ' ""moses chose seventy men from his people for our tryst, and when the earthquake seized them, he said, â€˜my lord, had you wished, you would have destroyed them and me before. will you destroy us because of what the fools amongst us have done? it is only your test by which you lead astray whomever you wish and guide whomever you wish. you are our master, so forgive us and have mercy on us, for you are the best of those who forgive."",...'}, {'surah_no': '7', 'verse_no': '156', 'content': ' ""and appoint goodness for us in this world and the hereafter, for indeed we have come back to you.â€™ said he, â€˜i visit my punishment on whomever i wish, but my mercy embraces all things.1 soon i shall appoint it for those who are godwary and give the zakÄt and those who believe in our signscf. 6...'}, {'surah_no': '7', 'verse_no': '203', 'content': ' ""when you do not bring them a sign, they say, â€˜why do you not improvise one?â€™ say, â€˜i only follow what is revealed to me from my lord; these are insights from your lord, and a guidance and mercy for a people who have faith.â€™"",...'}, {'surah_no': '7', 'verse_no': '204', 'content': ' ""when the qurÊ¾Än is recited, listen to it and be silent, maybe you will receive [allahâ€™s] mercy."",...'}, {'surah_no': '9', 'verse_no': '21', 'content': ' ""their lord gives them the good news of his mercy and [his] pleasure, and for them there will be gardens with lasting bliss,"",...'}, {'surah_no': '9', 'verse_no': '61', 'content': ' ""among them are those who torment the prophet, and say, â€˜he is an ear.â€™1 say, â€˜an ear that is good for you. he has faith in allah and trusts the faithful, and is a mercy for those of you who have faith.â€™ as for those who torment the apostle of allah, there is a painful punishment for them.that is, easily persuadable, unquestioning and naive."",...'}, {'surah_no': '9', 'verse_no': '71', 'content': ' ""but the faithful, men and women, are comrades of one another...'}, {'surah_no': '9', 'verse_no': '99', 'content': ' ""yet among the bedouins are [also] those who believe in allah and the last day, and regard what they spend as [a means of attaining] nearness to allah and the blessings of the apostle. look! it shall indeed bring them nearness, and allah will admit them into his mercy. indeed allah is all-forgiving, all-merciful."",...'}, {'surah_no': '10', 'verse_no': '21', 'content': ' ""when we let people taste [our] mercy after a distress that has befallen them, behold, they scheme against our signs! say, â€˜allah is more swift at devising.â€™ indeed our messengers write down what you scheme."",...'}, {'surah_no': '10', 'verse_no': '57', 'content': ' ""o mankind! there has certainly come to you an advice from your lord, and a cure for what is in the breasts, and a guidance and mercy for the faithful."",...'}, {'surah_no': '10', 'verse_no': '58', 'content': ' ""say, â€˜in allahâ€™s grace and his mercyâ€” let them rejoice in that! it is better than what they amass.â€™"",...'}, {'surah_no': '10', 'verse_no': '86', 'content': ' ""and deliver us by your mercy from the faithless lot.â€™"",...'}, {'surah_no': '11', 'verse_no': '9', 'content': ' ""if we let man taste a mercy from us, and then withdraw it from him, he becomes despondent, ungrateful."",...'}, {'surah_no': '11', 'verse_no': '17', 'content': ' ""is he who stands on a manifest proof from his lord, and whom a witness of his own [family] follows?1 and before him2 there was the book of moses, a guide and mercy. it is they who have faith in it, and whoever defies him from among the factions, the fire is their tryst. so do not be in doubt about it; it is the truth from your lord, but most people do not have faith.ellipsis. that is, is such a person like someone who is not such? or, can such a one be deterred by the denial of the ignorant?or â€˜before it,â€™ that is, the qurÊ¾Än."",...'}, {'surah_no': '11', 'verse_no': '28', 'content': ' ""he said, â€˜o my people! tell me, should i stand on a manifest proof from my lord, and he has granted me his own mercy â€”though it should be invisible to youâ€” shall we force it upon you while you are averse to it?"",...'}, {'surah_no': '11', 'verse_no': '43', 'content': ' ""he said, â€˜i shall take refuge on a mountain; it will protect me from the flood.â€™ he said, â€˜there is none today who can protect from allahâ€™s edict, except someone upon whom he has mercy.â€™ then the waves came between them, and he was among those who were drowned."",...'}, {'surah_no': '11', 'verse_no': '47', 'content': ' ""he said, â€˜my lord! i seek your protection lest i should ask you something of which i have no knowledge. if you do not forgive me and have mercy upon me i shall be among the losers.â€™"",...'}, {'surah_no': '11', 'verse_no': '58', 'content': ' ""and when our edict came, we delivered hÅ«d and the faithful who were with him, by a mercy from us, and we delivered them from a harsh punishment."",...'}, {'surah_no': '11', 'verse_no': '63', 'content': ' ""he said, â€˜o my people! tell me, should i stand on a manifest proof from my lord, and he has granted me his own mercy, who will protect me from allah should i disobey him? for then you will increase me in nothing but loss."",...'}, {'surah_no': '11', 'verse_no': '66', 'content': ' ""so when our edict came, we delivered á¹£Äliá¸¥ and the faithful who were with him by a mercy from us, and from the [punishment and] disgrace of that day. your lord is indeed the all-strong, the all-mighty."",...'}, {'surah_no': '11', 'verse_no': '73', 'content': ' ""they said, â€˜are you amazed at allahâ€™s dispensation? [that is] allahâ€™s mercy and his blessings upon you, members of the household. indeed he is all-laudable, all-glorious.â€™"",...'}, {'surah_no': '11', 'verse_no': '94', 'content': ' ""and when our edict came, we delivered shuÊ¿ayb and the faithful who were with him by a mercy from us. and the cry seized those who were wrongdoers, whereat they lay lifeless prostrate in their homes,"",...'}, {'surah_no': '11', 'verse_no': '119', 'content': ' ""except those on whom your lord has mercy â€”and that is why he created themâ€” and the word of your lord has been fulfilled...'}, {'surah_no': '12', 'verse_no': '53', 'content': ' ""yet i do not absolve my [own carnal] soul, for the [carnal] soul indeed prompts [men] to evil, except inasmuch as my lord has mercy. indeed my lord is all-forgiving, all-merciful.â€™"",...'}, {'surah_no': '12', 'verse_no': '56', 'content': ' ""that is how we established joseph in the land that he may settle in it wherever he wished. we confer our mercy on whomever we wish, and we do not waste the reward of the virtuous."",...'}, {'surah_no': '12', 'verse_no': '87', 'content': ' ""â€˜go, my sons, and look for joseph and his brother, and do not despair of allahâ€™s mercy. indeed no one despairs of allahâ€™s mercy except the faithless lot.â€™"",...'}, {'surah_no': '12', 'verse_no': '111', 'content': ' ""there is certainly a moral in their accounts for those who possess intellect. this [qurÊ¾Än] is not a fabricated discourse; rather it is a confirmation of what was [revealed] before it, and an elaboration1 of all things, and a guidance and mercy for a people who have faith.or â€˜unravelling.â€™"",...'}, {'surah_no': '13', 'verse_no': '6', 'content': ' ""they would press you for evil sooner than for good,1 though there have already gone by before them exemplary punishments. indeed your lord is forgiving to mankind despite their wrongdoing, and indeed your lord is severe in retribution.that is, they ask you to bring about the divine punishment with which you have threatened them, instead of pleading for divine mercy and forgiveness."",...'}, {'surah_no': '15', 'verse_no': '56', 'content': ' ""he said, â€˜who despairs of his lordâ€™s mercy except those who are astray?!â€™"",...'}, {'surah_no': '16', 'verse_no': '64', 'content': ' ""we did not send down the book to you except [for the purpose] that you may clarify for them what they differ about, and as a guidance and mercy for a people who have faith."",...'}, {'surah_no': '16', 'verse_no': '89', 'content': ' ""the day we raise in every nation a witness against them from among themselves, we shall bring you as a witness against these. we have sent down the book to you as a clarification of all things and as a guidance and mercy and good news for the muslims."",...'}, {'surah_no': '17', 'verse_no': '8', 'content': ' ""maybe your lord will have mercy on you, but if you revert, we [too] will revert, and we have made hell a prison for the faithless."",...'}, {'surah_no': '17', 'verse_no': '24', 'content': ' ""lower the wing of humility to them, out of mercy, and say, â€˜my lord! have mercy on them, just as they reared me when i was [a] small [child]!â€™"",...'}, {'surah_no': '17', 'verse_no': '28', 'content': ' ""and if you have to overlook them [for now], seeking the mercy of your lord which you expect [in the future], speak to them gentle words."",...'}, {'surah_no': '17', 'verse_no': '54', 'content': ' ""your lord knows you best. he will have mercy on you, if he wishes, or punish you, if he wishes, and we did not send you to watch over them."",...'}, {'surah_no': '17', 'verse_no': '57', 'content': ' ""they [themselves] are the ones who supplicate, seeking a recourse to their lord,1 whoever is nearer [to him], expecting his mercy and fearing his punishment.â€™ indeed your lordâ€™s punishment is a thing to beware of.or â€˜those whom they (i.e., the polytheists) invoke, themselves seek a recourse to their lordâ€¦ .â€™"",...'}, {'surah_no': '17', 'verse_no': '82', 'content': ' ""we send down in the qurÊ¾Än that which is a cure and mercy for the faithful; and it increases the wrongdoers only in loss."",...'}, {'surah_no': '17', 'verse_no': '87', 'content': ' ""except a mercy from your lord. indeed his grace has been great upon you."",...'}, {'surah_no': '17', 'verse_no': '100', 'content': ' ""say, â€˜even if you possessed the treasuries of my lordâ€™s mercy, you would withhold them for the fear of being spent, and man is very niggardly.â€™"",...'}, {'surah_no': '18', 'verse_no': '10', 'content': ' ""when the youths took refuge in the cave, they said, â€˜our lord! grant us a mercy from yourself, and help us on to rectitude in our affair.â€™"",...'}, {'surah_no': '18', 'verse_no': '16', 'content': ' ""when you have dissociated yourselves from them and from what they worship except allah, then take refuge in the cave. your lord will unfold his mercy for you, and he will help you on to ease in your affair.â€™"",...'}, {'surah_no': '18', 'verse_no': '58', 'content': ' ""your lord is the all-forgiving dispenser of mercy. were he to take them to task because of what they have committed, he would have surely hastened their punishment. but they have a tryst, [when] they will not find a refuge besides him.1or â€˜from which they will not find any refuge.â€™"",...'}, {'surah_no': '18', 'verse_no': '65', 'content': ' ""[there] they found one of our servants whom we had granted a mercy from ourselves, and taught him a knowledge from our own."",...'}, {'surah_no': '18', 'verse_no': '81', 'content': ' ""so we desired that their lord should give them in exchange one better than him in respect of purity and closer in mercy."",...'}, {'surah_no': '18', 'verse_no': '82', 'content': ' ""as for the wall, it belonged to two boy orphans in the city. under it there was a treasure belonging to them. their father had been a righteous man. so your lord desired that they should come of age and take out their treasure â€”as a mercy from your lord. i did not do that out of my own accord. this is the interpretation of that over which you could not maintain patience.â€™"",...'}, {'surah_no': '18', 'verse_no': '98', 'content': ' ""he said, â€˜this is a mercy from my lord. but when the promise of my lord is fulfilled, he will level it;1 and my lordâ€™s promise is true.â€™or, â€˜he will make it crumble.â€™"",...'}, {'surah_no': '19', 'verse_no': '2', 'content': ' ""[this is] an account1 of your lordâ€™s mercy on his servant, zechariah,or â€˜a mention,â€™ â€˜a reminder,â€™ or â€˜a recollection.â€™"",...'}, {'surah_no': '19', 'verse_no': '21', 'content': ' ""he said, â€˜so shall it be. your lord says, â€œit is simple for me.â€ and so that we may make him a sign for mankind and a mercy from us, and it is a matter [already] decided.â€™"",...'}, {'surah_no': '19', 'verse_no': '50', 'content': ' ""and we gave them out of our mercy, and conferred on them a worthy and lofty repute."",...'}, {'surah_no': '19', 'verse_no': '53', 'content': ' ""and we gave him out of our mercy his brother aaron, a prophet."",...'}, {'surah_no': '21', 'verse_no': '75', 'content': ' ""and we admitted him into our mercy. indeed he was one of the righteous."",...'}, {'surah_no': '21', 'verse_no': '84', 'content': ' ""so we answered his prayer and removed his distress, and we gave him [back] his family along with others like them, as a mercy from us, and an admonition for the devout."",...'}, {'surah_no': '21', 'verse_no': '86', 'content': ' ""we admitted them into our mercy. indeed they were among the righteous."",...'}, {'surah_no': '21', 'verse_no': '107', 'content': ' ""we did not send you but as a mercy to all the nations.1or â€˜to all the worlds.â€™"",...'}, {'surah_no': '23', 'verse_no': '75', 'content': ' ""should we have mercy upon them and remove their distress from them, they would surely persist, bewildered in their rebellion."",...'}, {'surah_no': '23', 'verse_no': '76', 'content': ' ""certainly we have seized them with punishment, yet they neither humbled themselves to their lord, nor did they entreat [him for mercy]."",...'}, {'surah_no': '23', 'verse_no': '109', 'content': ' ""indeed there was a part of my servants who would say, â€œour lord! we have believed. so forgive us, and have mercy on us, and you are the best of the merciful.â€"",...'}, {'surah_no': '23', 'verse_no': '118', 'content': ' ""say, â€˜my lord, forgive and have mercy, and you are the best of the merciful.â€™"",...'}, {'surah_no': '24', 'verse_no': '10', 'content': ' ""were it not for allahâ€™s grace and his mercy upon you, and that allah is all-clement, all-wiseâ€¦ .1ellipsis. for the omitted part of the sentence see verses 14 & 21 below."",...'}, {'surah_no': '24', 'verse_no': '14', 'content': ' ""were it not for allahâ€™s grace and his mercy upon you in this world and the hereafter, there would have befallen you a great punishment for what you ventured into,"",...'}, {'surah_no': '24', 'verse_no': '20', 'content': ' ""were it not for allahâ€™s grace and his mercy upon you, and that allah is all-kind, all-merciful."",...'}, {'surah_no': '24', 'verse_no': '21', 'content': ' ""o you who have faith! do not follow in satanâ€™s steps. whoever follows in satanâ€™s steps [should know that] he indeed prompts [you to commit] indecent acts and wrong. were it not for allahâ€™s grace and his mercy upon you, not one of you would ever be pure. but allah purifies whomever he wishes, and allah is all-hearing, all-knowing."",...'}, {'surah_no': '24', 'verse_no': '56', 'content': ' ""maintain the prayer and give the zakÄt, and obey the apostle so that you may receive [allahâ€™s] mercy."",...'}, {'surah_no': '25', 'verse_no': '48', 'content': ' ""and it is he who sends the winds as harbingers of his mercy, and we send down from the sky purifying water,"",...'}, {'surah_no': '27', 'verse_no': '19', 'content': ' ""whereat he smiled, amused at its words, and he said, â€˜my lord! inspire me to give thanks for your blessing with which you have blessed me and my parents, and that i may do righteous deeds which may please you, and admit me, by your mercy, among your righteous servants.â€™"",...'}, {'surah_no': '27', 'verse_no': '46', 'content': ' ""he said, â€˜o my people! why do you press for evil sooner than for good? why do you not plead to allah for forgiveness so that you may receive his mercy?â€™"",...'}, {'surah_no': '27', 'verse_no': '63', 'content': ' ""is he who guides you in the darkness of land and sea and who sends the winds as harbingers of his mercyâ€¦ ? what! is there a god besides allah? exalted is allah above [having] any partners they ascribe [to him]."",...'}, {'surah_no': '27', 'verse_no': '77', 'content': ' ""and it is indeed a guidance and mercy for the faithful."",...'}, {'surah_no': '28', 'verse_no': '43', 'content': ' ""certainly we gave moses the book, after we had destroyed the former generations, as [a set of] eye-openers for mankind, and as guidance and mercy so that they may take admonition."",...'}, {'surah_no': '28', 'verse_no': '46', 'content': ' ""and you were not on the side of the mount when we called out [to moses], but [we have sent you as] a mercy from your lord that you may warn a people to whom there did not come any warner before you, so that they may take admonition.1that is, â€˜the faithless imagine that this teaching is of your own contrivance. but it was we who sent our revelations to moses and gave him the scripture and the law before you even came into the world, nor it was you who lived among the people of midian to recite our signs to them. it was we who have been sending the apostles before you, and it is we who have sent you as a mercy and guidance to mankind, after the passage of time had taken its toll and obscured the path of the prophets.â€™"",...'}, {'surah_no': '28', 'verse_no': '73', 'content': ' ""out of his mercy he has made for you the night and the day, that you may rest therein and that you may seek from his grace and so that you may give thanks."",...'}, {'surah_no': '28', 'verse_no': '86', 'content': ' ""you did not expect that the book would be delivered to you; but it was a mercy from your lord. so do not be ever an advocate of the faithless."",...'}, {'surah_no': '29', 'verse_no': '21', 'content': ' ""he will punish whomever he wishes and have mercy on whomever he wishes, and to him you will be returned."",...'}, {'surah_no': '29', 'verse_no': '23', 'content': ' ""those who deny the signs of allah and the encounter with him, they have despaired of my mercy, and for such there is a painful punishment."",...'}, {'surah_no': '29', 'verse_no': '51', 'content': ' ""does it not suffice them that we have sent down to you the book which is recited to them? there is indeed a mercy and admonition in that for a people who have faith."",...'}, {'surah_no': '30', 'verse_no': '21', 'content': ' ""and of his signs is that he created for you mates from your own selves that you may take comfort in them, and he ordained affection and mercy between you. there are indeed signs in that for a people who reflect."",...'}, {'surah_no': '30', 'verse_no': '33', 'content': ' ""when distress befalls people, they supplicate their lord, turning to him in penitence. then, when he lets them taste his mercy, behold, a part of them ascribe partners to their lord,"",...'}, {'surah_no': '30', 'verse_no': '36', 'content': ' ""and when we let people taste [our] mercy, they exult in it; but should an ill visit them because of what their hands have sent ahead, behold, they become despondent!"",...'}, {'surah_no': '30', 'verse_no': '46', 'content': ' ""and of his signs is that he sends the winds as bearers of good news and to let you taste of his mercy, and that the ships may sail by his command, and that you may seek of his grace, and so that you may give [him] thanks."",...'}, {'surah_no': '30', 'verse_no': '50', 'content': ' ""so observe the effects of allahâ€™s mercy...'}, {'surah_no': '31', 'verse_no': '3', 'content': ' ""a guidance and mercy for the virtuous,"",...'}, {'surah_no': '33', 'verse_no': '17', 'content': ' ""say, â€˜who is it that can protect you from allah should he desire to cause you ill, or desire to grant you mercy?â€™ they will not find for themselves any protector or helper besides allah."",...'}, {'surah_no': '35', 'verse_no': '2', 'content': ' ""whatever mercy allah unfolds for the people, no one can withhold it; and whatever he withholds no one can release it after him,1 and he is the all-mighty, the all-wise.that is, after his withholding it. or â€˜no one can release it except him.â€™"",...'}, {'surah_no': '36', 'verse_no': '44', 'content': ' ""âŽ¯except by a mercy from us and for an enjoyment until some time."",...'}, {'surah_no': '36', 'verse_no': '45', 'content': ' ""and when they are told, â€˜beware of that which is before you and that which is behind you,1 so that you may receive [his] mercy.â€™2see 34...'}, {'surah_no': '38', 'verse_no': '9', 'content': ' ""do they possess the treasuries of the mercy of your lord, the all-mighty, the all-munificent?"",...'}, {'surah_no': '38', 'verse_no': '43', 'content': ' ""and we gave [back] his family to him along with others like them, as a mercy from us and an admonition for those who possess intellect."",...'}, {'surah_no': '39', 'verse_no': '9', 'content': ' ""is he who supplicates1 in the watches of the night, prostrating and standing, apprehensive of the hereafter and expecting the mercy of his lord â€¦ ?2 say, â€˜are those who know equal to those who do not know?â€™ only those who possess intellect take admonition.or â€˜is obedient.â€™ellipsis. the omitted phrase is, â€˜like someone who is not such?â€™"",...'}, {'surah_no': '39', 'verse_no': '38', 'content': ' ""if you ask them, â€˜who created the heavens and the earth?â€™ they will surely say, â€˜allah.â€™ say, â€˜have you considered what you invoke besides allah? should allah desire some distress for me, can they remove the distress visited by him? or should he desire some mercy for me, can they withhold his mercy?â€™ say, â€˜allah is sufficient for me. in him let all the trusting put their trust.â€™"",...'}, {'surah_no': '39', 'verse_no': '53', 'content': ' ""say [that allah declares,] â€˜o my servants who have committed excesses against their own souls, do not despair of the mercy of allah. indeed allah will forgive all sins. indeed he is the all-forgiving, the all-merciful."",...'}, {'surah_no': '40', 'verse_no': '7', 'content': ' ""those who bear the throne, and those around it, celebrate the praise of their lord and have faith in him, and they plead for forgiveness for the faithful...'}, {'surah_no': '40', 'verse_no': '9', 'content': ' ""save them from the ills; and whomever you save from the ills that day,1 you will have had mercy upon him, and that is the great success.â€™that is, the day of judgement."",...'}, {'surah_no': '41', 'verse_no': '50', 'content': ' ""and if we let him have a taste of our mercy after distress has befallen him, he will surely say, â€˜this is my due! i do not think the hour will ever set in, and in case i am returned to my lord, i will indeed have the best [reward] with him.â€™ but we will surely inform the faithless about what they have done, and will surely make them taste a harsh punishment."",...'}, {'surah_no': '42', 'verse_no': '8', 'content': ' ""had allah wished,he would have surely made them one community;but he admits whomever he wishesinto his mercy,and the wrongdoersdo not have any guardian or helper."",...'}, {'surah_no': '42', 'verse_no': '28', 'content': ' ""it is he who sends down the rainafter they have been despondent,and unfolds his mercy,and he is the guardian, the all-laudable."",...'}, {'surah_no': '42', 'verse_no': '48', 'content': ' ""but if they disregard [your warnings],we have not sent you as a keeper over them.your duty is only to communicate.indeed when we let man taste our mercy,he exults in it;but should an ill visit thembecause of what their hands have sent ahead,then man is indeed very ungrateful."",...'}, {'surah_no': '43', 'verse_no': '32', 'content': ' ""is it they who dispense the mercy of your lord?it is we who have dispensed among themtheir livelihood in the present life,and raised some of them above others in rank,so that some may take others into service,and your lordâ€™s mercy is better than what they amass."",...'}, {'surah_no': '44', 'verse_no': '6', 'content': ' ""as a mercy from your lordâ€”indeed he is the all-hearing, the all-knowingâ€”"",...'}, {'surah_no': '44', 'verse_no': '42', 'content': ' ""except for him on whom allah has mercy.indeed he is the all-mighty, the all-merciful."",...'}, {'surah_no': '45', 'verse_no': '30', 'content': ' ""as for those who have faithand do righteous deeds,their lord will admit them into his mercy.that is the manifest success!"",...'}, {'surah_no': '46', 'verse_no': '12', 'content': ' ""yet before it the book of moseswas a guide and a mercy,and this is a book which confirms it,in the arabic language,to warn those who do wrong,and is a [bearer of] good news for the virtuous."",...'}, {'surah_no': '49', 'verse_no': '10', 'content': ' ""the faithful are indeed brothers.therefore make peace between your brothersand be wary of allah,so that you may receive [his] mercy."",...'}, {'surah_no': '57', 'verse_no': '13', 'content': ' ""the day the hypocrites, men and women, will sayto the faithful,â€˜please let up on us,that we may glean something from your light!â€™they will be told...'}, {'surah_no': '57', 'verse_no': '27', 'content': ' ""then we followed them up with our apostlesand we followed [them] with jesus son of mary,and we gave him the evangel,and we put in the hearts of those who followed himkindness and mercy.but as for monasticism, they innovated itâ€”we had not prescribed it for themâ€”only seeking allahâ€™s pleasure.yet they did not observe it with due observance.so we gave to the faithful among themtheir [due] reward,but many of them are transgressors."",...'}, {'surah_no': '67', 'verse_no': '28', 'content': ' ""say, â€˜tell me,whether allah destroys me and those with me,or he has mercy on us,who will shelter the faithlessfrom a painful punishment?â€™"",...'}, {'surah_no': '75', 'verse_no': '27', 'content': ' ""and it is said, â€˜who will take him up?â€™1that is, by the angels of mercy and the angels of wrath present at the side of the dying person, as to which of them will take charge of him. or those who are present by the side of the dying person say, â€˜where is the medicine man?â€™"",...'}, {'surah_no': '76', 'verse_no': '31', 'content': ' ""he admits whomever he wishes into his mercy,and as for the wrongdoers,he has prepared for them a painful punishment."",...'}]
",0,0
"I would like to inquire about five types of information on biodiversity and specific plants, including their environment, ecological characteristics and conservation status, for ecological conservation or scientific research.

","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
BASE_URL = ""https://daily-knowledge.p.rapidapi.com/""

HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""daily-knowledge.p.rapidapi.com""
}

def fetch_data(endpoint):
    """"""Fetch data from a specified endpoint.""""""
    url = BASE_URL + endpoint
    response = requests.get(url, headers=HEADERS)
    if response.status_code == 200:
        print(f""Raw response from {endpoint}:"", response.json())  # Debug: Print raw response data
        return response.json()
    else:
        print(f""Failed to retrieve data: {response.status_code}"")
        return None

def display_biodiversity_info():
    """"""Display biodiversity and conservation information.""""""
    data = fetch_data(""sustainable-development-goals-en.json"")
    if isinstance(data, dict):  # Verify if data is a dictionary
        print(""\nBiodiversity Goals and Conservation Information:"")
        print(f""Goal: {data.get('goal', 'Unknown Goal')}"")
        print(f""Description: {data.get('description', 'No description available')}"")
    else:
        print(""Data format is incorrect or no biodiversity data available."")

def display_plant_info():
    """"""Display ecological and growth details of specific plants.""""""
    data = fetch_data(""plants-de.json"")
    if isinstance(data, dict):  # Verify if data is a dictionary
        print(""\nPlant Ecological and Growth Details:"")
        name = data.get(""name"", ""Unknown Plant"")
        habitat = data.get(""descrData"", {}).get(""infoGrow"", ""Habitat details not available"")
        description = data.get(""descrData"", {}).get(""infoShort"", ""Description not available"")

        print(f""Name: {name}"")
        print(f""Habitat: {habitat}"")
        print(f""Description: {description}"")
    else:
        print(""Data format is incorrect or no plant data available."")

def display_country_info():
    """"""Display country context information for biodiversity.""""""
    data = fetch_data(""countries-en.json"")
    if isinstance(data, dict):  # Verify if data is a dictionary
        print(""\nCountry Context for Biodiversity:"")
        country_name = data.get(""name"", ""Unknown Country"")
        capital = data.get(""capital"", ""Unknown Capital"")
        language = data.get(""languages"", ""Language information not available"")

        print(f""Country: {country_name}"")
        print(f""Capital: {capital}"")
        print(f""Languages: {language}"")
    else:
        print(""Data format is incorrect or no country data available."")

if __name__ == '__main__':
    # Step 1: Get information on biodiversity and conservation
    display_biodiversity_info()

    # Step 2: Query specific plant details
    display_plant_info()

    # Step 3: (Optional) Get geographical and country context if needed
    display_country_info()
","Raw response from sustainable-development-goals-en.json: {'title': 'Clean Water and Sanitation', 'slug': 'clean-water-and-sanitation', 'number': 6, 'backgroundColor': '#26bde2', 'image': '<svg viewBox=""0 0 1000 510""><path fill=""currentColor"" fill-rule=""evenodd"" d=""M659.2 79c-10-4.3-22.3-2.8-34.3 2-45 18.3-76 25-123 10-30-9.3-58-28.8-91-17-22 8-42.6 15.5-67 15.3L330 16h339.4l-10.2 63zm-115 217.5s-16.6 17-43.4 15c-27 2-43.3-15-43.3-15-27.2-27-17.5-61-8-78.4 5.8-10.3 47.5-73 51-78.3v-.5h.3v.4c3.6 5.3 45.5 68 51.3 79 10 17.4 19.8 51.2-8 78.3zm140-295c-.7-1-2-1.5-3.2-1.5H318.2c-1.2 0-2.4.5-3.2 1.4-.8 1-1 2.3-1 3.5l61 368c.3 2 2 3.4 4.2 3.4H454l5 60h-31.4L501 510l73-73h-31.5l5-60H624c2.2 0 4-1.6 4.2-3.7L685.2 5c.3-1.4 0-2.6-1-3.6z""></path></svg>\r\n', 'introduction': 'Ensure availability and sustainable management of water and sanitation for all.', 'manifest': 'One in three people live without sanitation. This is causing  unnecessary disease and death. Although huge strides have been made with access to clean drinking water, lack of sanitation is undermining these advances. If we provide affordable equipment and education in hygiene practices, we can stop this senseless suffering and loss of life.', 'targets': [{'title': 'Safe and Affordable Drinking Water', 'body': 'By 2030, achieve universal and equitable access to safe and affordable drinking water for all.', 'pictogram': 'https://prismic-io.s3.amazonaws.com/globalgoals%2Fd50d609e-266d-4297-a89c-8d91302670d5_goal_6.1_rgb_ng.svg'}, {'title': 'End Open Defecation and Provide Access to Sanitation and Hygiene', 'body': 'By 2030, achieve access to adequate and equitable sanitation and hygiene for all and end open defecation, paying special attention to the needs of women and girls and those in vulnerable situations.', 'pictogram': 'https://prismic-io.s3.amazonaws.com/globalgoals%2Fc0859998-99f8-46db-ae7a-e20050e1f2a6_goal_6.2_rgb_ng.svg'}, {'title': 'Improve Water Quality, Wastewater Treatment and Safe Reuse', 'body': 'By 2030, improve water quality by reducing pollution, eliminating dumping and minimizing release of hazardous chemicals and materials, halving the proportion of untreated wastewater and substantially increasing recycling and safe reuse globally.', 'pictogram': 'https://prismic-io.s3.amazonaws.com/globalgoals%2Fbf00f938-046c-41a5-95b1-df597942c7a6_goal_6.3_rgb_ng.svg'}, {'title': 'Increase Water-Use Efficiency and Ensure Freshwater Supplies', 'body': 'By 2030, substantially increase water-use efficiency across all sectors and ensure sustainable withdrawals and supply of freshwater to address water scarcity and substantially reduce the number of people suffering from water scarcity.', 'pictogram': 'https://prismic-io.s3.amazonaws.com/globalgoals%2F9f3c02f5-bd50-486a-8897-8282c199946b_goal_6.4_rgb_ng.svg'}, {'title': 'Implement Integrated Water Resources Management', 'body': 'By 2030, implement integrated water resources management at all levels, including through transboundary cooperation as appropriate.', 'pictogram': 'https://prismic-io.s3.amazonaws.com/globalgoals%2F673c361b-6f50-48b6-ac82-3798f556c90b_goal_6.5_rgb_ng.svg'}, {'title': 'Protect and Restore Water-Related Ecosystems', 'body': 'By 2020, protect and restore water-related ecosystems, including mountains, forests, wetlands, rivers, aquifers and lakes.', 'pictogram': 'https://prismic-io.s3.amazonaws.com/globalgoals%2F5201748a-514b-4b12-b6df-a95e3b5ded4c_goal_6.6_rgb_ng.svg'}, {'title': 'Expand Water and Sanitation Support to Developing Countries', 'body': 'By 2030, expand international cooperation and capacity-building support to developing countries in water- and sanitation-related activities and programmes, including water harvesting, desalination, water efficiency, wastewater treatment, recycling and reuse technologies.', 'pictogram': 'https://prismic-io.s3.amazonaws.com/globalgoals%2F379ebc32-444a-4348-8b43-39d9024de144_goal_6.a_rgb_ng.svg'}, {'title': 'Support Local Engagement in Water and Sanitation Management', 'body': 'Support and strengthen the participation of local communities in improving water and sanitation management.', 'pictogram': 'https://prismic-io.s3.amazonaws.com/globalgoals%2F81e1b132-ec02-4f87-81f0-d8070ee1e073_goal_6.b_rgb_ng.svg'}], 'tips': ['Find a Goal 6 charity you want to support. Any donation, big or small, can make a difference! See the ""Get Involved"" section above.', '4 billion people lack access to basic sanitation services. Lend your voice to talk about the lack of toilets in many communities around the world!', 'Run a campaign on hygiene. Raise awareness about the hygiene issues in your community through social media, school/university campaign or even a campaign in the neighborhood you live in.', ""Don't overuse water. Make sure to close a tap when washing dishes, take short showers - Bathtubs require gallons more water than a 5-10 minute shower. Consider getting a water meter to be aware of your water usage."", 'Organize a clean up project for rivers and oceans. Engage your whole community to clean up a local river, seaside or an ocean.', 'Stay informed. Follow your local news and stay in touch with the Global Goals online or on social media at\xa0@TheGlobalGoals.']}

Biodiversity Goals and Conservation Information:
Goal: Unknown Goal
Description: No description available
Raw response from plants-de.json: {'id': 'alpenfett', 'name': 'Alpen-Fettkraut', 'basicData': {'id': 'alpenfett', 'name': 'Alpen-Fettkraut', 'scName': 'PinguÃ­cula alpÃ­na', 'altNames': '', 'search': 'Pinguicula alpina', 'gatId': 'ping', 'famId': 'lenti', 'home': '17', 'climate': '4', 'life': '4', 'grow': '8', 'bloomCol': '1', 'pollen': '0', 'nectar': '0', 'props': '0', 'shows': '0', 'height': '15', 'dist': '0', 'light': '1', 'ground': '14', 'imgType': '2', 'imgNotes': 'Â© Griensteidl, Wikimedia Commons', 'imgLic': None, 'imgData': ['https://www.smagy.de/images/plantsOther/lenti/ping/alpenfett_01.jpg'], 'timeSeeds1': '0', 'timeSeeds2': '0', 'timeBloom1': '4', 'timeBloom2': '7', 'timeFruit1': '7', 'timeFruit2': '9', 'timeSeeds': '', 'timeBloom': 'Apr - Jul', 'timeFruit': 'Jul - Sep', 'gatName': '', 'gatScName': 'PinguÃ­cula', 'famName': 'WasserschlauchgewÃ¤chse', 'famScName': 'LentibulariÃ¡ceae'}, 'descrData': {'infoShort': 'Das Alpen-Fettkraut ist in den Alpen, in Skandinavien sowie in Teilen Asiens beheimatet. Die Pflanze zÃ¤hlt zu den Fleisch fressenden Pflanzen. Insekten bleiben an der BlattoberflÃ¤che kleben.', 'infoGrow': 'Das Alpen-Fettkraut ist in den Alpen sowie im Norden Skandinaviens zu finden. AuÃŸerdem ist die Pflanze in Sibirien, in China und im Himalaya in HÃ¶hen bis zu 4000 Metern beheimatet. Sie wÃ¤chst auf steinigen RasenflÃ¤chen sowie in Flachmooren und bevorzugt kalkhaltige, durchlÃ¤ssige, feuchte BÃ¶den.', 'infoAppear': 'Die Wurzeln der Pflanze sind lediglich 1 bis 2 cm lang. Die fleischigen LaubblÃ¤tter bilden am Boden eine Rosette. Die BlattoberflÃ¤che ist klebrig, wodurch Insekten gefangen werden. Die Verdauung der Insekten erfolgt mittels eines Enzyms, das aus DrÃ¼sen an der BlattoberflÃ¤che ausgeschieden wird. Die BlÃ¤tter kÃ¶nnen sich bis zur Mitte einrollen.', 'infoBloom': 'Die EinzelblÃ¼ten entspringen aus der Mitte der Rosette. Eine Pflanze kann mehr als 10 BlÃ¼ten bilden, allerdings dauert es mehrere Jahre, bis die Pflanze das erste Mal blÃ¼ht. Die BestÃ¤ubung erfolgt durch Fliegen.', 'infoRepro': 'Nach der Befruchtung werden KapselfrÃ¼chte gebildet, die winzige Samen enthalten. Manche Pflanzen vermehren sich auch vegetativ durch Brutzwiebeln, die nach der BlÃ¼te in den Blattachseln gebildet werden.', 'infoMisc': '', 'infoUse': ''}, 'medData': [], 'insectData': []}

Plant Ecological and Growth Details:
Name: Alpen-Fettkraut
Habitat: Das Alpen-Fettkraut ist in den Alpen sowie im Norden Skandinaviens zu finden. AuÃŸerdem ist die Pflanze in Sibirien, in China und im Himalaya in HÃ¶hen bis zu 4000 Metern beheimatet. Sie wÃ¤chst auf steinigen RasenflÃ¤chen sowie in Flachmooren und bevorzugt kalkhaltige, durchlÃ¤ssige, feuchte BÃ¶den.
Description: Das Alpen-Fettkraut ist in den Alpen, in Skandinavien sowie in Teilen Asiens beheimatet. Die Pflanze zÃ¤hlt zu den Fleisch fressenden Pflanzen. Insekten bleiben an der BlattoberflÃ¤che kleben.
Raw response from countries-en.json: {'name': 'Mozambique', 'capital': 'Maputo', 'countryCode': 'MZ', 'currency': 'Mozambican Metical', 'languages': 'Portuguese', 'tld': '.mz', 'flag': 'data:image/svg+xml;base64,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'}

Country Context for Biodiversity:
Country: Mozambique
Capital: Maputo
Languages: Portuguese",1,1
"I want to know about five different logical fallacies (such as the ""either/or"" fallacy), including the definition, examples, and background of each fallacy in order to better understand common mistakes in logical reasoning.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def logical_fallacies():
    """"""Retrieve details about a specific logical fallacy.""""""
    url = ""https://daily-knowledge.p.rapidapi.com/logical-fallacies-en.json""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""daily-knowledge.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        return response.json()
    else:
        print(""Failed to retrieve logical fallacies:"", response.status_code)
        return None

def display_fallacy(fallacy_data):
    """"""Display details of a logical fallacy.""""""
    if not fallacy_data:
        print(""No data available for logical fallacies."")
        return

    # Display information for a single fallacy
    name = fallacy_data.get(""name"", ""Unknown Fallacy"")
    definition = fallacy_data.get(""head"", ""No definition available."")
    example = fallacy_data.get(""example"", ""No example available."")
    background = fallacy_data.get(""description"", ""No background information available."")

    print(f""Fallacy Name: {name}"")
    print(f""Definition: {definition}"")
    print(f""Example: {example}"")
    print(f""Background: {background}"")

if __name__ == '__main__':
    # Retrieve and display details of a logical fallacy
    fallacy_data = logical_fallacies()

    if fallacy_data:
        display_fallacy(fallacy_data)
    else:
        print(""Could not retrieve logical fallacy information."")
","Fallacy Name: ambiguity
Definition: You used a double meaning or ambiguity of language to mislead or misrepresent the truth.
Example: When the judge asked the defendant why he hadn't paid his parking fines, he said that he shouldn't have to pay them because the sign said 'Fine for parking here' and so he naturally presumed that it would be fine to park there.
Background: Politicians are often guilty of using ambiguity to mislead and will later point to how they were technically not outright lying if they come under scrutiny. The reason that it qualifies as a fallacy is that it is intrinsically&nbsp;misleading.
",1,1
"I need to retrieve rentable properties in a specific location and allow filtering based on criteria such as price, number of rooms and whether they are furnished or not. Be able to access detailed information about each property, including price, location and other features. In addition, information about the relevant agents managing each property is displayed, including agent name and location. You also need to be able to handle location information correctly, whether it's a single object or a list.","import requests

# Define API Key
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


# Function to retrieve property list based on given filters
def properties_list(location_external_ids, purpose=""for-rent"", min_price=None, max_price=None, min_rooms=None,
                    max_rooms=None, furnished=None):
    url = ""https://bayut.p.rapidapi.com/properties/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""bayut.p.rapidapi.com""
    }
    querystring = {
        ""locationExternalIDs"": location_external_ids,
        ""purpose"": purpose,
        ""hitsPerPage"": ""10"",
        ""page"": ""0"",
        ""lang"": ""en"",
        ""sort"": ""price-asc""
    }

    # Apply optional filters
    if min_price is not None:
        querystring[""priceMin""] = str(min_price)
    if max_price is not None:
        querystring[""priceMax""] = str(max_price)
    if min_rooms is not None:
        querystring[""roomsMin""] = str(min_rooms)
    if max_rooms is not None:
        querystring[""roomsMax""] = str(max_rooms)
    if furnished is not None:
        querystring[""furnishingStatus""] = ""furnished"" if furnished else ""unfurnished""

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


# Function to retrieve detailed information for a specific property
def properties_detail(external_id):
    url = ""https://bayut.p.rapidapi.com/properties/detail""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""bayut.p.rapidapi.com""
    }
    querystring = {
        ""externalID"": external_id
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


# Function to retrieve agency details based on a query
def agencies_list(agency_query=None):
    url = ""https://bayut.p.rapidapi.com/agencies/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""bayut.p.rapidapi.com""
    }
    querystring = {
        ""hitsPerPage"": ""10"",
        ""page"": ""0"",
        ""lang"": ""en""
    }

    if agency_query:
        querystring[""query""] = agency_query

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


# Main code to demonstrate the functionality
if __name__ == '__main__':
    # Retrieve properties based on specific filters
    properties = properties_list(
        location_external_ids=""5002,6020"",  # Example location IDs
        min_price=50000,
        max_price=150000,
        min_rooms=2,
        furnished=True
    )

    # Iterate through the retrieved properties and get detailed information
    for property_item in properties.get('hits', []):
        external_id = property_item.get('externalID')
        if external_id:
            # Retrieve detailed information about the property
            property_details = properties_detail(external_id)

            # Display property details with corrected location handling
            print(f""\nProperty Title: {property_item.get('title')}"")
            print(f""Property Price: {property_item.get('price')}"")

            # Handle 'location' field correctly if it's a list
            location_data = property_item.get('location', [])
            if isinstance(location_data, list):
                # Join all location names if it's a list of location objects
                location_names = ', '.join([loc.get('name', '') for loc in location_data if loc.get('name')])
                print(f""Property Location: {location_names}"")
            else:
                # If it's not a list, handle as a dictionary
                print(f""Property Location: {location_data.get('name')}"")
","Property Title: Burj + Fountain View I State of Art Furnishing
Property Price: 50000.0
Property Location: UAE, Dubai, Downtown Dubai, Opera Grand

Property Title: Exquisite 2BR Luxury Retreat at Fairmont North, Palm Jumeirah â€“ Ground Floor Elegance
Property Price: 50000.0
Property Location: UAE, Dubai, Palm Jumeirah, The Fairmont Palm Residences, The Fairmont Palm Residence North

Property Title: Luxury Unit in Bluewater | Full Eye View
Property Price: 50000.0
Property Location: UAE, Dubai, Bluewaters Island, Bluewaters Residences, Bluewaters Residences 9

Property Title: Exclusive 4BR Full Burj & Fountain view - Downtown
Property Price: 50000.0
Property Location: UAE, Dubai, Downtown Dubai, Burj Royale

Property Title: Splendid Views | Hotel Facilities | Beach Access
Property Price: 50000.0
Property Location: UAE, Dubai, Jumeirah Beach Residence (JBR), The Address Residences Jumeirah Resort and Spa, Jumeirah Gate Tower 1

Property Title: Fairmont North | Sea View Newly Renovated 2BR | Huge Terrace
Property Price: 50000.0
Property Location: UAE, Dubai, Palm Jumeirah, The Fairmont Palm Residences, The Fairmont Palm Residence North

Property Title: Livbnb Suite - Luxury 3+1 with Burj View, Downtown
Property Price: 50000.0
Property Location: UAE, Dubai, Downtown Dubai, Vida Residence Downtown

Property Title: Utility Bills Included  | Sophisticated 4-Bedroom Villa with Private Garden in Maple 1
Property Price: 50000.0
Property Location: UAE, Dubai, Dubai Hills Estate, Maple, Maple 1

Property Title: Sunrise Bay Suite - 3 + 1 w/ Marina Skyline View Private Beach by Livbnb
Property Price: 50000.0
Property Location: UAE, Dubai, Dubai Harbour, Emaar Beachfront, Sunrise Bay

Property Title: Modern Luxurious 2 Bed Apartment | Bills Included
Property Price: 50000.0
Property Location: UAE, Dubai, Dubai Harbour, Emaar Beachfront, Beach Vista, Beach Vista 1
",1,1
"I want to explore geo-location through microsite search and provincial filtering. Once a location is selected, you can search for a list of rooms for rent or commercial properties available in that location. I wanted to filter the room listings based on criteria such as relevance, price and availability, while also being able to browse detailed commercial property information. It should support different languages and allow sorting according to my preferences.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_microsite_locations(microsite_short_name, location_code, locale, operation, location_id):
    url = ""https://idealista7.p.rapidapi.com/getmicrositelocations""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""idealista7.p.rapidapi.com""
    }
    querystring = {
        ""micrositeShortName"": microsite_short_name,
        ""location"": location_code,
        ""locale"": locale,
        ""operation"": operation,
        ""locationId"": location_id
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def list_rooms(order=""relevance"", location_id=None, location_name=None, num_page=1, max_items=10, location=""es"", locale=""en""):
    url = ""https://idealista7.p.rapidapi.com/listrooms""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""idealista7.p.rapidapi.com""
    }
    querystring = {
        ""order"": order,
        ""locationId"": location_id,
        ""locationName"": location_name,
        ""numPage"": str(num_page),
        ""maxItems"": str(max_items),
        ""location"": location,
        ""locale"": locale
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def list_commercial_properties(order=""relevance"", operation=""sale"", location_id=None, max_items=""10"", location_name=None, num_page=1, location=""es"", locale=""en""):
    url = ""https://idealista7.p.rapidapi.com/listcommercialproperties""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""idealista7.p.rapidapi.com""
    }
    querystring = {
        ""order"": order,
        ""operation"": operation,
        ""locationId"": location_id,
        ""maxItems"": str(max_items),
        ""locationName"": location_name,
        ""numPage"": str(num_page),
        ""location"": location,
        ""locale"": locale
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

if __name__ == '__main__':
    # Step 1: Retrieve locations from a specific microsite
    microsite_data = get_microsite_locations(
        microsite_short_name=""sumainmobiliaria"",
        location_code=""es"",
        locale=""en"",
        operation=""sale"",
        location_id=""0-EU-ES-28""
    )

    # Display provinces and municipalities
    if 'provinces' in microsite_data:
        print(""Available Provinces:"")
        for province in microsite_data['provinces']:
            print(f""- {province['locationName']} (ID: {province['locationId']})"")

    if 'municipalities' in microsite_data:
        print(""Available Municipalities:"")
        for municipality in microsite_data['municipalities']:
            print(f""- {municipality['locationName']} (ID: {municipality['locationId']})"")

    # Step 2: List rooms available in a chosen location (e.g., Madrid)
    room_listings = list_rooms(
        order=""price"",
        location_id=""0-EU-ES-28-07-001-079"",  # Example location ID for Madrid
        location_name=""Madrid"",
        num_page=1,
        max_items=5,
        locale=""en""
    )

    print(""Available Room Listings in Madrid:"")
    for idx, room in enumerate(room_listings.get('elementList', []), start=1):
        print(f""{idx}. {room.get('suggestedTexts', {}).get('title', 'No Title')} - Price: {room.get('price')}"")

    # Step 3: List commercial properties in the same location
    commercial_properties = list_commercial_properties(
        order=""relevance"",
        operation=""sale"",
        location_id=""0-EU-ES-28-07-001-079"",
        max_items=""5"",
        location_name=""Madrid"",
        locale=""en""
    )

    print(""Available Commercial Properties in Madrid:"")
    for idx, property_item in enumerate(commercial_properties.get('elementList', []), start=1):
        print(f""{idx}. {property_item.get('suggestedTexts', {}).get('title', 'No Title')} - Price: {property_item.get('price')}"")
","Available Provinces:
- Madrid (ID: 0-EU-ES-28)
Available Municipalities:
- Ciudalcampo, Madrid (ID: 0-EU-ES-28-01-008-504)
- La Moraleja, Madrid (ID: 0-EU-ES-28-01-007-502)
- Madrid, Madrid (ID: 0-EU-ES-28-07-001-079)
Available Room Listings in Madrid:
1. Room in calle ConcepciÃ³n de la Oliva, 12 - Price: 380.0
2. Room in calle de RÃ­os Rosas, 51 - Price: 630.0
3. Room in Braile, 20 - Price: 450.0
4. Room in aligustre, 8 - Price: 730.0
5. Room in calle JosÃ© Silva, 3 - Price: 610.0
Available Commercial Properties in Madrid:
1. Commercial premises in paseo de La Habana - Price: 3440000.0
2. Commercial premises in calle de Fortuny - Price: 700000.0
3. Commercial premises in calle de Agastia - Price: 5950000.0
4. Commercial premises in calle de Ferraz - Price: 850000.0
5. Commercial premises in paseo de San Francisco de Sales, 27 - Price: 675000.0
",1,1
"I retrieve listings of properties for sale in the city of Oxford, Oxfordshire, get details about estate agents in the Oxford area, and filter properties according to various criteria such as property type and price range. Be able to explore detailed property characteristics, including images, agent details and geographical location, while allowing sorting by criteria such as current listings or relevance.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_properties_v2_list(location_value, location_identifier, furnished_state=""Any"", sort_order=""newest_listings"", page=1):
    """"""
    Retrieve a detailed list of residential properties for sale in Oxford, Oxfordshire.
    """"""
    url = ""https://zoopla.p.rapidapi.com/properties/v2/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""zoopla.p.rapidapi.com""
    }
    querystring = {
        ""locationValue"": location_value,
        ""locationIdentifier"": location_identifier,
        ""furnishedState"": furnished_state,
        ""sortOrder"": sort_order,
        ""page"": str(page)
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_agents_list(location, search_type=""estate-agents"", agents_sort=""a_z"", page_number=1):
    """"""
    Retrieve detailed information about real estate agents in Oxford, including statistics and contact details.
    """"""
    url = ""https://zoopla.p.rapidapi.com/agents/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""zoopla.p.rapidapi.com""
    }
    querystring = {
        ""location"": location,
        ""search_type"": search_type,
        ""agents_sort"": agents_sort,
        ""pn"": str(page_number)
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_properties_list(search_location, search_location_subtext, region_type, channel=""buy"", sort_type=""relevance"", page=1, page_size=10, minimum_price=None, maximum_price=None):
    """"""
    Retrieve a list of properties based on filters like location, type, and price range.
    """"""
    url = ""https://realty-in-au.p.rapidapi.com/properties/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""realty-in-au.p.rapidapi.com""
    }
    querystring = {
        ""page"": str(page),
        ""pageSize"": str(page_size),
        ""sortType"": sort_type,
        ""channel"": channel,
        ""searchLocation"": search_location,
        ""searchLocationSubtext"": search_location_subtext,
        ""type"": region_type
    }

    # Optional price filters
    if minimum_price:
        querystring[""minimumPrice""] = str(minimum_price)
    if maximum_price:
        querystring[""maximumPrice""] = str(maximum_price)

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

if __name__ == '__main__':
    # Step 1: Retrieve property listings in Oxford, Oxfordshire
    properties_data = get_properties_v2_list(
        location_value=""Oxford, Oxfordshire"",
        location_identifier=""oxford"",
        furnished_state=""Any"",
        sort_order=""newest_listings"",
        page=1
    )

    # Display basic details of property listings
    if properties_data['success']:
        print(""Property Listings in Oxford, Oxfordshire:"")
        for idx, property_item in enumerate(properties_data['data']['listings']['regular'], start=1):
            print(f""{idx}. {property_item['title']} - Price: Â£{property_item['pricing']['value']} at {property_item['address']}"")

    # Step 2: Retrieve information about real estate agents in Oxford
    agents_data = get_agents_list(
        location=""oxford"",
        search_type=""estate-agents"",
        agents_sort=""a_z"",
        page_number=1
    )

    # Display agent details
    print(""\nReal Estate Agents in Oxford:"")
    for idx, agent in enumerate(agents_data['pageProps']['data']['agents']['results'], start=1):
        print(f""{idx}. {agent['displayName']} - Contact: {agent['contactNumber']} - Address: {agent['displayAddress']}"")

    # Step 3: Retrieve properties based on filters (example: buy properties in Oxford with price filter)
    filtered_properties_data = get_properties_list(
        search_location=""Oxford"",
        search_location_subtext=""Oxfordshire"",
        region_type=""region"",
        channel=""buy"",
        sort_type=""relevance"",
        page=1,
        page_size=5,
        minimum_price=250000,
        maximum_price=750000
    )

    # Display filtered properties
    print(""\nFiltered Property Listings in Oxford, Price Range Â£250,000 - Â£750,000:"")
    for idx, property_item in enumerate(filtered_properties_data['tieredResults'][0]['results'], start=1):
        print(f""{idx}. {property_item['title']} - Price: {property_item['price']['display']}"")
",Fail,-1,-1
"I want to integrate real estate and school data for potential home buyers in Australia. Find nearby schools based on the specified location, retrieve a list of available properties based on filters such as city, price, and type, and provide detailed information about each selected property. School options can be explored by primary, secondary or all types within a given radius, while also having access to comprehensive details of specific property listings.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_schools_list(lat, lon):
    """"""
    Retrieve a list of schools categorized by type (all, primary, secondary) near a specified GEO location.
    """"""
    url = ""https://realty-in-au.p.rapidapi.com/schools/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""realty-in-au.p.rapidapi.com""
    }
    querystring = {
        ""lat"": str(lat),
        ""lon"": str(lon)
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_real_estate_listings(city=""Cape Coral"", price_max=250000, limit=50, sort=""RELEVANCE""):
    """"""
    Retrieve detailed information about real estate listings in a specified city, with options to filter by price and sorting order.
    """"""
    url = ""https://florida-realty-api1.p.rapidapi.com/realty/listings""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""florida-realty-api1.p.rapidapi.com""
    }
    querystring = {
        ""city"": city,
        ""price_max"": str(price_max),
        ""limit"": str(limit),
        ""sort"": sort
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_listing_details(identifier):
    """"""
    Retrieve comprehensive details about a specific real estate listing using its unique identifier.
    """"""
    url = f""https://florida-realty-api1.p.rapidapi.com/realty/listings/{identifier}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""florida-realty-api1.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers)
    return response.json()

if __name__ == '__main__':
    # Step 1: Retrieve a list of schools near a specified GEO location
    latitude = -37.7987
    longitude = 144.9532
    schools_data = get_schools_list(lat=latitude, lon=longitude)

    # Display details of nearby schools
    print(""Nearby Schools:"")
    for idx, school in enumerate(schools_data['all'], start=1):
        print(f""{idx}. {school['name']} - Type: {school['school_type']} - Address: {school['address']['street']}, {school['address']['suburb']}, {school['address']['state']} {school['address']['postcode']}"")

    # Step 2: Retrieve real estate listings for a specified city and price range
    city = ""Cape Coral""
    max_price = 300000
    real_estate_data = get_real_estate_listings(city=city, price_max=max_price, limit=10, sort=""PRICE_LOW"")

    # Display basic information of the real estate listings
    print(""\nReal Estate Listings in Cape Coral (under $300,000):"")
    for idx, listing in enumerate(real_estate_data['Result']['Offers'], start=1):
        print(f""{idx}. {listing['ItemOffered']['Address']['Name']} - Price: ${listing['Price']} - City: {city}"")

    # Step 3: Retrieve detailed information about a specific listing using its unique identifier
    if real_estate_data['Result']['Offers']:
        first_listing_id = real_estate_data['Result']['Offers'][0]['Identifier']
        listing_details = get_listing_details(identifier=first_listing_id)

        # Display detailed information of the selected listing
        print(""\nDetailed Information for the First Listing:"")
        item_offered = listing_details['Result']['Offers'][0]['ItemOffered']
        address = item_offered['Address']
        print(f""Property: {item_offered['Name']}"")
        print(f""Address: {address['StreetAddress']}, {address['AddressLocality']}, {address['AddressRegion']} {address['PostalCode']}"")
        print(f""Price: ${listing_details['Result']['Offers'][0]['Price']}"")
        print(f""Details: {item_offered['Description']}"")
","Nearby Schools:
1. North Melbourne Primary School - Type: Primary - Address: 210 ERROL STREET, NORTH MELBOURNE, VIC 3051
2. University High School - Type: Secondary - Address: 77 STORY STREET, PARKVILLE, VIC 3052
3. St Joseph's Flexible Learning Centre - Type: Secondary - Address: 385 QUEENSBERRY STREET, NORTH MELBOURNE, VIC 3051
4. Simonds Catholic College - Type: Secondary - Address: 273 VICTORIA STREET, WEST MELBOURNE, VIC 3003
5. St Michael's School - Type: Primary - Address: 8 BROUGHAM ST, NORTH MELBOURNE, VIC 3051
6. St Aloysius College - Type: Secondary - Address: 31 CURRAN ST, NORTH MELBOURNE, VIC 3051
7. Stott's College - Type: Secondary - Address: 43 ELIZABETH STREET, MELBOURNE, VIC 3000
8. Haileybury College - City Campus - Type: Combined - Address: 383 King St, West Melbourne, VIC 3003
9. Carlton Primary School - Type: Primary - Address: CNR PALMERSTON & DRUMMOND STS, CARLTON, VIC 3053
10. Carlton Gardens Primary School - Type: Primary - Address: 215 RATHDOWNE STREET, CARLTON, VIC 3053

Real Estate Listings in Cape Coral (under $300,000):
1. 4513 SW 8th Ct Apt 111, Cape Coral, FL 33914, USA - Price: $132000 - City: Cape Coral
2. 608 SE 6th St Apt F, Cape Coral, FL 33990, USA - Price: $140000 - City: Cape Coral
3. 1100 Pondella Rd Apt 416, Cape Coral, FL 33909, USA - Price: $148000 - City: Cape Coral
4. 1100 Pondella Rd Apt 1001, Cape Coral, FL 33909, USA - Price: $149000 - City: Cape Coral
5. 1113 Cape Coral Pkwy W Apt 103, Cape Coral, FL 33914, USA - Price: $149900 - City: Cape Coral
6. 4608 SE 4th Pl Apt 5, Cape Coral, FL 33904, USA - Price: $149900 - City: Cape Coral
7. 4611 SE 5th Pl Apt 8, Cape Coral, FL 33904, USA - Price: $157000 - City: Cape Coral
8. 804 Mohawk Pkwy Apt 103, Cape Coral, FL 33914, USA - Price: $158400 - City: Cape Coral
9. 1100 Pondella Rd Apt 616, Cape Coral, FL 33909, USA - Price: $159000 - City: Cape Coral
10. 4014 Coronado Pkwy Unit 106, Cape Coral, FL 33904, USA - Price: $159000 - City: Cape Coral
11. 3914 SW 8th Ct Apt 102, Cape Coral, FL 33914, USA - Price: $159500 - City: Cape Coral
12. 1100 Pondella Rd Apt 712, Cape Coral, FL 33909, USA - Price: $159999 - City: Cape Coral
13. 2504 SE 16th Pl Apt 103, Cape Coral, FL 33904, USA - Price: $160000 - City: Cape Coral
14. 2504 SE 16th Pl Apt 101, Cape Coral, FL 33904, USA - Price: $160000 - City: Cape Coral
15. 4608 SE 4th Pl Apt 3, Cape Coral, FL 33904, USA - Price: $160000 - City: Cape Coral
16. 503 Cape Coral Pkwy W Apt 104, Cape Coral, FL 33914, USA - Price: $165000 - City: Cape Coral
17. 607 SE 47th St Apt 6, Cape Coral, FL 33904, USA - Price: $165000 - City: Cape Coral
18. 4616 Skyline Blvd Apt 104, Cape Coral, FL 33914, USA - Price: $169000 - City: Cape Coral
19. 4906 Victoria Dr Apt 103, Cape Coral, FL 33904, USA - Price: $169000 - City: Cape Coral
20. 4912 Vincennes Ct Apt 201, Cape Coral, FL 33904, USA - Price: $169900 - City: Cape Coral
21. 4722 SE 1st Pl Apt 12, Cape Coral, FL 33904, USA - Price: $169900 - City: Cape Coral
22. 4613 SE 5th Ave Apt 104, Cape Coral, FL 33904, USA - Price: $170000 - City: Cape Coral
23. 4613 SE 5th Ave Apt 104, Cape Coral, FL 33904, USA - Price: $170000 - City: Cape Coral
24. 804 Mohawk Pkwy Apt 105, Cape Coral, FL 33914, USA - Price: $171000 - City: Cape Coral
25. 1407 SW 16th Ter Apt 201, Cape Coral, FL 33991, USA - Price: $174000 - City: Cape Coral
26. 4514 SW 8th Pl Unit 11, Cape Coral, FL 33914, USA - Price: $174500 - City: Cape Coral
27. 4616 Skyline Blvd Apt 207, Cape Coral, FL 33914, USA - Price: $174900 - City: Cape Coral
28. 4920 Vincennes Ct Unit 104, Cape Coral, FL 33904, USA - Price: $174900 - City: Cape Coral
29. 950 Hancock Creek South Blvd Apt 213, Cape Coral, FL 33909, USA - Price: $174999 - City: Cape Coral
30. 4516 Santa Barbara Blvd Apt 3, Cape Coral, FL 33914, USA - Price: $175000 - City: Cape Coral
31. 2526 SE 16th Pl Apt 210, Cape Coral, FL 33904, USA - Price: $175000 - City: Cape Coral
32. 2526 SE 16th Pl Apt 210, Cape Coral, FL 33904, USA - Price: $175000 - City: Cape Coral
33. 1040 Hancock Creek South Blvd Apt 101, Cape Coral, FL 33909, USA - Price: $175000 - City: Cape Coral
34. 1865 Concordia Lake Cir Unit 408, Cape Coral, FL 33909, USA - Price: $177900 - City: Cape Coral
35. 1865 Concordia Lake Cir Unit 408, Cape Coral, FL 33909, USA - Price: $177900 - City: Cape Coral
36. 1080 Hancock Creek South Blvd Apt 101, Cape Coral, FL 33909, USA - Price: $178000 - City: Cape Coral
37. 1771 Four Mile Cove Pkwy Apt 1027, Cape Coral, FL 33990, USA - Price: $179000 - City: Cape Coral
38. 3956 Pomodoro Cir Unit 103, Cape Coral, FL 33909, USA - Price: $179000 - City: Cape Coral
39. 4706 SE 4th Pl Apt 13, Cape Coral, FL 33904, USA - Price: $179000 - City: Cape Coral
40. 2813 Del Prado Blvd S Unit A19, Cape Coral, FL 33904, USA - Price: $179900 - City: Cape Coral
41. 423 Cape Coral Pkwy W Apt 107, Cape Coral, FL 33914, USA - Price: $179950 - City: Cape Coral
42. 4641 SW Santa Barbara Pl Apt 2, Cape Coral, FL 33914, USA - Price: $180000 - City: Cape Coral
43. 3907 Del Prado Blvd S Unit 104C, Cape Coral, FL 33904, USA - Price: $180000 - City: Cape Coral
44. 4407 SW 7th Pl Apt 7, Cape Coral, FL 33914, USA - Price: $180000 - City: Cape Coral
45. 1050 Hancock Creek South Blvd Apt 203, Cape Coral, FL 33909, USA - Price: $180000 - City: Cape Coral
46. 3907 Del Prado Blvd S # 104, Cape Coral, FL 33904, USA - Price: $180000 - City: Cape Coral
47. 1141 Gleason Pkwy Unit 206, Cape Coral, FL 33914, USA - Price: $182000 - City: Cape Coral
48. 950 Hancock Creek South Blvd Apt 112, Cape Coral, FL 33909, USA - Price: $183000 - City: Cape Coral
49. 1512 Cape Coral Pkwy W Apt 102, Cape Coral, FL 33914, USA - Price: $183000 - City: Cape Coral
50. 950 Hancock Creek South Blvd Apt 112, Cape Coral, FL 33909, USA - Price: $183000 - City: Cape Coral
51. 1518 SW Courtyards Ln Apt 199, Cape Coral, FL 33914, USA - Price: $184900 - City: Cape Coral
52. 4608 SE 6th Ave Apt 1D, Cape Coral, FL 33904, USA - Price: $184900 - City: Cape Coral
53. 1769 Four Mile Cove Pkwy Apt 927, Cape Coral, FL 33990, USA - Price: $184900 - City: Cape Coral
54. 1769 Four Mile Cove Pkwy Apt 911, Cape Coral, FL 33990, USA - Price: $185000 - City: Cape Coral
55. 4420 Santa Barbara Blvd Apt 105, Cape Coral, FL 33914, USA - Price: $185000 - City: Cape Coral
56. 4109 SE 19th Ave Apt 207, Cape Coral, FL 33904, USA - Price: $185000 - City: Cape Coral
57. 1771 Four Mile Cove Pkwy Apt 1015, Cape Coral, FL 33990, USA - Price: $185000 - City: Cape Coral
58. 3968 Pomodoro Cir, Cape Coral, FL 33909, USA - Price: $185999 - City: Cape Coral
59. 1417 SE 24th Ave Apt D, Cape Coral, FL 33990, USA - Price: $187000 - City: Cape Coral
60. 2510 SE 16th Pl Apt 102, Cape Coral, FL 33904, USA - Price: $187000 - City: Cape Coral
61. 4805 Sorrento Ct Unit 106, Cape Coral, FL 33904, USA - Price: $189000 - City: Cape Coral
62. 1121 Van Loon Commons Cir Apt 203, Cape Coral, FL 33909, USA - Price: $189900 - City: Cape Coral
63. 1507 Cape Coral Pkwy W Apt 5, Cape Coral, FL 33914, USA - Price: $190000 - City: Cape Coral
64. 3969 Pomodoro Cir, Cape Coral, FL 33909, USA - Price: $190000 - City: Cape Coral
65. 3947 Del Sol Ln, Cape Coral, FL 33909, USA - Price: $190000 - City: Cape Coral
66. 1101 Van Loon Commons Cir Apt 302, Cape Coral, FL 33909, USA - Price: $192500 - City: Cape Coral
67. 1100 Pondella Rd, Cape Coral, FL 33909, USA - Price: $193732 - City: Cape Coral
68. Cape Coral 321 Pkwy, Cape Coral, FL 33914, USA - Price: $193900 - City: Cape Coral
69. 804 Mohawk Pkwy Apt 106, Cape Coral, FL 33914, USA - Price: $195000 - City: Cape Coral
70. 3739 SW 9th Ave Apt 109, Cape Coral, FL 33914, USA - Price: $195000 - City: Cape Coral
71. 950 Hancock Creek South Blvd Apt 222, Cape Coral, FL 33909, USA - Price: $196900 - City: Cape Coral
72. 1518 SW Courtyards Ln Apt 200, Cape Coral, FL 33914, USA - Price: $198000 - City: Cape Coral
73. 1503 SW 51st Ln Apt 141, Cape Coral, FL 33914, USA - Price: $198000 - City: Cape Coral
74. 3940 Pomodoro Cir, Cape Coral, FL 33909, USA - Price: $198999 - City: Cape Coral
75. 1524 SW 50th St Apt 203, Cape Coral, FL 33914, USA - Price: $199000 - City: Cape Coral
76. 4213 SE 19th Ave Apt 205, Cape Coral, FL 33904, USA - Price: $199900 - City: Cape Coral
77. 3953 Pomodoro Cir Unit 204, Cape Coral, FL 33909, USA - Price: $199900 - City: Cape Coral
78. 1304 SE 7th St Apt 110, Cape Coral, FL 33990, USA - Price: $199900 - City: Cape Coral
79. 4213 SE 19th Ave Apt 205, Cape Coral, FL 33904, USA - Price: $199900 - City: Cape Coral
80. 4213 SE 19th Ave Apt 106, Cape Coral, FL 33904, USA - Price: $199900 - City: Cape Coral
81. 950 Hancock Creek South Blvd Apt 511, Cape Coral, FL 33909, USA - Price: $199900 - City: Cape Coral
82. 4202 SE 4th Pl Unit 10, Cape Coral, FL 33904, USA - Price: $199900 - City: Cape Coral
83. 950 Hancock Creek South Blvd Apt 411, Cape Coral, FL 33909, USA - Price: $199999 - City: Cape Coral
84. 1519 Cape Coral Pkwy W Apt 6, Cape Coral, FL 33914, USA - Price: $199999 - City: Cape Coral
85. 1141 Van Loon Commons Cir Apt 103, Cape Coral, FL 33909, USA - Price: $200000 - City: Cape Coral
86. 3903 Del Prado Blvd S Unit 101, Cape Coral, FL 33904, USA - Price: $200000 - City: Cape Coral
87. 3911 Country Club Blvd Apt 104, Cape Coral, FL 33904, USA - Price: $200000 - City: Cape Coral
88. 4906 Victoria Dr Apt 413, Cape Coral, FL 33904, USA - Price: $200000 - City: Cape Coral
89. 1005 SE 40th St Apt 4, Cape Coral, FL 33904, USA - Price: $204999 - City: Cape Coral
90. 716 SE 12th Ct Apt 29, Cape Coral, FL 33990, USA - Price: $209000 - City: Cape Coral
91. 5116 Coronado Pkwy Apt 16, Cape Coral, FL 33904, USA - Price: $209000 - City: Cape Coral
92. 3913 SE 11th Ave Apt 206, Cape Coral, FL 33904, USA - Price: $209000 - City: Cape Coral
93. 3944 Pomodoro Cir Unit 203, Cape Coral, FL 33909, USA - Price: $209900 - City: Cape Coral
94. 2904 Tuscan Dr Unit 302, Cape Coral, FL 33909, USA - Price: $209900 - City: Cape Coral
95. 1313 SW 16th Ter Apt 101, Cape Coral, FL 33991, USA - Price: $210000 - City: Cape Coral
96. 1005 SE 40th St Apt 2, Cape Coral, FL 33904, USA - Price: $214900 - City: Cape Coral
97. 1100 Pondella Rd Apt 504, Cape Coral, FL 33909, USA - Price: $214900 - City: Cape Coral
98. 4205 SE 19th Ave Apt 203, Cape Coral, FL 33904, USA - Price: $214900 - City: Cape Coral
99. 621 SE 15th St Apt 116, Cape Coral, FL 33990, USA - Price: $214999 - City: Cape Coral
100. 830 SW 48th Ter Apt 10, Cape Coral, FL 33914, USA - Price: $215000 - City: Cape Coral
101. 3916 SE 11th Pl, Cape Coral, FL 33904, USA - Price: $215000 - City: Cape Coral
102. 3916 SE 11th Pl Apt 503, Cape Coral, FL 33904, USA - Price: $215000 - City: Cape Coral
103. 4508 Santa Barbara Blvd Apt 102, Cape Coral, FL 33914, USA - Price: $215999 - City: Cape Coral
104. 4021 SE 19th Ave Apt 103, Cape Coral, FL 33904, USA - Price: $218000 - City: Cape Coral
105. 4021 SE 19th Ave Apt 201, Cape Coral, FL 33904, USA - Price: $219000 - City: Cape Coral
106. 321 Cape Coral Pkwy W Apt 103, Cape Coral, FL 33914, USA - Price: $219000 - City: Cape Coral
107. 5110 SW Courtyards Way Apt 22, Cape Coral, FL 33914, USA - Price: $219000 - City: Cape Coral
108. 1110 SW 48th Ter Unit 101, Cape Coral, FL 33914, USA - Price: $219000 - City: Cape Coral
109. 5023 SW 16th Pl Unit 202, Cape Coral, FL 33914, USA - Price: $219700 - City: Cape Coral
110. 4007 Palm Tree Blvd Apt 105, Cape Coral, FL 33904, USA - Price: $219900 - City: Cape Coral
111. 708 Victoria Dr Apt 213, Cape Coral, FL 33904, USA - Price: $219900 - City: Cape Coral
112. 4007 Palm Tree Blvd Apt 105, Cape Coral, FL 33904, USA - Price: $219900 - City: Cape Coral
113. 3947 Del Sol Ln Unit 203, Cape Coral, FL 33909, USA - Price: $219900 - City: Cape Coral
114. 4013 Palm Tree Blvd Apt 306, Cape Coral, FL 33904, USA - Price: $220000 - City: Cape Coral
115. 909 SE 46th Ln Apt 210, Cape Coral, FL 33904, USA - Price: $220000 - City: Cape Coral
116. 5110 SW Courtyards Way Apt 22, Cape Coral, FL 33914, USA - Price: $220000 - City: Cape Coral
117. 1514 SW 50th St Apt 204, Cape Coral, FL 33914, USA - Price: $224900 - City: Cape Coral
118. 4205 SE 19th Ave Apt 202, Cape Coral, FL 33904, USA - Price: $225000 - City: Cape Coral
119. 4904 Viceroy St Apt 5, Cape Coral, FL 33904, USA - Price: $225000 - City: Cape Coral
120. 1320 Weeping Willow Ct, Cape Coral, FL 33909, USA - Price: $225000 - City: Cape Coral
121. 1722 Beach Pkwy Apt B3, Cape Coral, FL 33904, USA - Price: $225000 - City: Cape Coral
122. 3952 Pomodoro Cir Unit 103, Cape Coral, FL 33909, USA - Price: $225000 - City: Cape Coral
123. 2904 Tuscan Dr Unit 303, Cape Coral, FL 33909, USA - Price: $225000 - City: Cape Coral
124. 2904 Tuscan Dr Unit 303, Cape Coral, FL 33909, USA - Price: $225000 - City: Cape Coral
125. 4015 Palm Tree Blvd Apt 302, Cape Coral, FL 33904, USA - Price: $225000 - City: Cape Coral
126. 3711 Del Prado Blvd S Apt 2, Cape Coral, FL 33904, USA - Price: $225000 - City: Cape Coral
127. 4018 SE 12th Ave Apt 105, Cape Coral, FL 33904, USA - Price: $225000 - City: Cape Coral
128. 3901 SE 11th Pl Apt 103, Cape Coral, FL 33904, USA - Price: $225000 - City: Cape Coral
129. 4009 Palm Tree Blvd Apt 402, Cape Coral, FL 33904, USA - Price: $227000 - City: Cape Coral
130. 505 SW 47th Ter Apt 202, Cape Coral, FL 33914, USA - Price: $227000 - City: Cape Coral
131. 4013 Palm Tree Blvd Apt 301, Cape Coral, FL 33904, USA - Price: $228500 - City: Cape Coral
132. 1129 Van Loon Commons Cir Apt 303, Cape Coral, FL 33909, USA - Price: $228777 - City: Cape Coral
133. 3964 Pomodoro Cir Unit 104, Cape Coral, FL 33909, USA - Price: $229000 - City: Cape Coral
134. 1770 Concordia Lake Cir Unit 3105, Cape Coral, FL 33909, USA - Price: $229000 - City: Cape Coral
135. 5020 SW Courtyards Way Apt 10, Cape Coral, FL 33914, USA - Price: $229900 - City: Cape Coral
136. 5510 SW 4th Pl Apt 607, Cape Coral, FL 33914, USA - Price: $229900 - City: Cape Coral
137. 909 SE 46th Ln Apt 101, Cape Coral, FL 33904, USA - Price: $229900 - City: Cape Coral
138. 1104 SE 8th St Apt 7, Cape Coral, FL 33990, USA - Price: $230000 - City: Cape Coral
139. 1210 SE 6th Ter Apt 87, Cape Coral, FL 33990, USA - Price: $230000 - City: Cape Coral
140. 3944 Pomodoro Cir Unit 204, Cape Coral, FL 33909, USA - Price: $234900 - City: Cape Coral
141. 3807 SE 11th Pl Unit 1203, Cape Coral, FL 33904, USA - Price: $234900 - City: Cape Coral
142. 3952 Pomodoro Cir Unit 102, Cape Coral, FL 33909, USA - Price: $234975 - City: Cape Coral
143. 521 SW 47th Ter Apt 202, Cape Coral, FL 33914, USA - Price: $234999 - City: Cape Coral
144. 4613 SE 5th Ave Apt 210, Cape Coral, FL 33904, USA - Price: $235000 - City: Cape Coral
145. 1111 SE 8th Ter Apt 3F, Cape Coral, FL 33990, USA - Price: $235000 - City: Cape Coral
146. 5510 SW 4th Pl Apt 501, Cape Coral, FL 33914, USA - Price: $235000 - City: Cape Coral
147. 829 SW 47th Ter Apt 204, Cape Coral, FL 33914, USA - Price: $235000 - City: Cape Coral
148. 1102 SE 39th Ter Apt 105, Cape Coral, FL 33904, USA - Price: $235000 - City: Cape Coral
149. 4613 SE 5th Ave Apt 210, Cape Coral, FL 33904, USA - Price: $235000 - City: Cape Coral
150. 1133 Van Loon Commons Cir Apt 202, Cape Coral, FL 33909, USA - Price: $236000 - City: Cape Coral
151. 505 SW 47th Ter Apt 104, Cape Coral, FL 33914, USA - Price: $237500 - City: Cape Coral
152. 4708 SW 8th Pl Apt 102, Cape Coral, FL 33914, USA - Price: $238999 - City: Cape Coral
153. 4807 Sunset Ct Apt 607, Cape Coral, FL 33904, USA - Price: $239000 - City: Cape Coral
154. 4613 SE 5th Ave Apt 203, Cape Coral, FL 33904, USA - Price: $239000 - City: Cape Coral
155. 1518 SW 50th St Apt 103, Cape Coral, FL 33914, USA - Price: $239000 - City: Cape Coral
156. 4912 Vincennes St Apt 101, Cape Coral, FL 33904, USA - Price: $239000 - City: Cape Coral
157. 1212 SE 6th Ter Apt 82, Cape Coral, FL 33990, USA - Price: $239000 - City: Cape Coral
158. 3939 Pomodoro Cir Unit 204, Cape Coral, FL 33909, USA - Price: $239900 - City: Cape Coral
159. 1512 SW 50th St Apt 303, Cape Coral, FL 33914, USA - Price: $239900 - City: Cape Coral
160. 4802 Tudor Dr Apt 104, Cape Coral, FL 33904, USA - Price: $239900 - City: Cape Coral
161. 4613 SE 5th Ave Apt 101, Cape Coral, FL 33904, USA - Price: $239900 - City: Cape Coral
162. 834 SE 46th St Apt 1A, Cape Coral, FL 33904, USA - Price: $239900 - City: Cape Coral
163. 1220 SE 6th Ter Apt 60, Cape Coral, FL 33990, USA - Price: $240500 - City: Cape Coral
164. 4002 Coronado Pkwy Apt 101, Cape Coral, FL 33904, USA - Price: $244900 - City: Cape Coral
165. 1766 Cape Coral Pkwy E Apt 608, Cape Coral, FL 33904, USA - Price: $244999 - City: Cape Coral
166. 1027 SE 46th Ln Apt 1A, Cape Coral, FL 33904, USA - Price: $245000 - City: Cape Coral
167. 4924 Tudor Dr Apt 202, Cape Coral, FL 33904, USA - Price: $245000 - City: Cape Coral
168. 1109 SE 40th St Apt 102, Cape Coral, FL 33904, USA - Price: $245000 - City: Cape Coral
169. 47th 1521 Ter, Cape Coral, FL 33914, USA - Price: $245000 - City: Cape Coral
170. 4017 Palm Tree Blvd Apt 204, Cape Coral, FL 33904, USA - Price: $245500 - City: Cape Coral
171. 1120 SE 46th St Apt 1C, Cape Coral, FL 33904, USA - Price: $248000 - City: Cape Coral
172. 5023 SW 16th Pl Unit 101, Cape Coral, FL 33914, USA - Price: $248350 - City: Cape Coral
173. 5023 SW 16th Pl Unit 101, Cape Coral, FL 33914, USA - Price: $248350 - City: Cape Coral
174. 5308 Chiquita Blvd S Unit 102A, Cape Coral, FL 33914, USA - Price: $248800 - City: Cape Coral
175. 4629 SE 5th Ave Apt 203, Cape Coral, FL 33904, USA - Price: $249000 - City: Cape Coral
176. 1220 SE 40th St Apt 8, Cape Coral, FL 33904, USA - Price: $249000 - City: Cape Coral
177. 4715 Santa Barbara Blvd Apt F1, Cape Coral, FL 33914, USA - Price: $249000 - City: Cape Coral
178. 853 SE 46th Ln Apt 202, Cape Coral, FL 33904, USA - Price: $249900 - City: Cape Coral
179. 1781 Four Mile Cove Pkwy Unit 134, Cape Coral, FL 33990, USA - Price: $249900 - City: Cape Coral
180. 632 Victoria Dr Apt 104, Cape Coral, FL 33904, USA - Price: $249900 - City: Cape Coral
181. 4563 SE 5th Pl Apt 205, Cape Coral, FL 33904, USA - Price: $249900 - City: Cape Coral
182. 4116 SE 19th Ave Unit 101, Cape Coral, FL 33904, USA - Price: $249900 - City: Cape Coral
183. 1766 Cape Coral Pkwy E Apt 110, Cape Coral, FL 33904, USA - Price: $249900 - City: Cape Coral
184. 1800 Concordia Lake Cir Unit 2302, Cape Coral, FL 33909, USA - Price: $249900 - City: Cape Coral
185. 4921 SW 16th Pl, Cape Coral, FL 33914, USA - Price: $249900 - City: Cape Coral
186. 1518 SW 50th St Apt 204, Cape Coral, FL 33914, USA - Price: $249900 - City: Cape Coral
187. 904 SE 13th St Apt 201, Cape Coral, FL 33990, USA - Price: $249900 - City: Cape Coral
188. 5312 Chiquita Blvd S Unit 101B, Cape Coral, FL 33914, USA - Price: $249900 - City: Cape Coral
189. 1333 SE 40th Ter Apt 2C, Cape Coral, FL 33904, USA - Price: $250000 - City: Cape Coral
190. 1129 Van Loon Commons Cir Apt 301, Cape Coral, FL 33909, USA - Price: $250000 - City: Cape Coral
191. 4924 Tudor Dr Apt 103, Cape Coral, FL 33904, USA - Price: $250000 - City: Cape Coral
192. 5510 SW 4th Pl Apt 601, Cape Coral, FL 33914, USA - Price: $254900 - City: Cape Coral
193. 1514 SW 50th St Apt 301, Cape Coral, FL 33914, USA - Price: $254900 - City: Cape Coral
194. 4924 Tudor Dr Apt 203, Cape Coral, FL 33904, USA - Price: $255000 - City: Cape Coral
195. 1149 Winding Pines Cir Unit 101, Cape Coral, FL 33909, USA - Price: $255900 - City: Cape Coral
196. 1149 Winding Pines County Rd Unit 101, Cape Coral, FL 33909, USA - Price: $255900 - City: Cape Coral
197. 1788 Concordia Lake Cir Unit 2707, Cape Coral, FL 33909, USA - Price: $257500 - City: Cape Coral
198. 1137 Van Loon Commons Cir Apt 204, Cape Coral, FL 33909, USA - Price: $258700 - City: Cape Coral
199. 1137 Van Loon Commons Cir Apt 204, Cape Coral, FL 33909, USA - Price: $258700 - City: Cape Coral
200. 4924 Viceroy St Apt B1, Cape Coral, FL 33904, USA - Price: $258900 - City: Cape Coral

Detailed Information for the First Listing:
Property: 1 Bed, 1 Bath, 753 null
Address: 4513 SW 8th Ct Apt 111, Cape Coral, Lee 33914
Price: $132000
Details: This 1 bedroom 1 bathroom is located in SW Cape Coral, close to shopping, the Westin Hotel, Downtown Cape Coral, Approximately 16 miles to Fort Myers Beach, and 24 miles to Sanibel Island. This condo features an Open Floor plan, and a nice sized lanai. The building was recently painted and has a new roof.
",1,1
"Users want to get weekly economic indicators and display listings of rental properties in specific cities and states in the United States. First collect weekly data, analyze recent trends, and filter attributes based on the results. For example, if weekly data shows a decline in economic indicators, focus on properties below a certain price threshold; If the economic indicators are stable, we can consider widening the price range. Users can specify locations by city and state to screen and recommend relevant rental properties based on the latest economic trends.","import requests

# Define the API Key
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


# Function to retrieve weekly economic indicators
def get_weekly_data():
    """"""
    Retrieve weekly economic indicators using the Semanal API.
    """"""
    url = ""https://riesgo-pais.p.rapidapi.com/api/riesgopais/semanal""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""riesgo-pais.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching weekly economic data: {response.status_code}"")
        return None


# Function to retrieve rental property listings
def get_rental_properties(state, city, page, price_max=None):
    """"""
    Retrieve rental property listings using the v1_properties_rent API.
    Optionally filter properties by a maximum price based on economic indicators.
    """"""
    url = ""https://real-estate12.p.rapidapi.com/v1/real-estate/rent""
    querystring = {
        ""state"": state,
        ""city"": city,
        ""page"": page,
        ""sort"": ""newest""
    }

    if price_max:
        querystring[""price_h""] = str(price_max)

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""real-estate12.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers, params=querystring)

    # Debugging output for API response
    if response.status_code == 200:
        # print(""\nAPI Response Data:"")
        # print(response.json())
        return response.json()
    else:
        print(f""Error fetching rental properties: {response.status_code}"")
        return None


# Main code to demonstrate the functionality
if __name__ == '__main__':
    # Step 1: Retrieve weekly economic data
    weekly_data = get_weekly_data()
    if weekly_data:
        print(""\nWeekly Economic Data:"")
        print(weekly_data)

        # Analyze the weekly data to determine the price threshold
        recent_week_values = [entry['value'] for entry in weekly_data[-4:]]  # Consider the last 4 weeks
        average_value = sum(recent_week_values) / len(recent_week_values)
        print(f""\nAverage economic indicator for the last 4 weeks: {average_value}"")

        # Set a price threshold based on economic trend
        if average_value < 100:  # Example threshold: below 100, economic indicators are declining
            max_price = 1500  # Focus on properties below $1500 if economic indicators are declining
            print(""Economic indicators are declining. Focusing on properties below $1500."")
        else:
            max_price = 2000  # Consider broader price range otherwise
            print(""Economic indicators are stable. Considering properties up to $2000."")

        # Step 2: Retrieve rental properties based on economic analysis
        city = ""Los Angeles""
        state = ""CA""
        rental_properties = get_rental_properties(state=state, city=city, page=""1"", price_max=max_price)

        if rental_properties:
            # Correctly extract properties from the 'body' key
            property_list = rental_properties.get('body', [])
            if not property_list:
                print(""No rental properties found in the specified criteria."")
            else:
                print(""\nFiltered Rental Properties:"")
                for idx, property_item in enumerate(property_list, start=1):
                    title = property_item.get('description', {}).get('type', 'No title')
                    price = property_item.get('list_price', 'N/A')
                    address = property_item.get('location', {}).get('address', {}).get('line', 'N/A')
                    print(f""{idx}. {title} - Price: ${price} - Address: {address}"")
        else:
            print(""Failed to retrieve rental properties."")
",Fail,-1,-1
"Users want to get popular videos based on the specified region and language, allowing users to search for movies or TV shows related to those video titles and then retrieve popular reviews of popular videos of interest. The service should first get popular videos in a particular region. It should then allow users to select a title from a popular video to search for relevant movies or shows and provide detailed metadata about them. Finally, if the user wants to browse the comments of any selected popular video, the service should be able to fetch and display the popular comments related to that video.","import requests

# Define API Key
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

# Function to retrieve trending videos for a specific region and language
def get_trending_videos(geo='US', lang='en'):
    """"""
    Retrieve trending videos for a specified geo location and language.
    """"""
    url = ""https://yt-api.p.rapidapi.com/trending""
    querystring = {""geo"": geo, ""lang"": lang}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""yt-api.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching trending videos: {response.status_code}"")
        return None

# Function to search related movies or TV shows based on a query
def search_related_content(query):
    """"""
    Search for related movies or TV shows based on a given query.
    """"""
    url = ""https://movies-api14.p.rapidapi.com/search""
    querystring = {""query"": query}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""movies-api14.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error searching for related content: {response.status_code}"")
        return None

# Function to retrieve video comments for a specified video ID
def get_video_comments(video_id, sort_by='newesttop'):
    """"""
    Retrieve comments for a specified video ID, sorted by criteria (e.g., 'newesttop').
    """"""
    url = ""https://yt-api.p.rapidapi.com/comments""
    querystring = {""id"": video_id, ""sort_by"": sort_by}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""yt-api.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching video comments: {response.status_code}"")
        return None

# Main code to demonstrate the functionality
if __name__ == '__main__':
    # Step 1: Get trending videos
    trending_data = get_trending_videos(geo='US', lang='en')
    if trending_data and 'data' in trending_data:
        print(""Trending Videos Data:"")
        for idx, video in enumerate(trending_data['data'][:5], start=1):  # Limit output to top 5 for clarity
            title = video.get('title', 'No title')
            author = video.get('author', 'Unknown channel')
            print(f""{idx}. Title: {title} - Channel: {author}"")

        # Choose a video title from trending data to search for related movies/shows
        first_video_title = trending_data['data'][0].get('title', 'No title')
        print(f""\nSearching for movies or TV shows related to: '{first_video_title}'"")

        # Step 2: Search for related content
        search_results = search_related_content(first_video_title)
        if search_results and 'results' in search_results:
            print(""Search Results:"")
            for idx, result in enumerate(search_results['results'][:3], start=1):  # Limit to top 3 for clarity
                result_title = result.get('title', 'No title')
                result_type = result.get('type', 'Unknown type')
                release_year = result.get('year', 'N/A')
                print(f""{idx}. Title: {result_title} - Type: {result_type} - Release Year: {release_year}"")

        # Choose a video ID to retrieve comments (taking the first video ID from trending)
        first_video_id = trending_data['data'][0].get('videoId', None)
        if first_video_id:
            print(f""\nRetrieving comments for video ID: {first_video_id}"")

            # Step 3: Get comments for the chosen video
            comments_data = get_video_comments(first_video_id)
            if comments_data and 'data' in comments_data:
                print(""Top Video Comments:"")
                for idx, comment in enumerate(comments_data['data'][:3], start=1):  # Limit to top 3 comments for clarity
                    comment_author = comment.get('author', 'Unknown author')
                    comment_text = comment.get('text', 'No comment text')
                    print(f""{idx}. {comment_author} says: {comment_text}"")
            else:
                print(""No comments found for the selected video."")
        else:
            print(""Failed to retrieve a valid video ID from trending data."")
    else:
        print(""Failed to retrieve trending videos."")
","Trending Videos Data:
1. Title: ThatPinkScienceTeacher - Channel: Unknown channel
2. Title: Megan Thee Stallion - Bigger In Texas [Official Video] - Channel: Unknown channel
3. Title: F I N A L ï½œ Alien Stage - Channel: Unknown channel
4. Title: Trending Shorts - Channel: Unknown channel
5. Title: Summer Walker - Heart Of A Woman [Official Lyric Video] - Channel: Unknown channel

Searching for movies or TV shows related to: 'ThatPinkScienceTeacher'
Failed to retrieve a valid video ID from trending data.",1,1
"A user who wants to find a nearby cafe or restaurant in Paris uses the location ID in a nearby search to retrieve details about a particular business (such as name, contact details, ratings, and hours of operation) and displays user reviews of the selected business. You should first get a list of cafes or restaurants based on the coordinates specified by the user. After you select a particular business from the list, the service retrieves and displays details about that business and uses its location ID. Finally, users should be able to view reviews and feedback on the selected business","import requests

# Define API Key
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


# Function to search nearby cafes or restaurants
def search_nearby_cafes(lat, lng, query=""cafe""):
    url = ""https://maps-data.p.rapidapi.com/nearby.php""
    querystring = {
        ""query"": query,
        ""lat"": str(lat),
        ""lng"": str(lng),
        ""limit"": ""20"",
        ""country"": ""fr"",
        ""lang"": ""en""
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""maps-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching nearby places: {response.status_code}"")
        return None


# Function to retrieve place information based on place ID
def get_place_info(place_id):
    url = ""https://maps-data.p.rapidapi.com/place.php""
    querystring = {""place_id"": place_id, ""country"": ""fr"", ""lang"": ""en""}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""maps-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching place information: {response.status_code}"")
        return None


# Function to retrieve reviews for a specific business
def get_place_reviews(business_id):
    url = ""https://maps-data.p.rapidapi.com/reviews.php""
    querystring = {
        ""business_id"": business_id,
        ""country"": ""fr"",
        ""lang"": ""en"",
        ""limit"": ""20"",
        ""sort"": ""Relevant""
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""maps-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching reviews: {response.status_code}"")
        return None


# Function to print formatted place details
def print_place_details(place_info):
    if not place_info or 'data' not in place_info:
        print(""No detailed information available."")
        return

    data = place_info['data'][0]
    name = data.get('name', 'Unknown')
    address = data.get('full_address', 'Unknown address')
    rating = data.get('rating', 'N/A')
    review_count = data.get('review_count', 0)
    phone = data.get('phone_number', 'N/A')
    price_level = data.get('price_level', 'N/A')
    website = data.get('website_full', 'N/A')
    working_hours = data.get('working_hours', {})

    print(""\n=== Place Details ==="")
    print(f""Name: {name}"")
    print(f""Address: {address}"")
    print(f""Rating: {rating} / 5 ({review_count} reviews)"")
    print(f""Phone: {phone}"")
    print(f""Price Level: {price_level}"")
    print(f""Website: {website}"")

    print(""\nWorking Hours:"")
    for day, hours in working_hours.items():
        print(f""  {day}: {', '.join(hours)}"")


# Function to print formatted reviews
def print_reviews(reviews_list):
    if not reviews_list or not isinstance(reviews_list, list):
        print(""No reviews available."")
        return

    print(""\n=== Reviews ==="")
    for idx, review in enumerate(reviews_list[:3], start=1):  # Limit to top 3 reviews
        reviewer = review.get('author', 'Anonymous')
        rating = review.get('rating', 'N/A')
        comment = review.get('text', 'No comment available')
        print(f""{idx}. Reviewer: {reviewer}"")
        print(f""   Rating: {rating}/5"")
        print(f""   Comment: {comment}\n"")


# Main code to demonstrate the functionality
if __name__ == '__main__':
    # Step 1: Search for nearby cafes/restaurants
    lat = 48.8577848
    lng = 2.2888379
    nearby_places = search_nearby_cafes(lat, lng)

    if nearby_places and 'data' in nearby_places:
        print(""=== Nearby Cafes/Restaurants ==="")
        for idx, place in enumerate(nearby_places['data'][:5], start=1):  # Limit to top 5 results
            name = place.get('name', 'Unknown')
            address = place.get('address', 'Unknown address')
            place_id = place.get('place_id', 'N/A')
            print(f""{idx}. Name: {name}"")
            print(f""   Address: {address}"")
            print(f""   Place ID: {place_id}\n"")

        # Step 2: Get detailed information about a specific place (using first place ID)
        first_place_id = nearby_places['data'][0].get('place_id', None)
        if first_place_id:
            place_info = get_place_info(first_place_id)
            print_place_details(place_info)

            # Step 3: Get reviews for the specific place
            business_id = nearby_places['data'][0].get('business_id', None)
            if business_id:
                place_reviews = get_place_reviews(business_id)

                # Check if 'data' is a list and iterate safely
                reviews_list = place_reviews.get('data', [])
                print_reviews(reviews_list)
            else:
                print(""No business ID found for the selected place."")
        else:
            print(""No valid place ID found in the response."")
    else:
        print(""No nearby cafes or restaurants found."")
","=== Nearby Cafes/Restaurants ===
1. Name: CafÃ© Ella
   Address: Unknown address
   Place ID: ChIJFZfb3w1w5kcRZZzthobp5vE

2. Name: CafÃ© du TrocadÃ©ro
   Address: Unknown address
   Place ID: ChIJm2s0bPtv5kcR-p42XuFt7rw

3. Name: Coffee Parisien
   Address: Unknown address
   Place ID: ChIJ3aO5yPlv5kcRiISnak_U5TE

4. Name: CafÃ© Victoria
   Address: Unknown address
   Place ID: ChIJ10Rni21w5kcRjM5mBAp0Lo4

5. Name: CafÃ© KlÃ©ber
   Address: Unknown address
   Place ID: ChIJXQvlQftv5kcR0ssQdKPzrIY


=== Place Details ===
Name: CafÃ© Ella
Address: CafÃ© Ella, 85 Rue de la Convention, 75015 Paris
Rating: 4.2 / 5 (506 reviews)
Phone: 0980624950
Price Level: â‚¬â‚¬
Website:

Working Hours:
  Saturday: 7â€¯AMâ€“1â€¯AM
  Sunday: 7â€¯AMâ€“1â€¯AM
  Monday: 7â€¯AMâ€“1â€¯AM
  Tuesday: 7â€¯AMâ€“1â€¯AM
  Wednesday: 7â€¯AMâ€“1â€¯AM
  Thursday: 7â€¯AMâ€“1â€¯AM
  Friday: 7â€¯AMâ€“1â€¯AM
No reviews available.",1,1
"Users want to search for job opportunities based on specific job titles and locations, retrieve detailed information about the selected position, and estimate the expected salary for that position in a given region. The service should first obtain a list of job posts based on user-defined criteria such as job title, location, and Posting date. Users can then select a job from the search results to get other details, such as app links and employer information. Finally, the service should provide an estimated salary range for selected positions in the same region to help users assess potential earnings.","import requests

# Define API Key
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

# Function to search jobs based on query and location
def search_jobs(query, location, date_posted=""all"", page=1, num_pages=1):
    url = ""https://jsearch.p.rapidapi.com/search""
    querystring = {
        ""query"": f""{query} in {location}"",
        ""page"": str(page),
        ""num_pages"": str(num_pages),
        ""date_posted"": date_posted
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""jsearch.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching job search results: {response.status_code}"")
        return None

# Function to retrieve detailed job information based on job ID
def get_job_details(job_id):
    url = ""https://jsearch.p.rapidapi.com/job-details""
    querystring = {
        ""job_id"": job_id,
        ""extended_publisher_details"": ""false""
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""jsearch.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching job details: {response.status_code}"")
        return None

# Function to estimate job salary based on job title and location
def estimate_job_salary(job_title, location, radius=50):
    url = ""https://jsearch.p.rapidapi.com/estimated-salary""
    querystring = {
        ""job_title"": job_title,
        ""location"": location,
        ""radius"": str(radius)
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""jsearch.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error estimating job salary: {response.status_code}"")
        return None

# Main code to demonstrate the functionality
if __name__ == '__main__':
    # Step 1: Search for jobs based on job title and location
    job_title = ""Software Engineer""
    location = ""San Francisco, CA, USA""
    jobs_data = search_jobs(job_title, location)
    if jobs_data and 'data' in jobs_data:
        print(""Job Search Results:"")
        for idx, job in enumerate(jobs_data['data'][:5], start=1):  # Limit to top 5 results
            # Improved access with get method and checking for potential None values
            title = job.get('title', 'No title available')
            company = job.get('company_name', 'No company information available')
            job_location = job.get('location', 'No location available')
            job_id = job.get('job_id', 'N/A')
            print(f""{idx}. Job Title: {title} - Company: {company} - Location: {job_location} - Job ID: {job_id}"")

        # Step 2: Retrieve detailed information about a specific job (using first job ID)
        first_job_id = jobs_data['data'][0].get('job_id', None)
        if first_job_id:
            job_details = get_job_details(first_job_id)
            if job_details and 'data' in job_details and len(job_details['data']) > 0:
                detailed_job = job_details['data'][0]
                print(f""\nJob Details for Job ID {first_job_id}:"")
                print(f""Job Title: {detailed_job.get('job_title', 'No job title available')}"")
                print(f""Company: {detailed_job.get('employer_name', 'No company information')}"")
                print(f""Location: {detailed_job.get('job_city', 'No city')}, {detailed_job.get('job_state', 'No state')}"")
                print(f""Description: {detailed_job.get('job_description', 'No description available')}"")
                print(f""Apply Link: {detailed_job.get('job_apply_link', 'No apply link available')}"")

                # Step 3: Estimate salary for the job title in the same location
                salary_estimation = estimate_job_salary(job_title, location)
                if salary_estimation and 'data' in salary_estimation:
                    print(f""\nSalary Estimation for '{job_title}' in {location}:"")
                    for estimate in salary_estimation['data']:
                        min_salary = estimate.get('min_salary', 'N/A')
                        max_salary = estimate.get('max_salary', 'N/A')
                        median_salary = estimate.get('median_salary', 'N/A')
                        publisher = estimate.get('publisher_name', 'N/A')
                        print(f"" - Publisher: {publisher} | Min: ${min_salary} | Max: ${max_salary} | Median: ${median_salary}"")
                else:
                    print(""No salary estimation data available."")
            else:
                print(""No detailed information available for this job."")
        else:
            print(""No valid job ID found."")
    else:
        print(""No job search results found."")
","Job Search Results:
1. Job Title: No title available - Company: No company information available - Location: No location available - Job ID: hQ_qGqqYxnh2nW31AAAAAA==
2. Job Title: No title available - Company: No company information available - Location: No location available - Job ID: qoE2sLWeZIPC_ULHAAAAAA==
3. Job Title: No title available - Company: No company information available - Location: No location available - Job ID: 6JZJ1XpxEeEsghKhAAAAAA==
4. Job Title: No title available - Company: No company information available - Location: No location available - Job ID: 626OB_pvS-CuKZRZAAAAAA==
5. Job Title: No title available - Company: No company information available - Location: No location available - Job ID: wId2R2Yukp4o-KxvAAAAAA==

Job Details for Job ID hQ_qGqqYxnh2nW31AAAAAA==:
Job Title: Software Development Engineer
Company: Adobe
Location: San Francisco, CA
Description: JOB LEVEL
P50

EMPLOYEE ROLE
Individual Contributor

The Challenge

We are seeking an experienced Senior Software Engineer to join a highly motivated and fast-paced team, building a new creativity platform that will help define the future of Creative Cloud and Adobeâ€™s Digital Media business! You will need a keen eye for details and high standards for design, code quality, and efficiency. People who enjoy taking on complex technical challenges, have a real passion for delighting customers, and who are self-motivated to push themselves in a team-oriented culture will thrive in our environment.

What Youâ€™ll Do:

On the Developer Experience team, we are responsible for increasing the velocity and scale of our development team by providing easy to use development environments, infrastructure and collaborating with other peers and product managers to create a compelling and seamless developer experience on a multi-tenant web application platform. These systems are highly reliable, performant, and continuously address the most important development needs while ensuring commits meet the rigorous check-in gates and deployment guidelines.

Youâ€™ll work with some of the best designers and engineers to integrate with 3rd party content partners, extend current Adobe ecosystems.

Responsibilities:
â€¢ Design and develop reliable Continuous Delivery Pipeline for developing, building, testing, and deployment desktop, mobile, and service offerings.
â€¢ Maintain and support internal cloud-based automation systems: infrastructure and environment, build and test tools, automation orchestration, reports, and dashboards.
â€¢ Collect, refine, and prioritize needs from the engineering team.
â€¢ Work closely with engineering team members to integrate automation tools into daily processes.
â€¢ Build tools, dashboards, and other related infrastructure to help improve the development experience for our engineers.
â€¢ Work well both independently and collaboratively within a fast-moving global development team with clear, positive, and constructive communication.
â€¢ Influence the growth of a team DevOps culture and mindset.

Requirements:
â€¢ Based in San Francisco/San Jose/Seattle
â€¢ Educational Background: Bachelorâ€™s degree in computer science or a related field. Masterâ€™s degree preferred.
â€¢ 7+ years of experience working on build and release infrastructure
â€¢ Experience developing software for multiple devices and platforms.
â€¢ Familiarity with automated building and testing of desktop, service, and web-delivered software.
â€¢ Strong written and verbal communication skills
â€¢ Experience administering Linux, Mac, and Windows systems
â€¢ CI/CD Knowledge: Hands-on experience with Continuous Integration and Continuous Deployment tools like Jenkins, CircleCI, Travis CI, and GitHub Actions. Familiarity with container orchestration platforms such as Kubernetes is a plus.
â€¢ Familiarity with Bazel a plus
â€¢ Developer Productivity Tools: Experience with developer productivity and collaboration tools such as Jira, Confluence, Slack, and VS Code extensions or plugins.
â€¢ Familiarity and experience with relevant automation technologies:
â€¢ TypeScript/Node, Python, Groovy, Bash
â€¢ Webpack, RushJS, npm
â€¢ Kubernetes, Docker
â€¢ Familiarity with Cloud platforms like AWS and Azure
â€¢ Experience with Agile development processes including Scrum.

Our compensation reflects the cost of labor across severalâ€¯ U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this positionâ€¯is $154,000 -- $278,800 annually. Payâ€¯within this range varies by work locationâ€¯and may also depend on job-related knowledge, skills,â€¯and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.

At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).

In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.

Adobe will consider qualified applicants with arrest or conviction records for employment in accordance with state and local laws and â€œfair chanceâ€ ordinances.

Internal Opportunities

Creativity, curiosity, and constant learning are celebrated aspects of your career growth journey. Weâ€™re glad that youâ€™re pursuing a new opportunity at Adobe!

Put your best foot forward:

1. Update your Resume/CV and Workday profile â€“ donâ€™t forget to include your uniquely â€˜Adobeâ€™ experiences and volunteer work.

2. Visit the Internal Mobility page on Inside Adobe to learn more about the process and set up a job alert for roles youâ€™re interested in.

3. Check out these tips to help you prep for interviews.

4. If you are applying for a role outside of your current country, ensure you review the International Resources for Relocating Employees on Inside Adobe, including the impacts to your Benefits, AIP, Equity & Payroll.

Once you apply for a role via Workday, the Talent Team will reach out to you within 2 weeks. If you move into the official interview process with the hiring team, make sure you inform your manager so they can champion your career growth.

At Adobe, you will be immersed in an exceptional work environment that is recognized around the world. You will also be surrounded by colleagues who are committed to helping each other grow through our unique Check-In approach where ongoing feedback flows freely. If youâ€™re looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the meaningful benefits we offer.

Adobe is an equal opportunity and affirmative action employer. We welcome and encourage diversity in the workplace regardless of gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other characteristics protected by law.

If you have a disability or special need that requires accommodation to navigate our internal careers site or to complete the application process, please contact accommodations@adobe.com.
Apply Link: https://careers.adobe.com/us/en/job/R147427/Software-Development-Engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic

Salary Estimation for 'Software Engineer' in San Francisco, CA, USA:
 - Publisher: Indeed | Min: $None | Max: $None | Median: $70
 - Publisher: Levels | Min: $153000 | Max: $250000 | Median: $188000
 - Publisher: Builtinsf | Min: $80000 | Max: $300000 | Median: $162000
",1,1
"Develop a houseplant exploration service to help users browse different categories of houseplants, especially ferns. Users can view a list of all available plant categories, select a category and get detailed information about ferns in it (including common names, pictures, and climate adaptations). In addition, the service also provides a general overview of houseplants, showing basic information such as classification, scientific name and growing region, so that users can have a more comprehensive understanding of various houseplants.","import requests

# Define API Key
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


# Function to retrieve all available plant categories
def get_all_categories():
    url = ""https://house-plants2.p.rapidapi.com/categories""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""house-plants2.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching plant categories: {response.status_code}"")
        return None


# Function to retrieve species by category
def get_species_by_category(category):
    url = f""https://house-plants2.p.rapidapi.com/category/{category}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""house-plants2.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        print(""\nDEBUG: Species API response data:"")
        print(response.json())  # Debugging output to check the response structure
        return response.json()
    else:
        print(f""Error fetching species for category '{category}': {response.status_code}"")
        return None


# Function to retrieve basic information about all house plants
def get_all_lite_plant_info():
    url = ""https://house-plants2.p.rapidapi.com/all-lite""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""house-plants2.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        # print(""\nDEBUG: All lite plant info API response data:"")
        # print(response.json())  # Debugging output to check the response structure
        return response.json()
    else:
        print(f""Error fetching all lite plant info: {response.status_code}"")
        return None


# Main code to demonstrate the functionality
if __name__ == '__main__':
    # Step 1: Retrieve all available plant categories
    categories = get_all_categories()
    if categories:
        print(""Available Plant Categories:"")
        for idx, category in enumerate(categories[:5], start=1):  # Limit to top 5 categories for clarity
            category_name = category.get('Category', 'Unknown Category')
            print(f""{idx}. Category: \""{category_name}\"""")

        # Attempt to select a category that might have relevant plant information
        # Example: If ""Ferns"" is not available, choose the first available category
        selected_category = next((cat['Category'] for cat in categories if ""Ferns"" in cat['Category']),
                                 categories[0].get('Category', None))

        # Step 2: Get detailed information about species in a selected category
        if selected_category:
            print(f""\nFetching species under category: {selected_category}"")
            species_data = get_species_by_category(selected_category)
            if species_data:
                print(""Species Information:"")
                for plant in species_data:
                    # Display information if it has valid data
                    common_name = plant.get('Common name', ['No common name available'])[0]  # Use first common name
                    scientific_name = plant.get('Latin name', 'No scientific name available')
                    climate = plant.get('Climat', 'No climate information available')
                    image = plant.get('Img', 'No image available')

                    # Print only if relevant fields are available
                    if common_name and scientific_name and climate:
                        print(f"" - {common_name} ({scientific_name}) - Climate: {climate} - Image URL: {image}"")

    # Step 3: Retrieve basic information about all house plants
    all_plants_info = get_all_lite_plant_info()
    if all_plants_info:
        print(""\nBasic Information for All House Plants:"")
        for plant in all_plants_info[:5]:  # Limit to top 5 plants for clarity
            category = plant.get('category', 'Unknown category')
            scientific_name = plant.get('scientific_name', 'No scientific name available')
            growth_region = plant.get('growth_region', 'No growth region available')
            common_name = plant.get('common_name', 'No common name available')

            # Only display if essential fields are present
            if category and scientific_name and common_name:
                print(
                    f"" - Category: {category}, Common Name: {common_name}, Scientific Name: {scientific_name}, Growth Region: {growth_region}"")
","Available Plant Categories:
1. Category: ""Dracaena""
2. Category: ""Palm""
3. Category: ""Anthurium""
4. Category: ""Other""
5. Category: ""Aglaonema""

Fetching species under category: Dracaena

DEBUG: Species API response data:
[{'Categories': 'Dracaena', 'Common name (fr.)': 'Janet Craig', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5556.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Janet Craig'], 'Latin name': ""Dracaena deremensis 'Janet Craig'"", 'Other names': None, 'Description': None, 'Origin': ['Cultivar'], 'id': '53417c12-4824-5995-bce0-b81984ebbd1d', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Dracaena de Malaisie', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5586.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Malaysian Dracaena'], 'Latin name': ""Dracaena reflexa 'Song of Jamaica'"", 'Other names': 'Pleomele reflexa', 'Description': None, 'Origin': ['Cultivar'], 'id': '99596292-0712-5d51-8b94-669d621c504a', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Dracaena de Malaisie', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5585.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Malaysian Dracaena'], 'Latin name': ""Dracaena reflexa 'Song of Jamaica'"", 'Other names': 'Pleomele reflexa', 'Description': None, 'Origin': ['Cultivar'], 'id': 'e5be9fde-3069-552c-969c-bb59f0fab9f2', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Dracaena de Malaisie', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5579.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Malaysian Dracaena'], 'Latin name': 'Dracaena reflexa', 'Other names': 'Pleomele reflexa', 'Description': None, 'Origin': ['Madagascar'], 'id': '7852850e-3763-5dba-978f-c406ca6cda3c', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Dracaena jaune de Malaisie', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5581.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Yellow Malaysian Dracaena'], 'Latin name': ""Dracaena reflexa 'Song of India'"", 'Other names': 'Pleomele reflexa', 'Description': None, 'Origin': ['India'], 'id': '96befb83-ccee-5747-8c22-bbe8e6863ba1', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Dracaena lign', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5563.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Striped Dracaena'], 'Latin name': ""Dracaena deremensis 'Warneckei'"", 'Other names': None, 'Description': None, 'Origin': ['Africa'], 'id': 'f44400da-9bea-5c7d-bbc1-496704196ad2', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Janet Craig', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5554.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Janet Craig'], 'Latin name': ""Dracaena deremensis 'Janet Craig'"", 'Other names': None, 'Description': None, 'Origin': ['Cultivar'], 'id': '9836efdb-fdf8-5272-be0c-4fe16b52dc61', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': None, 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5788.jpg', 'Zone': ['11', '9'], 'Family': 'Liliaceae', 'Common name': ['Spineless yucca', 'Palm lily'], 'Latin name': 'Yucca elephantipes', 'Other names': None, 'Description': None, 'Origin': ['Mexico'], 'id': 'a5178737-1b07-56b8-8667-0be3477fef48', 'Climat': 'Subtropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Dragonier', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5572.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Madagascar dragon tree'], 'Latin name': 'Dracaena marginata', 'Other names': None, 'Description': None, 'Origin': ['Madagascar'], 'id': '21cee3a1-582f-568f-b3d7-a2eeaa66ff73', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Dracaena de Malaisie', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5578.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Malaysian Dracaena'], 'Latin name': 'Dracaena reflexa', 'Other names': 'Pleomele reflexa', 'Description': None, 'Origin': ['Madagascar'], 'id': '1066840a-0631-512c-a39d-8cd97b272fd6', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Dragonier', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5575.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Madagascar dragon tree'], 'Latin name': 'Dracaena marginata', 'Other names': None, 'Description': None, 'Origin': ['Madagascar'], 'id': 'dd248d1b-b098-517f-b872-d7d4e7b30e68', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Dragonier', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5576.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Madagascar dragon tree'], 'Latin name': 'Dracaena marginata', 'Other names': None, 'Description': None, 'Origin': ['Madagascar'], 'id': '88724c5f-bfce-57f9-a226-c3bda0cbeb82', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Plante maÃ¯s', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5565.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Cornstalk plant'], 'Latin name': ""Dracaena fragrans 'Massangeana'"", 'Other names': None, 'Description': None, 'Origin': ['Cultivar'], 'id': '537f9c6b-9a10-5cf4-b319-9dfca5c52fbf', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Plante maÃ¯s', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5566.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Cornstalk plant'], 'Latin name': ""Dracaena fragrans 'Massangeana'"", 'Other names': None, 'Description': None, 'Origin': ['Cultivar'], 'id': '61a18703-167a-553a-b01b-7a7e64e8ec77', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Dracaena lign', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5553.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Striped Dracaena'], 'Latin name': ""Dracaena deremensis 'Gold Star'"", 'Other names': None, 'Description': None, 'Origin': ['Cultivar'], 'id': '421b04c3-8e84-5fe3-b59b-904e4b05b337', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Dragonier tricolor', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5577.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Madagascar dragon tree'], 'Latin name': ""Dracaena marginata 'Tricolor'"", 'Other names': None, 'Description': None, 'Origin': ['Cultivar'], 'id': 'ec853e91-c5bb-5324-93dd-91c39e1a16cb', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Dragonier', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5573.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Madagascar dragon tree'], 'Latin name': 'Dracaena marginata', 'Other names': None, 'Description': None, 'Origin': ['Madagascar'], 'id': 'c45968de-b434-559a-bc67-b369a4d9256d', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Dragonier', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5574.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Madagascar dragon tree'], 'Latin name': 'Dracaena marginata', 'Other names': None, 'Description': None, 'Origin': ['Madagascar'], 'id': '7dea0ed6-af89-5952-8457-206c44933a4e', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Janet Craig', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5558.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Janet Craig'], 'Latin name': ""Dracaena deremensis 'Janet Craig'"", 'Other names': None, 'Description': None, 'Origin': ['Cultivar'], 'id': '6c8ddbeb-d1f0-5e97-9097-a1d3a3307deb', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Janet Craig', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5557.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Janet Craig'], 'Latin name': ""Dracaena deremensis 'Janet Craig'"", 'Other names': None, 'Description': None, 'Origin': ['Cultivar'], 'id': '794a15c1-576c-5d46-8e22-73393c6fd99e', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Dracaena lign', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5562.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Striped Dracaena'], 'Latin name': ""Dracaena deremensis 'Warneckei'"", 'Other names': None, 'Description': None, 'Origin': ['Africa'], 'id': '67015c75-a987-5a4a-9b45-8cc0e9e0cff6', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Dragonier', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5571.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Madagascar dragon tree'], 'Latin name': 'Dracaena marginata', 'Other names': None, 'Description': None, 'Origin': ['Madagascar'], 'id': 'c5bb8015-bba0-5b15-871b-b6de7eb62c24', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Dracaena jaune de Malaisie', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5582.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Yellow Malaysian Dracaena'], 'Latin name': ""Dracaena reflexa 'Song of India'"", 'Other names': 'Pleomele reflexa', 'Description': None, 'Origin': ['India'], 'id': '044f37bc-d09e-5481-b2ea-bf037ea75d81', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Dracaena jaune de Malaisie', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5583.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Yellow Malaysian Dracaena'], 'Latin name': ""Dracaena reflexa 'Song of India'"", 'Other names': 'Pleomele reflexa', 'Description': None, 'Origin': ['India'], 'id': '2f7889ae-5283-5bec-82d3-a57b64dc6d2d', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Reine Calypso, Bouquet nain, Bouquet compacte', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5559.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Dwarf bouquet', 'Calypso Queen'], 'Latin name': ""Dracaena deremensis 'Janet Craig Compacta'"", 'Other names': None, 'Description': None, 'Origin': None, 'id': 'f5ed8a32-645a-55b6-a3f8-fd7f787e4cc4', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': None, 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5787.jpg', 'Zone': ['11', '9'], 'Family': 'Liliaceae', 'Common name': ['Spineless yucca', 'Palm lily'], 'Latin name': 'Yucca elephantipes', 'Other names': None, 'Description': None, 'Origin': ['Mexico'], 'id': '761a50a2-995a-5237-83aa-ec1d48467459', 'Climat': 'Subtropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Dracaena Lisa', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5561.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Lisa Dracaena'], 'Latin name': ""Dracaena deremensis 'Lisa'"", 'Other names': None, 'Description': None, 'Origin': ['Cultivar'], 'id': '4bf59df7-cea0-52cc-8665-2631cd1b57d2', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Dracaena jaune de Malaisie', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5580.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Yellow Malaysian Dracaena'], 'Latin name': ""Dracaena reflexa 'Song of India'"", 'Other names': 'Pleomele reflexa', 'Description': None, 'Origin': ['India'], 'id': 'fd6e85ca-2e7e-596d-a4b4-83f92eab592f', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Plante maÃ¯s', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5567.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Cornstalk plant'], 'Latin name': ""Dracaena fragrans 'Massangeana'"", 'Other names': None, 'Description': None, 'Origin': ['Cultivar'], 'id': '92b35a4c-f042-5405-86aa-9ecd5bfae2bf', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': None, 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5786.jpg', 'Zone': ['11', '9'], 'Family': 'Liliaceae', 'Common name': ['Spineless yucca', 'Palm lily'], 'Latin name': 'Yucca elephantipes', 'Other names': None, 'Description': None, 'Origin': ['Mexico'], 'id': 'a25a9896-8cfa-59b9-baaf-eba6134dbf4a', 'Climat': 'Subtropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Plante maÃ¯s', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5568.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Cornstalk plant'], 'Latin name': ""Dracaena fragrans 'Massangeana'"", 'Other names': None, 'Description': None, 'Origin': ['Cultivar'], 'id': 'b4321d0e-2d37-502c-8047-67da050430f1', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Dragonier', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5569.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Madagascar dragon tree'], 'Latin name': 'Dracaena marginata', 'Other names': None, 'Description': None, 'Origin': ['Madagascar'], 'id': 'e9c4c758-dcd4-553a-85b5-1a0295e81c82', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Dracaena de Malaisie', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5584.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Malaysian Dracaena'], 'Latin name': ""Dracaena reflexa 'Song of Jamaica'"", 'Other names': 'Pleomele reflexa', 'Description': None, 'Origin': ['Cultivar'], 'id': '909b1c96-236a-5b66-98c9-11ccda20efac', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Dracaena lign', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5564.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Striped Dracaena'], 'Latin name': ""Dracaena deremensis 'Warneckei'"", 'Other names': None, 'Description': None, 'Origin': ['Africa'], 'id': '8302f3da-d1ab-5e33-b15f-8eceec43365e', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Dracaena lign', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5560.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Striped Dracaena'], 'Latin name': ""Dracaena deremensis 'Lemon Lime'"", 'Other names': None, 'Description': None, 'Origin': ['Cultivar'], 'id': '8a486f54-050d-5bec-82fc-3161a3b8ccaa', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Janet Craig', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5555.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Janet Craig'], 'Latin name': ""Dracaena deremensis 'Janet Craig'"", 'Other names': None, 'Description': None, 'Origin': ['Cultivar'], 'id': '8036d7ac-428e-5875-b61e-a181249975a0', 'Climat': 'Tropical'}, {'Categories': 'Dracaena', 'Common name (fr.)': 'Dragonier', 'Img': 'http://www.tropicopia.com/house-plant/thumbnails/5570.jpg', 'Zone': ['11-10'], 'Family': 'Liliaceae', 'Common name': ['Madagascar dragon tree'], 'Latin name': 'Dracaena marginata', 'Other names': None, 'Description': None, 'Origin': ['Madagascar'], 'id': '922b8c20-ec1a-51d7-bfd4-0a5a9b6f4f02', 'Climat': 'Tropical'}]
Species Information:
 - Janet Craig (Dracaena deremensis 'Janet Craig') - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5556.jpg
 - Malaysian Dracaena (Dracaena reflexa 'Song of Jamaica') - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5586.jpg
 - Malaysian Dracaena (Dracaena reflexa 'Song of Jamaica') - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5585.jpg
 - Malaysian Dracaena (Dracaena reflexa) - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5579.jpg
 - Yellow Malaysian Dracaena (Dracaena reflexa 'Song of India') - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5581.jpg
 - Striped Dracaena (Dracaena deremensis 'Warneckei') - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5563.jpg
 - Janet Craig (Dracaena deremensis 'Janet Craig') - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5554.jpg
 - Spineless yucca (Yucca elephantipes) - Climate: Subtropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5788.jpg
 - Madagascar dragon tree (Dracaena marginata) - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5572.jpg
 - Malaysian Dracaena (Dracaena reflexa) - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5578.jpg
 - Madagascar dragon tree (Dracaena marginata) - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5575.jpg
 - Madagascar dragon tree (Dracaena marginata) - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5576.jpg
 - Cornstalk plant (Dracaena fragrans 'Massangeana') - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5565.jpg
 - Cornstalk plant (Dracaena fragrans 'Massangeana') - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5566.jpg
 - Striped Dracaena (Dracaena deremensis 'Gold Star') - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5553.jpg
 - Madagascar dragon tree (Dracaena marginata 'Tricolor') - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5577.jpg
 - Madagascar dragon tree (Dracaena marginata) - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5573.jpg
 - Madagascar dragon tree (Dracaena marginata) - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5574.jpg
 - Janet Craig (Dracaena deremensis 'Janet Craig') - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5558.jpg
 - Janet Craig (Dracaena deremensis 'Janet Craig') - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5557.jpg
 - Striped Dracaena (Dracaena deremensis 'Warneckei') - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5562.jpg
 - Madagascar dragon tree (Dracaena marginata) - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5571.jpg
 - Yellow Malaysian Dracaena (Dracaena reflexa 'Song of India') - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5582.jpg
 - Yellow Malaysian Dracaena (Dracaena reflexa 'Song of India') - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5583.jpg
 - Dwarf bouquet (Dracaena deremensis 'Janet Craig Compacta') - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5559.jpg
 - Spineless yucca (Yucca elephantipes) - Climate: Subtropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5787.jpg
 - Lisa Dracaena (Dracaena deremensis 'Lisa') - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5561.jpg
 - Yellow Malaysian Dracaena (Dracaena reflexa 'Song of India') - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5580.jpg
 - Cornstalk plant (Dracaena fragrans 'Massangeana') - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5567.jpg
 - Spineless yucca (Yucca elephantipes) - Climate: Subtropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5786.jpg
 - Cornstalk plant (Dracaena fragrans 'Massangeana') - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5568.jpg
 - Madagascar dragon tree (Dracaena marginata) - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5569.jpg
 - Malaysian Dracaena (Dracaena reflexa 'Song of Jamaica') - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5584.jpg
 - Striped Dracaena (Dracaena deremensis 'Warneckei') - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5564.jpg
 - Striped Dracaena (Dracaena deremensis 'Lemon Lime') - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5560.jpg
 - Janet Craig (Dracaena deremensis 'Janet Craig') - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5555.jpg
 - Madagascar dragon tree (Dracaena marginata) - Climate: Tropical - Image URL: http://www.tropicopia.com/house-plant/thumbnails/5570.jpg

Basic Information for All House Plants:
 - Category: Unknown category, Common Name: No common name available, Scientific Name: No scientific name available, Growth Region: No growth region available
 - Category: Unknown category, Common Name: No common name available, Scientific Name: No scientific name available, Growth Region: No growth region available
 - Category: Unknown category, Common Name: No common name available, Scientific Name: No scientific name available, Growth Region: No growth region available
 - Category: Unknown category, Common Name: No common name available, Scientific Name: No scientific name available, Growth Region: No growth region available
 - Category: Unknown category, Common Name: No common name available, Scientific Name: No scientific name available, Growth Region: No growth region available
",1,1
"Explore detailed business and location information based on the address or coordinates provided by the user. First retrieve the geographic details of a given address, including its latitude, longitude, and time zone. Once the location is determined, nearby businesses should be found on the specified coordinates, including operational details and ratings. Finally, for any business selected, details such as name, address, review, and working hours should be retrieved and displayed.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def get_geocode_info(query, country='fr', lang='en'):
    url = ""https://maps-data.p.rapidapi.com/geocoding.php""
    querystring = {""query"": query, ""lang"": lang, ""country"": country}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""maps-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def get_nearby_places(lat, lng, country='fr', lang='en'):
    url = ""https://maps-data.p.rapidapi.com/whatishere.php""
    querystring = {""lat"": str(lat), ""lng"": str(lng), ""lang"": lang, ""country"": country}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""maps-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def get_place_info(business_id, country='fr', lang='en', place_id=None):
    url = ""https://maps-data.p.rapidapi.com/place.php""
    querystring = {""business_id"": business_id, ""country"": country, ""lang"": lang}
    if place_id:
        querystring[""place_id""] = place_id
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""maps-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def format_geocode_info(data):
    print(""\n### Geocode Information ###"")
    if 'data' in data:
        address = data['data'].get('address', 'Unknown')
        lat = data['data'].get('lat', 'Unknown')
        lng = data['data'].get('lng', 'Unknown')
        timezone = data['data'].get('timezone', 'Unknown')

        print(f""Address   : {address}"")
        print(f""Latitude  : {lat}"")
        print(f""Longitude : {lng}"")
        print(f""Timezone  : {timezone}"")
    else:
        print(""No geocode information available. Response:"", data)


def format_nearby_places(data):
    print(""\n### Nearby Places Information ###"")
    if 'data' in data:
        address = data['data'].get('address', 'Unknown')
        town = data['data'].get('town', 'Unknown')
        country = data['data'].get('country', 'Unknown')
        timezone = data['data'].get('timezone', 'Unknown')

        print(f""Address   : {address}"")
        print(f""Town      : {town}"")
        print(f""Country   : {country}"")
        print(f""Timezone  : {timezone}"")

        places = data['data'].get('places', [])
        if places:
            print(""\n### List of Nearby Places ###"")
            for i, place in enumerate(places, start=1):
                business_id = place.get('business_id', 'Unknown')
                place_name = place.get('name', 'Unknown')
                print(f""{i}. {place_name} (Business ID: {business_id})"")
        else:
            print(""No nearby places found."")
    else:
        print(""No nearby places information available. Response:"", data)


def format_place_info(data):
    print(""\n### Detailed Place Information ###"")
    if 'data' in data:
        name = data['data'].get('name', 'Unknown')
        address = data['data'].get('address', 'Unknown')
        phone = data['data'].get('phone', 'Unknown')
        website = data['data'].get('website', 'Unknown')

        print(f""Name     : {name}"")
        print(f""Address  : {address}"")
        print(f""Phone    : {phone}"")
        print(f""Website  : {website}"")
    else:
        print(""No detailed place information available. Response:"", data)


if __name__ == '__main__':
    # Step 1: Geocode information from an address
    address = ""Place de la Navigation""
    geo_data = get_geocode_info(address)
    format_geocode_info(geo_data)

    if 'data' in geo_data:
        lat = geo_data['data']['lat']
        lng = geo_data['data']['lng']

        # Step 2: Get nearby places based on the lat/lng
        nearby_places = get_nearby_places(lat, lng)
        format_nearby_places(nearby_places)

        # Step 3: Get detailed place information for the first business found
        if 'data' in nearby_places and len(nearby_places['data'].get('places', [])) > 0:
            first_business_id = nearby_places['data']['places'][0].get('business_id')
            if first_business_id:
                place_info = get_place_info(first_business_id)
                format_place_info(place_info)
","### Geocode Information ###
Address   : Pl. de la Navigation, 1110 Morges, Switzerland
Latitude  : 46.5062704
Longitude : 6.497078699999999
Timezone  : Europe/Zurich

### Nearby Places Information ###
Address   : Aspro Grand Surprise, Pl. de la Navigation, 1110 Morges, Switzerland
Town      : Morges, Switzerland
Country   : CH
Timezone  : Europe/Zurich
No nearby places found.
",1,1
"Build a service that allows users to search for Reddit posts on a specific topic or within a subreddit, retrieve hot or top posts from a specific subreddit, and get detailed information about individual posts. The service should begin by allowing users to search Reddit posts using specific keywords, subreddit filters, and sort options. After displaying the results, users can retrieve a list of trending posts (hot or top) from a particular subreddit. For selected posts, detailed metadata such as the title, author, and engagement metrics should be fetched.","import requests

# Define API Key
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


# Function to search Reddit posts based on search term, subreddit, and filters
def search_reddit_posts(search_term, subreddit='', filter_type='posts', time_filter='all', sort_type='relevance'):
    url = ""https://reddit3.p.rapidapi.com/v1/reddit/search""
    querystring = {
        ""search"": search_term,
        ""subreddit"": subreddit,
        ""filter"": filter_type,
        ""timeFilter"": time_filter,
        ""sortType"": sort_type
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""reddit3.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching Reddit search results: {response.status_code}"")
        return None


# Function to retrieve posts from a specific subreddit (hot or top)
def get_subreddit_posts(subreddit_url, filter_type='hot'):
    url = ""https://reddit3.p.rapidapi.com/v1/reddit/posts""
    querystring = {
        ""url"": subreddit_url,
        ""filter"": filter_type
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""reddit3.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching subreddit posts: {response.status_code}"")
        return None


# Function to retrieve details of a specific post
def get_post_details(post_url):
    url = ""https://reddit3.p.rapidapi.com/v1/reddit/post""
    querystring = {
        ""url"": post_url
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""reddit3.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        print(""\nDEBUG: Post details API response data:"")
        print(response.json())  # Debugging output to check the response structure
        return response.json()
    else:
        print(f""Error fetching post details: {response.status_code}"")
        return None


# Main code to demonstrate the functionality
if __name__ == '__main__':
    # Step 1: Search Reddit posts based on a search term
    search_term = ""investing""
    subreddit = ""wallstreetbets""
    search_results = search_reddit_posts(search_term, subreddit, filter_type=""posts"", time_filter=""year"",
                                         sort_type=""relevance"")

    if search_results and 'body' in search_results:
        print(""Search Results:"")
        for idx, post in enumerate(search_results['body'][:5], start=1):  # Limit to top 5 results for clarity
            title = post.get('title', 'No title available')
            author = post.get('author', 'Unknown author')
            score = post.get('score', 'N/A')
            permalink = post.get('permalink', 'No URL available')
            print(f""{idx}. Title: {title} - Author: {author} - Score: {score} - URL: {permalink}"")
    else:
        print(""No search results found."")

    # Step 2: Retrieve top or hot posts from a specific subreddit
    subreddit_url = ""https://www.reddit.com/r/wallstreetbets""
    subreddit_posts = get_subreddit_posts(subreddit_url, filter_type=""hot"")

    if subreddit_posts and 'body' in subreddit_posts:
        print(""\nSubreddit Posts (Hot):"")
        for idx, post in enumerate(subreddit_posts['body'][:5], start=1):  # Limit to top 5 results for clarity
            title = post.get('title', 'No title available')
            author = post.get('author', 'Unknown author')
            score = post.get('score', 'N/A')
            post_url = post.get('url', 'No URL available')
            print(f""{idx}. Title: {title} - Author: {author} - Score: {score} - URL: {post_url}"")

    # Step 3: Get detailed information about a specific post
    if 'body' in subreddit_posts and len(subreddit_posts['body']) > 0:
        first_post_url = subreddit_posts['body'][0].get('url', None)
        if first_post_url:
            post_details = get_post_details(first_post_url)

            if post_details and 'body' in post_details:
                post_info = post_details['body'].get('post', {})

                # Extract and display data if available
                detailed_title = post_info.get('title', 'No title available')
                detailed_author = post_info.get('author', 'Unknown author')
                upvotes = post_info.get('ups', 'N/A')
                comments = post_info.get('num_comments', 'N/A')
                post_content = post_info.get('selftext', 'No content available')

                print(""\nPost Details:"")
                print(f""Title: {detailed_title}"")
                print(f""Author: {detailed_author}"")
                print(f""Upvotes: {upvotes}"")
                print(f""Comments: {comments}"")
                print(f""Content: {post_content}"")
            else:
                print(""No valid data found in the post details response."")
        else:
            print(""No valid URL found for the selected post."")
    else:
        print(""No subreddit posts found."")
","Search Results:
1. Title: After 4 years of investing I made $16 lol - Author: raseC_Ceda - Score: 28979 - URL: /r/wallstreetbets/comments/1ctf81t/after_4_years_of_investing_i_made_16_lol/
2. Title: If investing is so easy, why doesn't everybody do it? - Author: precita - Score: 544 - URL: /r/Bogleheads/comments/1aohea2/if_investing_is_so_easy_why_doesnt_everybody_do_it/
3. Title: [OC] Why you should start investing early in life - Author: PieChartPirate - Score: 19619 - URL: /r/dataisbeautiful/comments/wo6fpu/oc_why_you_should_start_investing_early_in_life/
4. Title: Does Investing Actually Work? - Author: Low_Entry5644 - Score: 96 - URL: /r/stocks/comments/1694h71/does_investing_actually_work/
5. Title: How do I start investing? - Author: MirafuCh - Score: 5 - URL: /r/ValueInvesting/comments/18410ic/how_do_i_start_investing/

Subreddit Posts (Hot):
1. Title: Weekend Discussion Thread for the Weekend of October 25, 2024 - Author: wsbapp - Score: 66 - URL: https://www.reddit.com/r/wallstreetbets/comments/1gc374e/weekend_discussion_thread_for_the_weekend_of/
2. Title: Weekly Earnings thread 10/28 - 11/1 - Author: OSRSkarma - Score: 300 - URL: https://i.redd.it/xy8kbewinywd1.jpeg
3. Title: New York Stock Exchange to extend trading hours to 22 hours a day.  - Author: Maximum_Fill987 - Score: 3941 - URL: https://i.redd.it/fk0gh5kvb0xd1.jpeg
4. Title: Turned $10k into $172k in one trade. You guys told me to sell, so I held - Author: vuw960 - Score: 15484 - URL: https://www.reddit.com/gallery/1gc45oi
5. Title: Just protecting retail traders - Author: King-of-Plebss - Score: 2172 - URL: https://i.redd.it/1ye15t7rg0xd1.jpeg

DEBUG: Post details API response data:
{'meta': {'version': 'v1.0', 'status': 200, 'copywrite': 'https://apicalls.io', 'totalComments': 83}, 'body': {'post': {'approved_at_utc': None, 'subreddit': 'wallstreetbets', 'selftext': 'This post contains content not supported on old Reddit. [Click here to view the full post](https://sh.reddit.com/r/wallstreetbets/comments/1gc374e)', 'user_reports': [], 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Weekend Discussion Thread for the Weekend of October 25, 2024', 'link_flair_richtext': [{'e': 'text', 't': 'Weekend Discussion'}], 'subreddit_name_prefixed': 'r/wallstreetbets', 'hidden': False, 'pwls': 7, 'link_flair_css_class': 'weekend', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gc374e', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 63, 'total_awards_received': 0, 'media_embed': [], 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'author_fullname': 't2_qbvp0eq8b', 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': [], 'link_flair_text': 'Weekend Discussion', 'can_mod_post': False, 'score': 63, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': [], 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1729886233, 'link_flair_type': 'richtext', 'wls': 7, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.wallstreetbets', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This post contains content not supported on old Reddit. &lt;a href=""https://sh.reddit.com/r/wallstreetbets/comments/1gc374e""&gt;Click here to view the full post&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': 'new', 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': True, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2th52', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': '1gc374e', 'is_robot_indexable': True, 'num_duplicates': 0, 'report_reasons': None, 'author': 'wsbapp', 'discussion_type': None, 'num_comments': 3972, 'send_replies': False, 'media': None, 'contest_mode': False, 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/wallstreetbets/comments/1gc374e/weekend_discussion_thread_for_the_weekend_of/', 'stickied': True, 'url': 'https://www.reddit.com/r/wallstreetbets/comments/1gc374e/weekend_discussion_thread_for_the_weekend_of/', 'subreddit_subscribers': 16922418, 'created_utc': 1729886233, 'num_crossposts': 0, 'mod_reports': [], 'is_video': False}, 'post_comments': [{'id': 'ltvwgjo', 'author': 'cryptohorn', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729966270, 'content': 'People at the rally thought they were getting bey-once but got willie Nelson instead ![img](emote|t5_2th52|4271)![img](emote|t5_2th52|4271)![img](emote|t5_2th52|4271)'}, {'id': 'ltvw9yh', 'author': 'OB1KENOB', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729966210, 'content': 'Whiplash doesnâ€™t make any sense. No one on earth would leave Supergirl to play jazz drums.'}, {'id': 'ltvw733', 'author': 'BigBoiBenisBlueBalls', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729966183, 'content': 'My grandpa is in his 80s but still has a full head of hair. Itâ€™s thinned a lot but still good. If he were to take fin and minoxidil would it become thicker? This wonâ€™t happen but Iâ€™m just curious'}, {'id': 'ltvw4xr', 'author': 'Anoos_specialist', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729966163, 'content': '![img](emote|t5_2th52|4260)'}, {'id': 'ltvw3q0', 'author': 'Anoos_specialist', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729966152, 'content': 'Carefully waiting for the next dump. And THEN I POUNCE like a total regard!'}, {'id': 'ltvvv5o', 'author': 'lipmanz', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729966072, 'content': 'David Goggins is a hard worker but I think appeals most to dudes with a humiliation fetish'}, {'id': 'ltvvpbd', 'author': 'NotSoTough-Tony', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729966019, 'content': 'Breaking News: ðŸ¥­ announce if he wins, Ivanka will be releasing her porno films on Pornhub every month during his term\xa0'}, {'id': 'ltvvjr8', 'author': 'AquamDeus', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729965968, 'content': ""Don't care who wins the election, simply trying to profit. Calls or puts on DJT?""}, {'id': 'ltvvfma', 'author': 'oregonLov', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729965929, 'content': 'When tf did talking about the leader of the US become banned?\n\nTariffs gon crash my stock'}, {'id': 'ltvv4ly', 'author': 'DeepSweatyButthole', 'up_votes': 4, 'score': 4, 'likes': '', 'created_utc': 1729965827, 'content': 'Lot of mango banners and flags in front of peoples houses. The time is near ![img](emote|t5_2th52|12787)'}, {'id': 'ltvuqko', 'author': '[deleted]', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729965695, 'content': '[removed]'}, {'id': 'ltvuou3', 'author': 'warlock22041', 'up_votes': 4, 'score': 4, 'likes': '', 'created_utc': 1729965679, 'content': ""AMD 170 please god. I'll never gamble again""}, {'id': 'ltvtzx5', 'author': 'mysuruhuduga', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729965448, 'content': 'I told this bmw dealership to give me 3.99% on this used 2023 3 series with &lt; 5k miles else I would wait until jpow will cut the interest rate in Nov/ Dec meeting\n\nThey are offering 3.99% on 2019-2022, either they agree to my deal else they can duck off'}, {'id': 'ltvtzqa', 'author': 'WildConsideration376', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729965446, 'content': 'War is so bullish..next week run fueled by big cap earnings will send spy to $600.Next week will birth thousandnaires &amp; millionaires..all you have to do is drop otm calls &amp; puts on 5 big stocks'}, {'id': 'ltvtyub', 'author': 'spellbadgrammargood', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729965438, 'content': 'i took 2 ""i voted"" stickers.. does this make me a felon?'}, {'id': 'ltvtynq', 'author': 'smart_doge', 'up_votes': 3, 'score': 3, 'likes': '', 'created_utc': 1729965436, 'content': 'Lunch time ðŸ ![img](emote|t5_2th52|4258)'}, {'id': 'ltvtqh2', 'author': 'Totallynotfakenews', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729965361, 'content': 'CVNA has to be a big earnings dump right?'}, {'id': 'ltvtmr0', 'author': 'DeepSweatyButthole', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729965326, 'content': 'What do you think about starting the bike ride off with a little 53rd and 3rd, Youâ€™re Gonna Kill that girl and then going right into Red Fucken Hot ðŸ¤”'}, {'id': 'ltvtjhy', 'author': 'VisualMod', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729965297, 'content': '#Ban Bet Lost\n\n/u/No_Onion_8612 made a bet that TSLA would go to 200.0 within **5 days** when it was 217.865 and it did not, so they were banned for a week.\n\nTheir record is now 0 wins and 1 losses\n\n[**Join WSB Discord**](http://discord.gg/wsbverse)'}, {'id': 'ltvtb5b', 'author': 'lipmanz', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729965220, 'content': 'When I was in college I had a classmate whoâ€™s dad owned some big company in Brazil, drove a Ferrari to class and had his own beachfront condo at 19â€¦we were playing doubles tennis with 2 hot chicks and he just whipped his thing out and starting running around the court as a â€œjokeâ€, wonder whatever happened to him'}, {'id': 'ltvszaw', 'author': 'Rich265', 'up_votes': -1, 'score': -1, 'likes': '', 'created_utc': 1729965110, 'content': 'Democrats blame Republicans for, â€œtax cuts that led to low revenue levels that increased the debt.â€\n\nLet\'s see.. Tax Cuts took effect in 2018.\n\nTax Revenues for 2018:  $3.33 trillion  \nTax Revenues for 2023:  $4.44 trillion\n\nA 33% increase in tax revenue caused debt to increase?\n\nLet\'s see what the non-partisan Congressional Budget Office report said on the tax cuts..\n\n""CBO also estimated the macroeconomic feedback resulting from the tax act. CBO estimated that the economic changes resulting from the actâ€”not including its effects on debt serviceâ€”would reduce primary deficits by $0.571 trillion over the 2018â€“2028 period. The main reason was that the act would boost taxable income and thus federal revenues.""\n\nOh, they say it would reduce deficits due to boosting federal revenues.. weird!'}, {'id': 'ltvsrxk', 'author': 'strictlyPr1mal', 'up_votes': 4, 'score': 4, 'likes': '', 'created_utc': 1729965043, 'content': ""I will be banned next week, but just know I'm still going to be yoloing on earnings every day""}, {'id': 'ltvsraw', 'author': 'lipmanz', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729965037, 'content': 'IMO the pundits claiming no hike on the 7th are wrong, the whole point is to bring rates down which hasnâ€™t happened, in fact they are practically where they were before the last cut'}, {'id': 'ltvskbg', 'author': 'isolated-cat', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729964974, 'content': 'should i call the cute girl who approached me last night and left me her number? she was texting me last night once she left, so i do believe that she was interested. my gut says to just call her today and get it out of the system. what do i got to lose? right fellas? should i wait another day?'}, {'id': 'ltvsgt4', 'author': 'DeepSweatyButthole', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729964943, 'content': 'We are men! We shit in our pants and grab each others cocks!'}, {'id': 'ltvsfce', 'author': 'Coffee_Buzzzz', 'up_votes': 3, 'score': 3, 'likes': '', 'created_utc': 1729964929, 'content': 'I expect things to change for the better without changing myself or doing anything different ![img](emote|t5_2th52|12787)'}, {'id': 'ltvs8vf', 'author': 'AngusMcTibbins', 'up_votes': 4, 'score': 4, 'likes': '', 'created_utc': 1729964869, 'content': 'Beautiful fall day. Got a fresh cup of coffee sitting on the edge of the tub while I sit on the toilet for a relaxing poop. Life is good, my friends ![img](emote|t5_2th52|8882)'}, {'id': 'ltvs1rg', 'author': 'luoyuke', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729964804, 'content': ""feel like it will be a sell on the news no matter what happens. No one knows how to react, in a situation like this it's always sell first and jump in later""}, {'id': 'ltvs1ej', 'author': 'Slow-Information-847', 'up_votes': -1, 'score': -1, 'likes': '', 'created_utc': 1729964801, 'content': 'stocks will be up big time next Monday. Iran just said they will retaliate.'}, {'id': 'ltvrrvj', 'author': 'Blazzck7', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729964715, 'content': 'Rolling the dice on some otm tesla calls for 3 weeks out \U0001fae1'}, {'id': 'ltvr8rk', 'author': 'goodbuddyedb', 'up_votes': 3, 'score': 3, 'likes': '', 'created_utc': 1729964540, 'content': ""Plunge protection ain't got shit on my puts . Suck it you dumb sheep that think you are 'bulls'""}, {'id': 'ltvqt7g', 'author': 'SwiftSG1', 'up_votes': 3, 'score': 3, 'likes': '', 'created_utc': 1729964403, 'content': 'Election pump. Rate cut pump. Santa pump. Then fomo pump because itâ€™s up so much. ![img](emote|t5_2th52|4258)'}, {'id': 'ltvqeew', 'author': 'GenericUser4104', 'up_votes': 4, 'score': 4, 'likes': '', 'created_utc': 1729964271, 'content': 'Bers furiously refreshing BBC News for more war porn'}, {'id': 'ltvqb49', 'author': 'wildcat1055', 'up_votes': 5, 'score': 5, 'likes': '', 'created_utc': 1729964242, 'content': 'I thought I was one of the worst, horniest and most degenerate gamblers out there, but I just found out about the regards losing money on Taylor Mathis meme coin. There are levels to this shit.\n\nGod Bless America!'}, {'id': 'ltvq4oz', 'author': 'Razvanell95', 'up_votes': 5, 'score': 5, 'likes': '', 'created_utc': 1729964186, 'content': 'I sold a big part of my portfolio right before this great month. I want to stop smelling like disappointment. ![img](emote|t5_2th52|4260)\n\nCan someone recommend me a good fragrance?'}, {'id': 'ltvpvnu', 'author': 'necarpenter417', 'up_votes': 4, 'score': 4, 'likes': '', 'created_utc': 1729964105, 'content': 'Dang. Somehow I lost 12k last week.'}, {'id': 'ltvpv5l', 'author': 'SilentHillFan12', 'up_votes': 4, 'score': 4, 'likes': '', 'created_utc': 1729964101, 'content': 'So what - they ban you now just for posting 600 times a day in the daily thread? This fucking country'}, {'id': 'ltvpeln', 'author': 'rufus170', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729963951, 'content': 'When is LUNR gonna moon? So far iâ€™ve only seen jumps to around 8,50 but iâ€™ve been expecting much more, just like a lot of the ppl on this sub\n\nIâ€™m patient, itâ€™s just taking longer than what i was thinking'}, {'id': 'ltvp9wd', 'author': 'EnginrA', 'up_votes': 6, 'score': 6, 'likes': '', 'created_utc': 1729963909, 'content': 'They used to call me Johnny Thickloads ![img](emote|t5_2th52|4275)'}, {'id': 'ltvovtr', 'author': 'rc5193', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729963783, 'content': 'Who else pops a large chunk of their port into corn for the weekend for the pumps? Is this addiction? ![img](emote|t5_2th52|4271)'}, {'id': 'ltvohan', 'author': 'BigBoiBenisBlueBalls', 'up_votes': -3, 'score': -3, 'likes': '', 'created_utc': 1729963650, 'content': 'Anyone excited for Tyler the creators new album? I havenâ€™t listened to any of the songs from it yet but I donâ€™t really care for it or him anymore. What bugs me about him is his clothing brand. Golf wang. Shit is expensive AND shit quality. The t shirts felt like paper. Itâ€™s awful. Also he kept releasing terrible designs for at least 2 years. He kept putting out shit so I stopped listening to him'}, {'id': 'ltvo83y', 'author': 'sloanemonroe', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729963568, 'content': 'I have covid. Anyone want to come over and hang out?'}, {'id': 'ltvo43f', 'author': 'Diamondhands4dagainz', 'up_votes': 3, 'score': 3, 'likes': '', 'created_utc': 1729963531, 'content': 'Watching top gun, it better have my favourite plane ever: F-22 raptor $LMT'}, {'id': 'ltvnv12', 'author': 'StockMarketDavid', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729963448, 'content': 'I invested 15K at 10.50$ into the ****ware stock that seems to follow DJT ðŸ‘€ im thinking leading into election it go up then fall off day before election and the week after ( regardless of winner ) what yall thinking lol'}, {'id': 'ltvnttd', 'author': 'VisualMod', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729963437, 'content': '#Ban Bet Lost\n\n/u/Cantthinkofacoolnam made a bet that NOW would go to 1000.0 within **1 week** when it was 922.375 and it did not, so they were banned for a week.\n\nTheir record is now 1 wins and 2 losses\n\n[**Join WSB Discord**](http://discord.gg/wsbverse)'}, {'id': 'ltvnq8d', 'author': 'ScoopityWoop89', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729963404, 'content': 'Are apple calls regarded?'}, {'id': 'ltvnm5p', 'author': 'Funny_Story2759', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729963366, 'content': 'when i roll these options to shares after moass ima petition to rename purecycle to PlasticStop'}, {'id': 'ltvnhgs', 'author': 'mcs5280', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729963325, 'content': ""tfw your choice in the Halloween candy basket is a candy that is obviously poisoned and bad for you or a candy that isn't obviously poisoned but\xa0you strongly dislike it and you'd rather not choose either""}, {'id': 'ltvnbxn', 'author': 'joe_bidens_underwear', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729963275, 'content': '\nThink MARA gets back to $20 soon?'}, {'id': 'ltvn8gz', 'author': 'WSBNon-Believer', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729963244, 'content': 'All around me are familiar faces'}, {'id': 'ltvn858', 'author': 'Learningcurvve', 'up_votes': 3, 'score': 3, 'likes': '', 'created_utc': 1729963241, 'content': '#$311.53\n\nTSLA hits a $1T market cap'}, {'id': 'ltvn1r4', 'author': 'alittlerogue', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729963185, 'content': 'Thinking aloud, in my play acc I am up 45% on NVDA shares with avg cost $99. I have $17k loss in fucking POS Nio. Should I sell both and collect profit or sell Nio and ride out NVDA? Still have NVDA in my retirement since 2017. Just weary on elections playing out and the selling chips to China sentiment.'}, {'id': 'ltvmtds', 'author': 'spy69eod', 'up_votes': 3, 'score': 3, 'likes': '', 'created_utc': 1729963108, 'content': 'Market sell off has just begun bols. Enjoy being exit liquidity'}, {'id': 'ltvlp5t', 'author': 'Ok_Ambassador_4311', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729962743, 'content': 'https://www.instagram.com/reel/DArBEPjPHDA/?igsh=Y3N0dWRpdHdmZTNz me at Wall St in 5 years'}, {'id': 'ltvlevn', 'author': 'Accomplished_Dot9815', 'up_votes': 4, 'score': 4, 'likes': '', 'created_utc': 1729962651, 'content': 'Sup faggets'}, {'id': 'ltvkwp3', 'author': 'No-Improvement3164', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729962485, 'content': 'What are the chances of Meta Netflix and NVDA ever selling off like they did a couple years ago.'}, {'id': 'ltvkuhh', 'author': 'ChildhoodOpening3360', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729962465, 'content': 'Started my account at the beg of month with 1.5k. Consistently was up until 13.8k then got cocky and bought dumb shit. Lost 11k of it yesterday and for some reason bought calls going into the weekend. Usually only make money flipping between 945am and 11am each day but might be fucked with these series of abysmal decisions.\n\nHolding options for any length of time is how I lose my port'}, {'id': 'ltvjylc', 'author': 'E39_CBX', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729962175, 'content': 'I need to turn 8k into 30k by EOY whatâ€™s the play'}, {'id': 'ltvjx1e', 'author': 'Ok_Pineapple_5700', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729962160, 'content': ""Don't be so humble, you're not that great""}, {'id': 'ltvj4ab', 'author': 'cinJESUS', 'up_votes': -1, 'score': -1, 'likes': '', 'created_utc': 1729961904, 'content': 'can this Elon fucker let me invest in spacex? ![img](emote|t5_2th52|27421)![img](emote|t5_2th52|27421)![img](emote|t5_2th52|27421)'}, {'id': 'ltvitop', 'author': 'yeetwagon', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729961809, 'content': 'Canâ€™t wait to buy $7,000 of RTX on January 1st ![img](emote|t5_2th52|4258)'}, {'id': 'ltvim0h', 'author': 'Nomynametoday', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729961740, 'content': 'How do u guys play weekly calls, if the companyâ€™s earnings is on friday?'}, {'id': 'ltvi984', 'author': 'Heedunk', 'up_votes': 5, 'score': 5, 'likes': '', 'created_utc': 1729961625, 'content': 'The next 1.5 weeks are going to be crazy with tech earnings, Halloween, and the election!'}, {'id': 'ltvi5df', 'author': 'ManBearPig_1983', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729961590, 'content': 'McDonaldâ€™s frosty machine down AGAIN!! Wtf'}, {'id': 'ltvi4fr', 'author': 'BigDiplomacy', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729961582, 'content': ""Nothing like a relaxing Saturday day, meal-prepping some protein packed meals for your wife's boyfriend.""}, {'id': 'ltvi3kp', 'author': 'LarryStink', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729961574, 'content': 'Going to close all open positions this week, except my TLT puts, and open a single SPX 1wte long condor betting on volatility from earnings this week. Skewed with lower put deltas as I see risk to the downside as greater and the move would be larger and faster than to the upside. And I will hesitate to get directional with any size until after this week is over.\xa0'}, {'id': 'ltvi0w2', 'author': 'Ok_Pineapple_5700', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729961551, 'content': ""Easy to spot when he's full of shit""}, {'id': 'ltvhhp1', 'author': 'digdugbug', 'up_votes': 4, 'score': 4, 'likes': '', 'created_utc': 1729961377, 'content': 'If I had a million could have lost them yesterday! Glad I only had 5k.'}, {'id': 'ltvh8e1', 'author': 'Resident_Pudding_681', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729961293, 'content': 'sad boi hours already startin I see'}, {'id': 'ltvgs14', 'author': 'Slizano12', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729961144, 'content': ""Still waiting for we ride to go public and make an official. In the meantime been buying up chubb and Amazon because it's almost Christmas I have a feeling Amazon is just got rocket along with Alibaba and Bank of America because everybody's going to get swipe happy. Plus I don't know why I have a feeling that Bitcoin my skyrocket.""}, {'id': 'ltvgjbp', 'author': 'LarryStink', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729961066, 'content': 'Had I not been a brokie and used more money for that trade, say 17,000 instead of 170, I would have made 200,000$ in a day instead of 2,000'}, {'id': 'ltvfy5z', 'author': 'BushLov3r', 'up_votes': 3, 'score': 3, 'likes': '', 'created_utc': 1729960873, 'content': 'Sentiment is pretty shit on google, might be the perfect call play because of it'}, {'id': 'ltvfnto', 'author': 'yaboiRich', 'up_votes': 4, 'score': 4, 'likes': '', 'created_utc': 1729960779, 'content': 'Just jacked off. What do now?'}, {'id': 'ltvf8dr', 'author': 'Slizano12', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729960640, 'content': 'This last week the market was all over the place wtf!!!'}, {'id': 'ltvf3yi', 'author': 'Hwangoutpltr', 'up_votes': 5, 'score': 5, 'likes': '', 'created_utc': 1729960599, 'content': '#ðŸ¥­'}, {'id': 'ltveuwi', 'author': 'UsernameTaken_123', 'up_votes': 7, 'score': 7, 'likes': '', 'created_utc': 1729960517, 'content': 'Penis day'}, {'id': 'ltvepp6', 'author': 'felipesizzosmoes', 'up_votes': 3, 'score': 3, 'likes': '', 'created_utc': 1729960469, 'content': 'Back day'}, {'id': 'ltvedf5', 'author': 'Glowie-in-Chief', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729960358, 'content': '#The WEAVE'}, {'id': 'ltveb9d', 'author': 'nerdy-nate', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729960339, 'content': 'I bought a TOGA costume from Spirit Halloween for $20 last night and it FUCKINNNN SUCKSSS (I know, I know, $20). Anyways, there is a costume/halloween party theme at this nightclub me and my friends are going to tonight and I need a new costume ASAP! Is anyone here doing a low-effort costume or anyone have any ideas on what I could quickly throw together for a half-ass costume? ![img](emote|t5_2th52|4271)'}, {'id': 'ltve9v6', 'author': 'MSTR_CallsAtOpen', 'up_votes': 3, 'score': 3, 'likes': '', 'created_utc': 1729960326, 'content': 'I loaded up on MSTR calls for over the weekend because I believe Bitcoin could be about to break out. If I am right I am rich'}, {'id': 'ltve0wa', 'author': 'DeepSweatyButthole', 'up_votes': 0, 'score': 0, 'likes': '', 'created_utc': 1729960245, 'content': '#McCREAM!!!!!!!!!'}, {'id': 'ltvdzir', 'author': 'isolated-cat', 'up_votes': 3, 'score': 3, 'likes': '', 'created_utc': 1729960233, 'content': 'anyone else feeling that mag7 earnings will be mild and not very strong bringing the market further down?'}, {'id': 'ltvcpb6', 'author': '', 'up_votes': '', 'score': '', 'likes': '', 'created_utc': '', 'content': ''}]}}

Post Details:
Title: Weekend Discussion Thread for the Weekend of October 25, 2024
Author: wsbapp
Upvotes: 63
Comments: 3972
Content: This post contains content not supported on old Reddit. [Click here to view the full post](https://sh.reddit.com/r/wallstreetbets/comments/1gc374e)
",1,1
"The service allows users to retrieve and explore global language information, get a list of all available languages, and see the country information for each language. In addition, users can choose to view a list of all countries in the world, regardless of language, to support a wider range of references. The service validates the API response format to ensure that it can accommodate different data return structures.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_all_languages():
    url = ""https://auto-parts-catalog.p.rapidapi.com/languages/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""auto-parts-catalog.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching languages: {response.status_code}"")
        return None

def get_all_countries():
    url = ""https://auto-parts-catalog.p.rapidapi.com/countries/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""auto-parts-catalog.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        data = response.json()
        # Debug information for data structure
        return data
    else:
        print(f""Error fetching countries: {response.status_code}"")
        return None

def get_countries_by_language(lang_id):
    url = f""https://auto-parts-catalog.p.rapidapi.com/countries/list-countries-by-lang-id/{lang_id}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""auto-parts-catalog.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        data = response.json()
        # Debug information for data structure
        return data
    else:
        print(f""Error fetching countries for language ID {lang_id}: {response.status_code}"")
        return None

if __name__ == '__main__':
    # Step 1: Retrieve a list of all available languages
    languages = get_all_languages()
    if languages:
        print(""### Available Languages ###"")
        # Show up to 20 languages
        for idx, lang in enumerate(languages[:20], start=1):
            print(f""{idx}. Language: {lang.get('name', 'Unknown')} - ID: {lang.get('lngId', 'N/A')}"")

    # Step 2: Retrieve a list of all countries
    all_countries = get_all_countries()
    if all_countries and isinstance(all_countries, dict) and 'countries' in all_countries:
        print(""\n### All Countries ###"")
        # Show up to 20 countries
        for idx, country in enumerate(all_countries['countries'][:20], start=1):
            print(f""{idx}. Country: {country.get('countryName', 'Unknown')} - Code: {country.get('couCode', 'N/A')}"")
    else:
        print(""\nNo valid country data available."")

    # Step 3: Retrieve countries associated with a selected language ID
    if languages and len(languages) > 0:
        first_lang_id = languages[0].get('lngId')
        if first_lang_id:
            countries_by_language = get_countries_by_language(first_lang_id)
            if countries_by_language and isinstance(countries_by_language, dict) and 'countries' in countries_by_language:
                print(f""\n### Countries for Language ID {first_lang_id} ###"")
                # Show up to 20 countries associated with the language
                for idx, country in enumerate(countries_by_language['countries'][:20], start=1):
                    print(f""{idx}. Country: {country.get('countryName', 'Unknown')} - Code: {country.get('couCode', 'N/A')}"")
            else:
                print(f""\nNo valid country data available for language ID {first_lang_id}."")
",Fail,-1,-1
"This service retrieves detailed information about vehicle types and engine specifications. It first lists all available vehicle types, each identified by a unique ID and category. For a selected vehicle type, the service fetches technical specifications such as manufacturer, model, and engine attributes. Additionally, the service allows for retrieving specific engine details for a selected model, focusing on attributes like engine power, production dates, fuel type, and engine codes.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def list_vehicle_types():
    url = ""https://auto-parts-catalog.p.rapidapi.com/types/list-vehicles-type""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""auto-parts-catalog.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()


def get_vehicle_type_details(vehicle_id, manufacturer_id, lang_id, country_filter_id, type_id):
    url = f""https://auto-parts-catalog.p.rapidapi.com/types/vehicle-type-details/{vehicle_id}/manufacturer-id/{manufacturer_id}/lang-id/{lang_id}/country-filter-id/{country_filter_id}/type-id/{type_id}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""auto-parts-catalog.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()


def get_vehicle_engine_types(model_series_id, manufacturer_id, lang_id, country_filter_id, type_id):
    url = f""https://auto-parts-catalog.p.rapidapi.com/types/list-vehicles-types/{model_series_id}/manufacturer-id/{manufacturer_id}/lang-id/{lang_id}/country-filter-id/{country_filter_id}/type-id/{type_id}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""auto-parts-catalog.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()


def display_engine_types(engine_data, max_examples=20):
    if 'modelTypes' in engine_data:
        for idx, engine in enumerate(engine_data['modelTypes'][:max_examples], start=1):
            print(f""\n### Example {idx}: {engine['typeEngineName']} ###"")
            print(f""- Manufacturer: {engine.get('manufacturerName', 'Unknown')}"")
            print(f""- Model: {engine.get('modelName', 'Unknown')}"")
            print(f""- Engine Name: {engine.get('typeEngineName', 'Unknown')}"")
            print(
                f""- Production Date: {engine.get('constructionIntervalStart', 'Unknown')} - {engine.get('constructionIntervalEnd', 'Unknown')}"")
            print(f""- Power: {engine.get('powerKw', 'N/A')} kW / {engine.get('powerPs', 'N/A')} PS"")
            print(f""- Fuel Type: {engine.get('fuelType', 'Unknown')}"")
            print(f""- Body Type: {engine.get('bodyType', 'Unknown')}"")
            print(f""- Number of Cylinders: {engine.get('numberOfCylinders', 'Unknown')}"")
            print(
                f""- Engine Capacity: {engine.get('capacityLt', 'Unknown')} liters ({engine.get('capacityTech', 'Unknown')} cc)"")
            print(f""- Engine Codes: {engine.get('engineCodes', 'Unknown')}"")
            print(""---"")


if __name__ == '__main__':
    # Step 1: Retrieve a list of all available vehicle types
    vehicle_types = list_vehicle_types()
    print(""### Available Vehicle Types ###"")
    if vehicle_types:
        for idx, v_type in enumerate(vehicle_types[:20], start=1):
            print(f""{idx}. Type: {v_type['vehicleType']} - ID: {v_type['id']}"")

    # Step 2: Retrieve detailed information for a specific vehicle type
    # Assuming the first vehicle type from the list for demonstration
    if vehicle_types and len(vehicle_types) > 0:
        vehicle_id = vehicle_types[0]['id']
        manufacturer_id = ""184""  # Example manufacturer ID
        lang_id = ""4""  # English language
        country_filter_id = ""62""  # Example for Germany
        type_id = ""1""  # Type ID for Automobile
        vehicle_details = get_vehicle_type_details(vehicle_id, manufacturer_id, lang_id, country_filter_id, type_id)
        print(""\n### Vehicle Type Details ###"")
        if 'vehicleTypeDetails' in vehicle_details:
            details = vehicle_details['vehicleTypeDetails']
            print(f""- Brand: {details.get('brand', 'Unknown')}"")
            print(f""- Model Type: {details.get('modelType', 'Unknown')}"")
            print(f""- Engine Type: {details.get('typeEngine', 'Unknown')}"")
            print(f""- Production Date: {details.get('constructionIntervalStart', 'Unknown')} - {details.get('constructionIntervalEnd', 'Unknown')}"")
            print(f""- Power: {details.get('powerKw', 'N/A')} kW / {details.get('powerPs', 'N/A')} PS"")
            print(f""- Body Type: {details.get('bodyType', 'Unknown')}"")
            print(f""- Fuel Type: {details.get('fuelType', 'Unknown')}"")
            print(f""- Engine Codes: {details.get('engCodes', 'Unknown')}"")
        else:
            print(""No vehicle details available."")

    # Step 3: Retrieve engine types for a specific model series
    # Example model series parameters for KIA CEE'D Hatchback (ED)
    model_series_id = ""5626""  # Example model series ID
    engine_types = get_vehicle_engine_types(model_series_id, manufacturer_id, lang_id, country_filter_id, type_id)

    # Display the engine details in a formatted way
    display_engine_types(engine_types)
","### Available Vehicle Types ###
1. Type: Automobile - ID: 1
2. Type: Commercial-Vehicles - ID: 2
3. Type: Moto - ID: 3

### Vehicle Type Details ###
- Brand: OPEL
- Model Type: CORSA D (S07)
- Engine Type: 1.4 (L08, L68)
- Production Date: 2006-07-01 - 2014-08-01
- Power: 66.0000 kW / 90.0000 PS
- Body Type: Hatchback
- Fuel Type: Petrol
- Engine Codes: Z 14 XEP

### Example 1: 1.4 CVVT ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.4 CVVT
- Production Date: 2009-08-01 - 2012-12-01
- Power: 66.0000 kW / 90.0000 PS
- Fuel Type: Petrol
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.4000 liters (1396.0000 cc)
- Engine Codes: G4FA
---

### Example 2: 1.6 CVVT ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.6 CVVT
- Production Date: 2010-10-01 - 2012-12-01
- Power: 92.0000 kW / 125.0000 PS
- Fuel Type: Petrol
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.6000 liters (1591.0000 cc)
- Engine Codes: G4FC
---

### Example 3: 1.6 CRDi 128 ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.6 CRDi 128
- Production Date: 2010-10-01 - 2012-12-01
- Power: 94.0000 kW / 128.0000 PS
- Fuel Type: Diesel
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.6000 liters (1582.0000 cc)
- Engine Codes: D4FB
---

### Example 4: 1.6 ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.6
- Production Date: 2007-09-01 - 2012-12-01
- Power: 85.0000 kW / 115.0000 PS
- Fuel Type: Petrol
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.6000 liters (1591.0000 cc)
- Engine Codes: G4FC
---

### Example 5: 1.4 ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.4
- Production Date: 2006-12-01 - 2012-12-01
- Power: 77.0000 kW / 105.0000 PS
- Fuel Type: Petrol
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.4000 liters (1396.0000 cc)
- Engine Codes: G4FA
---

### Example 6: 1.4 ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.4
- Production Date: 2006-12-01 - 2012-12-01
- Power: 80.0000 kW / 109.0000 PS
- Fuel Type: Petrol
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.4000 liters (1396.0000 cc)
- Engine Codes: G4FA
---

### Example 7: 1.6 ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.6
- Production Date: 2006-12-01 - 2012-12-01
- Power: 90.0000 kW / 122.0000 PS
- Fuel Type: Petrol
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.6000 liters (1591.0000 cc)
- Engine Codes: G4FC
---

### Example 8: 2.0 ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 2.0
- Production Date: 2006-12-01 - 2012-12-01
- Power: 105.0000 kW / 143.0000 PS
- Fuel Type: Petrol
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 2.0000 liters (1975.0000 cc)
- Engine Codes: G4GC
---

### Example 9: 1.6 CRDi 90 ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.6 CRDi 90
- Production Date: 2006-12-01 - 2012-12-01
- Power: 66.0000 kW / 90.0000 PS
- Fuel Type: Diesel
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.6000 liters (1582.0000 cc)
- Engine Codes: D4FB
---

### Example 10: 1.6 CRDi 115 ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.6 CRDi 115
- Production Date: 2006-12-01 - 2012-12-01
- Power: 85.0000 kW / 115.0000 PS
- Fuel Type: Diesel
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.6000 liters (1582.0000 cc)
- Engine Codes: D4FB
---

### Example 11: 2.0 CRDi 140 ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 2.0 CRDi 140
- Production Date: 2007-09-01 - 2012-12-01
- Power: 103.0000 kW / 140.0000 PS
- Fuel Type: Diesel
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 2.0000 liters (1991.0000 cc)
- Engine Codes: D4EA
---

### Example 12: 1.6 ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.6
- Production Date: 2008-02-01 - 2012-12-01
- Power: 93.0000 kW / 126.0000 PS
- Fuel Type: Petrol
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.6000 liters (1591.0000 cc)
- Engine Codes: G4FC
---

### Example 13: 2.0 CRDi ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 2.0 CRDi
- Production Date: 2007-06-01 - 2012-12-01
- Power: 100.0000 kW / 136.0000 PS
- Fuel Type: Diesel
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 2.0000 liters (1991.0000 cc)
- Engine Codes: D4EA
---

### Example 14: 1.6 LPG ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.6 LPG
- Production Date: 2007-06-01 - 2011-12-01
- Power: 91.0000 kW / 124.0000 PS
- Fuel Type: Petrol/Liquified Petroleum Gas (LPG)
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.6000 liters (1591.0000 cc)
- Engine Codes: G4FC
---

### Example 15: 1.4 LPG ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.4 LPG
- Production Date: 2009-01-01 - 2010-12-01
- Power: 80.0000 kW / 109.0000 PS
- Fuel Type: Petrol/Liquified Petroleum Gas (LPG)
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.4000 liters (1396.0000 cc)
- Engine Codes: G4FA
---

### Example 16: 1.6 LPG ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.6 LPG
- Production Date: 2011-06-01 - 2011-12-01
- Power: 87.0000 kW / 118.0000 PS
- Fuel Type: Petrol/Liquified Petroleum Gas (LPG)
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.6000 liters (1591.0000 cc)
- Engine Codes: G4FC
---

### Example 17: 1.4 LPG ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.4 LPG
- Production Date: 2011-06-01 - 2012-04-01
- Power: 64.0000 kW / 87.0000 PS
- Fuel Type: Petrol/Liquified Petroleum Gas (LPG)
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.4000 liters (1396.0000 cc)
- Engine Codes: G4FA
---",1,1
"Users can query a list of countries that speak that language with a specific language ID and retrieve related historical events based on a specified year. If no eligible country or historical event is found, the system will kindly inform the user ""No valid country data"" or ""No historical event found for that year."" The service is designed to help users filter important historical information by language and year.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_countries_by_language_id(lang_id):
    url = f""https://auto-parts-catalog.p.rapidapi.com/countries/list-countries-by-lang-id/{lang_id}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""auto-parts-catalog.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def get_historical_events_by_year(year):
    url = ""https://world-history-timeline.p.rapidapi.com/History-By-Year""
    querystring = {""year"": year}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""world-history-timeline.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def display_countries(countries_data, max_display=20):
    if 'countries' in countries_data:
        print(""\n### Countries for Specified Language ID ###"")
        for idx, country in enumerate(countries_data['countries'][:max_display], start=1):
            print(f""{idx}. {country['countryName']} (Code: {country['couCode']})"")
    else:
        print(""No countries found for the specified language ID."")

def display_historical_events(events_data):
    if 'results' in events_data:
        print(""\n### Historical Events ###"")
        for event in events_data['results']:
            print(f""- Year: {event['Year']}"")
            print(f""  Event: {event['Event']}"")
            print(f""  Event ID: {event['id']}"")
            print(""---"")
    else:
        print(""No historical events found for the specified year."")

if __name__ == '__main__':
    # Step 1: Retrieve all countries based on a specified language ID
    lang_id = ""4""  # Example language ID for English
    countries = get_countries_by_language_id(lang_id)
    display_countries(countries)

    # Step 2: Retrieve historical events for a specific year
    year = ""400""  # Example year
    historical_events = get_historical_events_by_year(year)
    display_historical_events(historical_events)
","### Countries for Specified Language ID ###
1. Austria (Code: A)
2. Yemen (People's Democratic Republic) (Code: ADN)
3. Except Europe (Code: AEU)
4. Afghanistan (Code: AFG)
5. Anguilla (Code: AIA)
6. Africa (Code: AK)
7. Albania (Code: AL)
8. Central America (Code: AM)
9. Netherlands Antilles (Code: AN)
10. Andorra (Code: AND)
11. Angola (Code: ANG)
12. Asia / Pacific (Code: APA)
13. Antarctica (Code: AQ)
14. Armenia (Code: ARM)
15. South America (Code: AS)
16. ASEAN (Code: ASE)
17. American Samoa (Code: ASM)
18. Antigua (Code: ATG)
19. Australia (Code: AUS)
20. Aruba (Code: AW)

### Historical Events ###
- Year: 400
  Event: Begins the decline of Maya culture and language in some parts of the highlands.
  Event ID: 215
---",1,1
"I want to create a service that retrieves yarn brands and their details, including brand name, brand ID, number of available yarns, and number of color variations. In addition, the service should support the retrieval of a list of available yarn weights and display each weight category and its corresponding yarn quantity. Users can use the service to browse different yarn brands and filter according to the desired yarn weight to find specific yarn types more efficiently.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_yarn_brands():
    url = ""https://yarn-colorways.p.rapidapi.com/v1/brands""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""yarn-colorways.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def get_yarn_weights():
    url = ""https://yarn-colorways.p.rapidapi.com/v1/weights""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""yarn-colorways.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

if __name__ == '__main__':
    # Step 1: Retrieve a list of yarn brands
    brands_data = get_yarn_brands()
    print(""Yarn Brands:"")
    if brands_data and 'data' in brands_data:
        if brands_data['data']:
            for idx, brand in enumerate(brands_data['data'], start=1):
                print(f""{idx}. Brand: {brand.get('brandName', 'Unknown')} - ID: {brand.get('brandId', 'N/A')} - Yarns: {brand.get('yarns', 0)} - Colorways: {brand.get('colorways', 0)}"")
        else:
            print(""No yarn brand data found."")
    else:
        print(""No yarn brand data found."")

    # Step 2: Retrieve yarn weights for filtering or categorizing yarns
    weights_data = get_yarn_weights()
    print(""\nYarn Weights:"")
    if weights_data and 'data' in weights_data:
        if weights_data['data']:
            for idx, weight in enumerate(weights_data['data'], start=1):
                print(f""{idx}. Weight: {weight.get('name', 'Unknown')} - ID: {weight.get('id', 'N/A')} - Yarns: {weight.get('yarns', 0)}"")
        else:
            print(""No yarn weight data found."")
    else:
        print(""No yarn weight data found."")
",Fail,-1,-1
"Users calculate a new date by adding or subtracting a specified number of business days from a given start date, taking into account holidays and non-working days in a particular country. In addition, a list of non-working days (such as holidays) between two dates in the specified country should be retrieved. Allows users to calculate future or past work dates and learn which dates are considered invalid in a given country.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def calculate_new_date(country_code, start_date, increment):
    url = ""https://working-days.p.rapidapi.com/1.3/add_working_days""
    querystring = {
        ""country_code"": country_code,
        ""start_date"": start_date,
        ""increment"": str(increment)
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""working-days.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def get_non_working_days(country_code, start_date, end_date):
    url = ""https://working-days.p.rapidapi.com/1.3/list_non_working_days""
    querystring = {
        ""country_code"": country_code,
        ""start_date"": start_date,
        ""end_date"": end_date
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""working-days.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def display_new_date(data):
    print(""\n### New Date Calculation ###"")
    end_date = data.get('end_date', 'N/A')
    working_days = data.get('working_days', {})
    public_holidays = data.get('public_holidays', {})

    print(f""- New Date After Increment: {end_date}"")
    print(f""- Total Working Days in Period: {working_days.get('total', 'N/A')}"")
    print(f""- Work Hours in Period: {working_days.get('work_hours', 'N/A')}"")
    print(f""- Public Holidays in Period: {public_holidays.get('total', 'N/A')}"")
    if 'list' in public_holidays:
        for holiday in public_holidays['list']:
            print(f""  - {holiday['date']}: {holiday['description']}"")


def display_non_working_days(data):
    print(""\n### Non-Working Days ###"")
    non_working_days = data.get('non_working_days', [])

    if non_working_days:
        for day in non_working_days:
            date = day.get('date', 'N/A')
            description = day.get('description', 'N/A')
            day_type = day.get('type', 'N/A')
            day_type_desc = ""Weekend"" if day_type == 1 else ""Public Holiday"" if day_type == 2 else ""Special Day""
            print(f""- {date} ({day_type_desc}): {description}"")
    else:
        print(""No non-working days found for the specified period."")


if __name__ == '__main__':
    # Step 1: Calculate a new date by adding 10 working days from the given start date
    country_code = ""CA""
    start_date = ""2023-10-01""
    increment = 10
    new_date_info = calculate_new_date(country_code, start_date, increment)
    display_new_date(new_date_info)

    # Step 2: Retrieve non-working days in Australia between two dates
    country_code = ""AU""
    start_date = ""2023-01-01""
    end_date = ""2023-12-31""
    non_working_days_info = get_non_working_days(country_code, start_date, end_date)
    display_non_working_days(non_working_days_info)
","### New Date Calculation ###
- New Date After Increment: 2023-10-16
- Total Working Days in Period: 10
- Work Hours in Period: 80
- Public Holidays in Period: 1
  - 2023-10-09: Thanksgiving

### Non-Working Days ###
- 2023-01-01 (Special Day): New Year's Day
- 2023-01-02 (Public Holiday): New Year's Day
- 2023-01-07 (Weekend): weekend day
- 2023-01-08 (Weekend): weekend day
- 2023-01-14 (Weekend): weekend day
- 2023-01-15 (Weekend): weekend day
- 2023-01-21 (Weekend): weekend day
- 2023-01-22 (Weekend): weekend day
- 2023-01-26 (Public Holiday): Australia Day
- 2023-01-28 (Weekend): weekend day
- 2023-01-29 (Weekend): weekend day
- 2023-02-04 (Weekend): weekend day
- 2023-02-05 (Weekend): weekend day
- 2023-02-11 (Weekend): weekend day
- 2023-02-12 (Weekend): weekend day
- 2023-02-18 (Weekend): weekend day
- 2023-02-19 (Weekend): weekend day
- 2023-02-25 (Weekend): weekend day
- 2023-02-26 (Weekend): weekend day
- 2023-03-04 (Weekend): weekend day
- 2023-03-05 (Weekend): weekend day
- 2023-03-11 (Weekend): weekend day
- 2023-03-12 (Weekend): weekend day
- 2023-03-13 (Public Holiday): Canberra Day
- 2023-03-18 (Weekend): weekend day
- 2023-03-19 (Weekend): weekend day
- 2023-03-25 (Weekend): weekend day
- 2023-03-26 (Weekend): weekend day
- 2023-04-01 (Weekend): weekend day
- 2023-04-02 (Weekend): weekend day
- 2023-04-07 (Public Holiday): Good Friday
- 2023-04-08 (Special Day): Easter Saturday
- 2023-04-09 (Special Day): Easter Sunday
- 2023-04-10 (Public Holiday): Easter Monday
- 2023-04-15 (Weekend): weekend day
- 2023-04-16 (Weekend): weekend day
- 2023-04-22 (Weekend): weekend day
- 2023-04-23 (Weekend): weekend day
- 2023-04-25 (Public Holiday): Anzac Day
- 2023-04-29 (Weekend): weekend day
- 2023-04-30 (Weekend): weekend day
- 2023-05-06 (Weekend): weekend day
- 2023-05-07 (Weekend): weekend day
- 2023-05-13 (Weekend): weekend day
- 2023-05-14 (Weekend): weekend day
- 2023-05-20 (Weekend): weekend day
- 2023-05-21 (Weekend): weekend day
- 2023-05-27 (Weekend): weekend day
- 2023-05-28 (Weekend): weekend day
- 2023-05-29 (Public Holiday): Reconciliation Day
- 2023-06-03 (Weekend): weekend day
- 2023-06-04 (Weekend): weekend day
- 2023-06-10 (Weekend): weekend day
- 2023-06-11 (Weekend): weekend day
- 2023-06-12 (Public Holiday): King's Birthday
- 2023-06-17 (Weekend): weekend day
- 2023-06-18 (Weekend): weekend day
- 2023-06-24 (Weekend): weekend day
- 2023-06-25 (Weekend): weekend day
- 2023-07-01 (Weekend): weekend day
- 2023-07-02 (Weekend): weekend day
- 2023-07-08 (Weekend): weekend day
- 2023-07-09 (Weekend): weekend day
- 2023-07-15 (Weekend): weekend day
- 2023-07-16 (Weekend): weekend day
- 2023-07-22 (Weekend): weekend day
- 2023-07-23 (Weekend): weekend day
- 2023-07-29 (Weekend): weekend day
- 2023-07-30 (Weekend): weekend day
- 2023-08-05 (Weekend): weekend day
- 2023-08-06 (Weekend): weekend day
- 2023-08-12 (Weekend): weekend day
- 2023-08-13 (Weekend): weekend day
- 2023-08-19 (Weekend): weekend day
- 2023-08-20 (Weekend): weekend day
- 2023-08-26 (Weekend): weekend day
- 2023-08-27 (Weekend): weekend day
- 2023-09-02 (Weekend): weekend day
- 2023-09-03 (Weekend): weekend day
- 2023-09-09 (Weekend): weekend day
- 2023-09-10 (Weekend): weekend day
- 2023-09-16 (Weekend): weekend day
- 2023-09-17 (Weekend): weekend day
- 2023-09-23 (Weekend): weekend day
- 2023-09-24 (Weekend): weekend day
- 2023-09-30 (Weekend): weekend day
- 2023-10-01 (Weekend): weekend day
- 2023-10-02 (Public Holiday): Labour Day
- 2023-10-07 (Weekend): weekend day
- 2023-10-08 (Weekend): weekend day
- 2023-10-14 (Weekend): weekend day
- 2023-10-15 (Weekend): weekend day
- 2023-10-21 (Weekend): weekend day
- 2023-10-22 (Weekend): weekend day
- 2023-10-28 (Weekend): weekend day
- 2023-10-29 (Weekend): weekend day
- 2023-11-04 (Weekend): weekend day
- 2023-11-05 (Weekend): weekend day
- 2023-11-11 (Weekend): weekend day
- 2023-11-12 (Weekend): weekend day
- 2023-11-18 (Weekend): weekend day
- 2023-11-19 (Weekend): weekend day
- 2023-11-25 (Weekend): weekend day
- 2023-11-26 (Weekend): weekend day
- 2023-12-02 (Weekend): weekend day
- 2023-12-03 (Weekend): weekend day
- 2023-12-09 (Weekend): weekend day
- 2023-12-10 (Weekend): weekend day
- 2023-12-16 (Weekend): weekend day
- 2023-12-17 (Weekend): weekend day
- 2023-12-23 (Weekend): weekend day
- 2023-12-24 (Weekend): weekend day
- 2023-12-25 (Public Holiday): Christmas Day
- 2023-12-26 (Public Holiday): Boxing Day
- 2023-12-30 (Weekend): weekend day
- 2023-12-31 (Weekend): weekend day",1,1
"Helps users analyze a specified date range for a given country, retrieving detailed breakdowns of dates, weekdays, weekends, public holidays and custom dates. Also include the ability to calculate a new date by adding or subtracting a specified number of working days, taking into account holidays in specific countries. In addition, users should be allowed to obtain details of any particular date, including whether that date is a weekday, public holiday, weekend, or custom date, as well as the corresponding working hours, salary, and remote working details.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def analyse_date_range(start_date, end_date, country_code):
    url = ""https://working-days.p.rapidapi.com/1.3/analyse""
    querystring = {
        ""start_date"": start_date,
        ""end_date"": end_date,
        ""country_code"": country_code
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""working-days.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def calculate_new_date(country_code, start_date, increment):
    url = ""https://working-days.p.rapidapi.com/1.3/add_working_days""
    querystring = {
        ""country_code"": country_code,
        ""start_date"": start_date,
        ""increment"": str(increment)
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""working-days.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def get_day_info(country_code, date):
    url = ""https://working-days.p.rapidapi.com/1.3/get_info_day""
    querystring = {
        ""country_code"": country_code,
        ""date"": date
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""working-days.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def display_date_analysis(data):
    print(""\n### Date Range Analysis ###"")
    days = data.get('days', {})
    working_days = data.get('working_days', {})
    weekend_days = data.get('weekend_days', {})
    public_holidays = data.get('public_holidays', {})

    print(""\n** Days Overview **"")
    print(f""- Total Days: {days.get('total', 'N/A')}"")
    print(f""- Mondays: {days.get('mondays', 'N/A')}"")
    print(f""- Tuesdays: {days.get('tuesdays', 'N/A')}"")
    print(f""- Wednesdays: {days.get('wednesdays', 'N/A')}"")
    print(f""- Thursdays: {days.get('thursdays', 'N/A')}"")
    print(f""- Fridays: {days.get('fridays', 'N/A')}"")
    print(f""- Saturdays: {days.get('saturdays', 'N/A')}"")
    print(f""- Sundays: {days.get('sundays', 'N/A')}"")

    print(""\n** Working Days Overview **"")
    print(f""- Total Working Days: {working_days.get('total', 'N/A')}"")
    print(f""- Work Hours: {working_days.get('work_hours', 'N/A')}"")
    print(f""- Wages: ${working_days.get('wages', 'N/A')}"")

    print(""\n** Public Holidays **"")
    print(f""- Total Public Holidays: {public_holidays.get('total', 'N/A')}"")
    if 'list' in public_holidays:
        for holiday in public_holidays['list']:
            print(f""  - {holiday['date']}: {holiday['description']}"")


def display_new_date(data):
    print(""\n### New Date Calculation ###"")
    end_date = data.get('end_date', 'N/A')
    working_days = data.get('working_days', {})

    print(f""- New Date After Increment: {end_date}"")
    print(f""- Total Working Days in Period: {working_days.get('total', 'N/A')}"")
    print(f""- Work Hours in Period: {working_days.get('work_hours', 'N/A')}"")


def display_day_info(data):
    print(""\n### Specific Day Information ###"")
    print(f""- Working Day: {'Yes' if data.get('working_day', 0) == 1 else 'No'}"")
    print(f""- Public Holiday: {'Yes' if data.get('public_holiday', '0') == '1' else 'No'}"")
    if data.get('public_holiday', '0') == '1':
        print(f""  - Holiday Description: {data.get('public_holiday_description', 'N/A')}"")
    print(f""- Weekend Day: {'Yes' if data.get('weekend_day', 0) == 1 else 'No'}"")


if __name__ == '__main__':
    # Step 1: Analyze a date range for detailed breakdowns
    start_date = ""2023-01-01""
    end_date = ""2023-12-31""
    country_code = ""US""
    analysis = analyse_date_range(start_date, end_date, country_code)
    display_date_analysis(analysis)

    # Step 2: Calculate a new date by adding 5 working days from the given start date
    start_date = ""2023-10-01""
    increment = 5
    new_date = calculate_new_date(country_code, start_date, increment)
    display_new_date(new_date)

    # Step 3: Get detailed information about a specific date
    specific_date = ""2023-12-25""
    day_info = get_day_info(country_code, specific_date)
    display_day_info(day_info)
","### Date Range Analysis ###

** Days Overview **
- Total Days: 365
- Mondays: 52
- Tuesdays: 52
- Wednesdays: 52
- Thursdays: 52
- Fridays: 52
- Saturdays: 52
- Sundays: 53

** Working Days Overview **
- Total Working Days: 249
- Work Hours: 1992
- Wages: $39840

** Public Holidays **
- Total Public Holidays: 13
  - 2023-01-01: New Year's Day
  - 2023-01-02: New Year's Day (observance)
  - 2023-01-16: Martin Luther King Day
  - 2023-02-20: Washington's Birthday
  - 2023-05-29: Memorial Day
  - 2023-06-19: Juneteenth National Independence Day
  - 2023-07-04: Independence Day
  - 2023-09-04: Labor Day
  - 2023-10-09: Columbus Day
  - 2023-11-10: Veterans Day (observance)
  - 2023-11-11: Veterans Day
  - 2023-11-23: Thanksgiving
  - 2023-12-25: Christmas

### New Date Calculation ###
- New Date After Increment: N/A
- Total Working Days in Period: N/A
- Work Hours in Period: N/A

### Specific Day Information ###
- Working Day: No
- Public Holiday: No
- Weekend Day: No",1,1
"I want to retrieve real-time stock quotes from multiple tickers, focusing on key trade indicators such as high, low, volume, and last trade data. Details of dividend-paying stocks should also be provided, including trend stocks with high dividend yields, recent dividend increases, and stocks that are going ex-dividend. This gives investors a comprehensive view of stock performance and dividend trends, helping them make informed investment decisions.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_realtime_quotes(sa_ids):
    url = ""https://seeking-alpha.p.rapidapi.com/market/get-realtime-quotes""
    querystring = {
        ""sa_ids"": sa_ids  # Comma-separated list of IDs for stock tickers
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""seeking-alpha.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_dividend_investing_info():
    url = ""https://seeking-alpha.p.rapidapi.com/market/get-dividend-investing""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""seeking-alpha.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def display_realtime_quotes(data):
    print(""\n### Real-Time Stock Quotes ###"")
    quotes = data.get('real_time_quotes', [])
    if quotes:
        for idx, quote in enumerate(quotes, start=1):
            print(f""\nStock {idx}:"")
            print(f""- Symbol: {quote.get('symbol', 'N/A')}"")
            print(f""- High: {quote.get('high', 'N/A')}"")
            print(f""- Low: {quote.get('low', 'N/A')}"")
            print(f""- Open: {quote.get('open', 'N/A')}"")
            print(f""- Close: {quote.get('close', 'N/A')}"")
            print(f""- Last: {quote.get('last', 'N/A')}"")
            print(f""- Volume: {quote.get('volume', 'N/A')}"")
            print(f""- Market Info: {quote.get('info', 'N/A')}"")
            print(f""- Last Updated: {quote.get('updated_at', 'N/A')}"")
    else:
        print(""No real-time quotes found."")

def display_dividend_info(data):
    print(""\n### Dividend Investing Information ###"")
    attributes = data.get('data', {}).get('attributes', {})

    # Display Trending Dividend Stocks
    trending_stocks = attributes.get('trending_dividend_stocks', [])
    if trending_stocks:
        print(""\n** Trending Dividend Stocks **"")
        for stock in trending_stocks:
            print(f""- Name: {stock.get('name', 'N/A')}"")
            print(f""  - Dividend Yield (Forward): {stock.get('div_yield_fwd', 'N/A')}%"")

    # Display Dividend Increases
    dividend_increases = attributes.get('dividend_increases', [])
    if dividend_increases:
        print(""\n** Dividend Increases **"")
        for increase in dividend_increases:
            print(f""- {increase.get('name', 'N/A')}"")

    # Display Upcoming Ex-Dates
    upcoming_exdates = attributes.get('upcoming_exdates', [])
    if upcoming_exdates:
        print(""\n** Upcoming Ex-Dates **"")
        for exdate in upcoming_exdates:
            print(f""- {exdate.get('name', 'N/A')}"")
            print(f""  - Ex-Date: {exdate.get('date', 'N/A')}"")

if __name__ == '__main__':
    # Step 1: Retrieve real-time quotes for specified tickers
    ticker_ids = ""612888,16123""  # Example ticker IDs, replace with desired IDs
    realtime_quotes = get_realtime_quotes(ticker_ids)
    display_realtime_quotes(realtime_quotes)

    # Step 2: Retrieve information on dividend-paying stocks
    dividend_info = get_dividend_investing_info()
    display_dividend_info(dividend_info)
",Fail,-1,-1
"Please provide an overall view of the daily activity in the stock market, highlighting the biggest winners, losers, most active stocks and noteworthy sectors. Includes metadata on specific financial instruments and companies, sorted by index, and covers global markets. In addition, an integrated news feed for the latest market news provides users with current updates on financial trends and events affecting their investments.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def get_market_day_watch():
    url = ""https://seeking-alpha.p.rapidapi.com/market/get-day-watch""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""seeking-alpha.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()


def get_equity_metadata(filter_category):
    url = ""https://seeking-alpha.p.rapidapi.com/market/get-equity""
    querystring = {
        ""filterCategory"": filter_category  # e.g., 'global-equity'
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""seeking-alpha.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def get_hot_news(limit=10):
    url = ""https://yahoo-finance-api-data.p.rapidapi.com/news/hot-news""
    querystring = {
        ""limit"": str(limit)  # Number of news notifications to retrieve
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""yahoo-finance-api-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def display_market_day_watch(data):
    print(""\n### Market Day Watch Overview ###"")
    if 'data' in data and 'attributes' in data['data']:
        attributes = data['data']['attributes']

        # Display Top Gainers
        print(""\n** Top Gainers **"")
        for item in attributes.get('top_gainers', []):
            print(f""- {item.get('name', 'N/A')} (Slug: {item.get('slug', 'N/A')})"")

        # Display Top Losers
        print(""\n** Top Losers **"")
        for item in attributes.get('top_losers', []):
            print(f""- {item.get('name', 'N/A')} (Slug: {item.get('slug', 'N/A')})"")

        # Display Cryptocurrencies
        print(""\n** Cryptocurrencies **"")
        for item in attributes.get('cryptocurrencies', []):
            print(f""- {item.get('name', 'N/A')} (Slug: {item.get('slug', 'N/A')})"")

        # Display Most Active Stocks
        print(""\n** Most Active Stocks **"")
        for item in attributes.get('most_active', []):
            print(f""- {item.get('name', 'N/A')} (Slug: {item.get('slug', 'N/A')})"")
    else:
        print(""No market day watch data found."")


def display_equity_metadata(data):
    print(""\n### Equity Metadata for Global Markets ###"")
    if 'data' in data:
        for idx, item in enumerate(data['data'], start=1):
            attributes = item.get('attributes', {})
            print(f""\nEquity {idx}:"")
            print(f""- Name: {attributes.get('name', 'N/A')}"")
            print(f""- Company: {attributes.get('company', 'N/A')}"")
            print(f""- Alias: {attributes.get('alias_name', 'N/A')}"")
            print(f""- Tag Kind: {attributes.get('tagKind', 'N/A')}"")
    else:
        print(""No equity metadata found."")


def display_hot_news(data):
    print(""\n### Latest Financial Market News ###"")
    if data.get('success') and 'data' in data:
        for idx, news in enumerate(data['data'], start=1):
            print(f""\nNews {idx}:"")
            print(f""- Title: {news.get('notificationTitle', 'N/A')}"")
            print(f""- Published Date: {news.get('publishTs', 'N/A')}"")
            print(f""- Link: {news.get('articleUrl', 'N/A')}"")
    else:
        print(""No hot news found."")


if __name__ == '__main__':
    # Step 1: Retrieve market day watch data for top movers and active stocks
    market_day_watch = get_market_day_watch()
    display_market_day_watch(market_day_watch)

    # Step 2: Retrieve equity metadata based on the global equity filter
    equity_metadata = get_equity_metadata(""global-equity"")
    display_equity_metadata(equity_metadata)

    # Step 3: Retrieve the latest hot news for the financial market
    hot_news = get_hot_news(limit=3)
    display_hot_news(hot_news)
","### Market Day Watch Overview ###

** Top Gainers **
- ImmunityBio, Inc. (Slug: IBRX)
- Newell Brands Inc. (Slug: NWL)
- Daqo New Energy Corp. (Slug: DQ)
- Spirit Airlines, Inc. (Slug: SAVE)
- Tapestry, Inc. (Slug: TPR)
- Canopy Growth Corporation (Slug: CGC)
- Saia, Inc. (Slug: SAIA)
- Integral Ad Science Holding Corp. (Slug: IAS)
- AppFolio, Inc. (Slug: APPF)
- Deckers Outdoor Corporation (Slug: DECK)

** Top Losers **
- Capri Holdings Limited (Slug: CPRI)
- Evolv Technologies Holdings, Inc. (Slug: EVLV)
- Tyra Biosciences, Inc. (Slug: TYRA)
- World Kinect Corporation (Slug: WKC)
- Joby Aviation, Inc. (Slug: JOBY)
- The Bancorp, Inc. (Slug: TBBK)
- Mohawk Industries, Inc. (Slug: MHK)
- Carter's, Inc. (Slug: CRI)
- TriNet Group, Inc. (Slug: TNET)
- Universal Logistics Holdings, Inc. (Slug: ULH)

** Cryptocurrencies **
- Bitcoin USD (Slug: BTC-USD)
- Ethereum USD (Slug: ETH-USD)
- Binance Coin USD (Slug: BNB-USD)
- Ripple USD (Slug: XRP-USD)
- Cardano USD (Slug: ADA-USD)

** Most Active Stocks **
- NVIDIA Corporation (Slug: NVDA)
- Tesla, Inc. (Slug: TSLA)
- Joby Aviation, Inc. (Slug: JOBY)
- ImmunityBio, Inc. (Slug: IBRX)
- NIO Inc. (Slug: NIO)
- Vale S.A. (Slug: VALE)
- MARA Holdings, Inc. (Slug: MARA)
- SoFi Technologies, Inc. (Slug: SOFI)
- Intel Corporation (Slug: INTC)
- Capri Holdings Limited (Slug: CPRI)

### Equity Metadata for Global Markets ###

Equity 1:
- Name: VT
- Company: Vanguard Total World Stock Index Fund ETF Shares
- Alias: World Index
- Tag Kind: Tags::Ticker

Equity 2:
- Name: VXUS
- Company: Vanguard Total International Stock Index Fund ETF Shares
- Alias: World Ex-US
- Tag Kind: Tags::Ticker

Equity 3:
- Name: VWO
- Company: Vanguard FTSE Emerging Markets Index Fund ETF Shares
- Alias: Emerging Markets
- Tag Kind: Tags::Ticker

Equity 4:
- Name: VEA
- Company: Vanguard FTSE Developed Markets Index Fund ETF Shares
- Alias: Developed Markets
- Tag Kind: Tags::Ticker

Equity 5:
- Name: VGK
- Company: Vanguard FTSE Europe ETF
- Alias: Europe
- Tag Kind: Tags::Ticker

### Latest Financial Market News ###

News 1:
- Title: First-time homebuyers hold off on purchases amid 'slim' pickings, high mortgage rates
- Published Date: 1729951202000
- Link: https://finance.yahoo.com/news/first-time-homebuyers-hold-off-on-purchases-amid-slim-pickings-high-mortgage-rates-120053583.html

News 2:
- Title: Chart of the Week: Taking a closer look at the Magnificent Seven
- Published Date: 1729942324000
- Link: https://finance.yahoo.com/news/chart-of-the-week-taking-a-closer-look-at-the-magnificent-seven-100014792.html

News 3:
- Title: Walmart maintains the upper hand as it battles Target for Americans' wallets
- Published Date: 1729902611000
- Link: https://finance.yahoo.com/news/walmart-maintains-the-upper-hand-as-it-battles-target-for-americans-wallets-223503946.html

News 4:
- Title: Biden rolls out another student debt relief plan, targeting borrowers at risk of default
- Published Date: 1729898113000
- Link: https://finance.yahoo.com/news/biden-rolls-out-another-student-debt-relief-plan-this-time-targeting-borrowers-at-risk-of-default-190544187.html

News 5:
- Title: Tesla is still a car company â€” and for now, that's OK
- Published Date: 1729893602000
- Link: https://finance.yahoo.com/news/tesla-is-still-a-car-company--and-for-now-thats-ok-morning-brief-100047251.html

News 6:
- Title: Microsoft tells shareholders to reject call to invest corporate cash in bitcoin (Fortune)
- Published Date: 1729892293000
- Link: https://finance.yahoo.com/news/microsoft-tells-shareholders-reject-call-190230517.html

News 7:
- Title: Nasdaq closes near record high, ending rough week for stocks
- Published Date: 1729886902000
- Link: https://finance.yahoo.com/news/live/stock-market-today-nasdaq-closes-near-record-high-amid-rough-week-for-stocks-133121560.html

News 8:
- Title: Meta Platforms to use Reuters news content in AI chatbot
- Published Date: 1729877926000
- Link: https://finance.yahoo.com/news/meta-platforms-reuters-news-content-172713675.html

News 9:
- Title: Nvidia overtakes Apple as world's most valuable company
- Published Date: 1729872031000
- Link: https://finance.yahoo.com/news/nvidia-overtakes-apple-worlds-most-155501232.html

News 10:
- Title: Alphabet's Waymo closes $5.6 billion funding to expand autonomous ride-hailing service
- Published Date: 1729870907000
- Link: https://finance.yahoo.com/news/alphabets-waymo-closes-5-6-153151057.html

News 11:
- Title: US nuclear regulator hears Three Mile Island power plant restart plan
- Published Date: 1729870687000
- Link: https://finance.yahoo.com/news/us-nuclear-regulator-hears-three-150627577.html

News 12:
- Title: Elon Musk might be hurting Donald Trump more than helping, Yahoo Finance survey shows
- Published Date: 1729869523000
- Link: https://finance.yahoo.com/news/elon-musk-might-be-hurting-donald-trump-more-than-helping-yf-survey-131637252.html

News 13:
- Title: A new rule could make it easier to switch your bank. (Guess who hates it.)
- Published Date: 1729868410000
- Link: https://finance.yahoo.com/news/a-new-rule-could-make-it-easier-to-switch-your-bank-guess-who-hates-it-090014454.html

News 14:
- Title: Nasdaq leads stocks higher as Treasury yields ease, but weekly losses loom
- Published Date: 1729863240000
- Link: https://finance.yahoo.com/news/live/stock-market-today-us-futures-inch-up-as-treasury-yields-ease-but-weekly-losses-loom-112437103.html

News 15:
- Title: Companies wrestle with when to merge as they assess Harris and Trump
- Published Date: 1729855801000
- Link: https://finance.yahoo.com/news/companies-wrestle-with-when-to-merge-as-they-assess-harris-and-trump-080004995.html

News 16:
- Title: Nvidiaâ€™s billionaire CEO says AI can do a lot of thingsâ€”except take his job (Fortune)
- Published Date: 1729854418000
- Link: https://finance.yahoo.com/news/nvidia-billionaire-ceo-says-ai-082900997.html

News 17:
- Title: Tesla is still a car company â€” and for now, that's okay
- Published Date: 1729850932000
- Link: https://finance.yahoo.com/news/tesla-is-still-a-car-company--and-for-now-thats-okay-morning-brief-100047386.html

News 18:
- Title: Microsoft CEO Nadella's 2024 pay rose 63% to $79 million on stock awards
- Published Date: 1729846804000
- Link: https://finance.yahoo.com/news/microsoft-ceo-nadellas-2024-pay-232937504.html

News 19:
- Title: Microsoft and Salesforce clash as autonomous agent race heats up
- Published Date: 1729816206000
- Link: https://finance.yahoo.com/news/microsoft-and-salesforce-clash-as-autonomous-agent-race-heats-up-100056224.html

News 20:
- Title: Wall Streetâ€™s trading desks are having a great year. The election could keep that going.
- Published Date: 1729811704000
- Link: https://finance.yahoo.com/news/wall-streets-trading-desks-are-having-a-great-year-the-election-could-keep-that-going-080001141.html
",1,1
"I need a service that aggregates and presents financial market news and data from a variety of sources. The service should offer the latest articles and videos related to specific stock symbols, grouped by region, and allow users to access specific market stories focused on currencies. The goal is to keep users informed of global market conditions, currency trends and stock-specific news through a unified interface, with the option to filter by region and specific ticker symbol.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def get_news_by_symbol(symbol, limit=5):
    url = ""https://yahoo-finance-api-data.p.rapidapi.com/news/list""
    querystring = {
        ""symbol"": symbol,  # Stock symbol (e.g., 'AAPL')
        ""limit"": str(limit)  # Maximum number of news items to return
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""yahoo-finance-api-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def display_news(data, symbol):
    print(f""\n### News Articles for Symbol '{symbol}' ###"")
    if data.get('success') and data.get('data') and data['data'].get('main') and data['data']['main'].get('stream'):
        for idx, news_item in enumerate(data['data']['main']['stream'], start=1):
            content = news_item.get('content', {})
            # å®‰å…¨æ£€æŸ¥ clickThroughUrl æ˜¯å¦ä¸º None
            link = content.get('clickThroughUrl', {})
            link_url = link.get('url', 'N/A') if link else 'N/A'

            print(f""\nNews {idx}:"")
            print(f""- Title: {content.get('title', 'N/A')}"")
            print(f""- Published Date: {content.get('pubDate', 'N/A')}"")
            print(f""- Source: {content.get('provider', {}).get('displayName', 'N/A')}"")
            print(f""- Link: {link_url}"")
    else:
        print(""No news found."")
    print(""---"")


if __name__ == '__main__':
    # Step 1: Retrieve news articles related to a specific stock symbol (e.g., AAPL)
    symbol_news = get_news_by_symbol(""AAPL"", limit=5)
    display_news(symbol_news, ""AAPL"")
","### News Articles for Symbol 'AAPL' ###

News 1:
- Title: Chart of the Week: Taking a closer look at the Magnificent Seven
- Published Date: 2024-10-26T10:00:14Z
- Source: Yahoo Finance
- Link: https://finance.yahoo.com/news/chart-of-the-week-taking-a-closer-look-at-the-magnificent-seven-100014792.html

News 2:
- Title: PCE data, Big Tech earnings: What to Watch
- Published Date: 2024-10-25T21:52:50Z
- Source: Yahoo Finance Video
- Link: https://finance.yahoo.com/video/pce-data-big-tech-earnings-215250140.html

News 3:
- Title: 3 Warren Buffett Stocks That Are Screaming Buys Right Now
- Published Date: 2024-10-26T12:00:00Z
- Source: Motley Fool
- Link: https://finance.yahoo.com/news/3-warren-buffett-stocks-screaming-120000652.html

News 4:
- Title: Meet the Ultra-Low-Cost Vanguard ETF That Has 33% of Its Holdings in Nvidia, Apple, and Microsoft
- Published Date: 2024-10-26T08:33:00Z
- Source: Motley Fool
- Link: https://finance.yahoo.com/news/meet-ultra-low-cost-vanguard-083300377.html

News 5:
- Title: Should You Buy Stocks at a Record High? Here's What Warren Buffett Is Doing.
- Published Date: 2024-10-26T07:15:00Z
- Source: Motley Fool
- Link: https://finance.yahoo.com/news/buy-stocks-record-high-heres-071500022.html

News 6:
- Title: Jim Cramer on Apple Inc. (AAPL)â€™s iPhone Sales: â€˜Not Enough To Make Me Wanna Sell, But Enough To Ding The Stockâ€™
- Published Date: 2024-10-25T23:35:04Z
- Source: Insider Monkey
- Link: https://finance.yahoo.com/news/jim-cramer-apple-inc-aapl-233504574.html

News 7:
- Title: Jury awards Apple $250 in design patent lawsuit against Masimo
- Published Date: 2024-10-25T23:08:24Z
- Source: CNN Business
- Link: https://finance.yahoo.com/news/jury-awards-apple-250-design-230824006.html

News 8:
- Title: Between Mag 7 earnings and jobs data, next week is 'the Super Bowl for investors'
- Published Date: 2024-10-25T21:30:00Z
- Source: Yahoo Finance Video
- Link: https://finance.yahoo.com/video/between-mag-7-earnings-jobs-213000281.html

News 9:
- Title: Stock Market Action Plan: Apple, Alphabet, Microsoft Wrap Up October
- Published Date: 2024-10-25T21:23:06Z
- Source: Investor's Business Daily
- Link: N/A

News 10:
- Title: Apple Inc. (AAPL) Unveils iOS 18.2 Beta with AI Features, Including ChatGPT Integration, Boosting Investor Confidence
- Published Date: 2024-10-25T20:40:21Z
- Source: Insider Monkey
- Link: https://finance.yahoo.com/news/apple-inc-aapl-unveils-ios-204021321.html

News 11:
- Title: Five Stocks, 3 Days, $12 Trillion In Market Cap. Buckle Up For Huge Earnings.
- Published Date: 2024-10-25T20:29:32Z
- Source: Investor's Business Daily
- Link: N/A

News 12:
- Title: Apple Stock Downgraded To Sell Ahead Of Earnings. Here's Why.
- Published Date: 2024-10-25T20:18:05Z
- Source: Investor's Business Daily
- Link: N/A

News 13:
- Title: Apple Remains King And Finds Support. But Nvidia Plots A Coup.
- Published Date: 2024-10-25T20:00:21Z
- Source: Investor's Business Daily
- Link: N/A

News 14:
- Title: Apple Dips After KeyBanc Downgrade on iPhone SE Sales Concerns
- Published Date: 2024-10-25T19:49:30Z
- Source: GuruFocus.com
- Link: https://finance.yahoo.com/news/apple-dips-keybanc-downgrade-iphone-194930288.html

News 15:
- Title: Sector Update: Tech Stocks Gain Late Afternoon
- Published Date: 2024-10-25T19:47:56Z
- Source: MT Newswires
- Link: https://finance.yahoo.com/news/sector-tech-stocks-gain-afternoon-194756326.html

News 16:
- Title: Apple Trials Prediabetes App, Eyes Future in Glucose Monitoring Tech
- Published Date: 2024-10-25T19:21:07Z
- Source: GuruFocus.com
- Link: https://finance.yahoo.com/news/apple-trials-prediabetes-app-eyes-192107959.html

News 17:
- Title: Apple Returns to Top 5 in China Smartphone Market in Q3, IDC Report Says
- Published Date: 2024-10-25T17:53:12Z
- Source: GuruFocus.com
- Link: https://finance.yahoo.com/news/apple-returns-top-5-china-175312711.html

News 18:
- Title: Lloyds suffers Â£3bn hit after car finance court ruling
- Published Date: 2024-10-25T17:46:32Z
- Source: The Telegraph
- Link: https://finance.yahoo.com/news/rachel-reeves-admits-working-people-174632036.html

News 19:
- Title: 3 Must-See Earnings Reports Next Week (GOOGL, META, AAPL)
- Published Date: 2024-10-25T17:42:00Z
- Source: Zacks
- Link: https://finance.yahoo.com/news/3-must-see-earnings-reports-174200403.html

News 20:
- Title: Apple Tests Blood-Sugar App in Sign of Its Health Ambitions
- Published Date: 2024-10-25T17:16:02Z
- Source: Bloomberg
- Link: https://finance.yahoo.com/news/apple-tests-blood-sugar-app-171602061.html
---
",1,1
"Users need to search for securities by name or keyword, retrieve detailed information about related news articles, and obtain comprehensive metadata for specific news items. Take advantage of autocomplete to assist in finding relevant securities and provide the ability to access in-depth content details of selected news articles. The goal is to enable users to conduct detailed investigations of security-related news, explore the metadata of articles, and identify stock-specific data through a user-friendly interface.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_auto_complete_suggestions(query):
    url = ""https://bb-finance.p.rapidapi.com/market/auto-complete""
    querystring = {
        ""query"": query  # Search term or phrase (e.g., 'apple')
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""bb-finance.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_news_detail(news_id):
    url = ""https://yahoo-finance-api-data.p.rapidapi.com/news/detail""
    querystring = {
        ""id"": news_id  # Unique identifier of the news article
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""yahoo-finance-api-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def display_auto_complete_suggestions(data, query, limit=5):
    print(f""\n### Auto-complete Suggestions for '{query}' ###"")
    if 'quote' in data:
        for idx, item in enumerate(data['quote'][:limit], start=1):
            print(f""\nSuggestion {idx}:"")
            print(f""- Name: {item.get('name', 'N/A')}"")
            print(f""- Symbol: {item.get('symbol', 'N/A')}"")
            print(f""- Exchange: {item.get('exchange', 'N/A')}"")
            print(f""- Country: {item.get('country', 'N/A')}"")
            print(f""- Currency: {item.get('currency', 'N/A')}"")
            print(f""- Security Type: {item.get('securityType', 'N/A')}"")
    else:
        print(""No suggestions found."")
    print(""---"")

def display_news_details(data, news_id):
    print(f""\n### News Details for ID '{news_id}' ###"")
    if data.get('success'):
        news_item = data.get('result', {})
        print(f""- Title: {news_item.get('title', 'N/A')}"")
        print(f""- Source: {news_item.get('source', 'N/A')}"")
        print(f""- Published Date: {news_item.get('published_date', 'N/A')}"")
        print(f""- Summary: {news_item.get('summary', 'N/A')}"")
        print(f""- Content: {news_item.get('content', 'N/A')}"")
    else:
        print(""No news details found."")
    print(""---"")

if __name__ == '__main__':
    # Step 1: Use auto-complete to search for securities related to a query (e.g., 'apple')
    search_query = ""apple""
    auto_complete_data = get_auto_complete_suggestions(search_query)
    display_auto_complete_suggestions(auto_complete_data, search_query, limit=5)  # Limit to 5 suggestions

    # Step 2: Assume a news item ID is retrieved from auto-complete results, fetch detailed news info
    example_news_id = ""4cd7395e-588c-4a64-a33b-64371b30dc95""
    news_detail = get_news_detail(example_news_id)
    display_news_details(news_detail, example_news_id)
","### Auto-complete Suggestions for 'apple' ###

Suggestion 1:
- Name: Apple Inc
- Symbol: AAPL
- Exchange: NASDAQ GS
- Country: United States
- Currency: USD
- Security Type: Common Stock

Suggestion 2:
- Name: Apple Hospitality REIT Inc
- Symbol: APLE
- Exchange: New York
- Country: United States
- Currency: USD
- Security Type: REIT

Suggestion 3:
- Name: Apple International Co Ltd
- Symbol: 2788
- Exchange: Tokyo
- Country: Japan
- Currency: JPY
- Security Type: Common Stock

Suggestion 4:
- Name: Apple Green Holding Inc
- Symbol: AGPL
- Exchange: N/A
- Country: Malaysia
- Currency: USD
- Security Type: Common Stock

Suggestion 5:
- Name: Apple Flavor & Fragrance Group Co Ltd
- Symbol: 603020
- Exchange: Shanghai
- Country: China
- Currency: CNY
- Security Type: Common Stock
---

### News Details for ID '4cd7395e-588c-4a64-a33b-64371b30dc95' ###
- Title: N/A
- Source: N/A
- Published Date: N/A
- Summary: N/A
- Content: N/A
---",1,0
"Users need to obtain real-time exchange rates for multiple currencies relative to the specified base currency and convert from one currency to another based on the latest exchange rate. First retrieve the real-time exchange rate for a set of selected currencies using the base currency, and then use the obtained exchange rate to convert the specified amount between the two selected currencies. Allow users to monitor current exchange rates and seamlessly perform real-time currency conversions.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_live_rates(base_currency, target_currencies):
    url = ""https://forex-apised1.p.rapidapi.com/live-rates""
    querystring = {
        ""base_currency_code"": base_currency,
        ""currency_codes"": target_currencies
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""forex-apised1.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def convert_currency(from_currency, to_currency, amount):
    url = ""https://currency-converter18.p.rapidapi.com/api/v1/convert""
    querystring = {
        ""from"": from_currency,
        ""to"": to_currency,
        ""amount"": amount
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""currency-converter18.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def display_live_rates(data, base_currency):
    if data.get('success'):
        print(f""\n### Live Rates for {base_currency} ###"")
        for code, details in data.get('rates', {}).items():
            rate = details['rate']
            currency_name = details['currency_name']
            print(f""- {currency_name} ({code}): Exchange Rate: {rate}"")
        print(""---"")
    else:
        print(""Failed to retrieve live rates."")

def display_conversion_result(data, from_currency, to_currency, amount):
    if data.get('success'):
        converted_amount = data['result']['convertedAmount']
        print(f""\n### Conversion Result ###"")
        print(f""- {amount} {from_currency} to {to_currency}: {converted_amount:.2f}"")
        print(""---"")
    else:
        print(""Failed to retrieve conversion data."")

if __name__ == '__main__':
    # Step 1: Get live exchange rates for a base currency to multiple target currencies
    base_currency = ""USD""
    target_currencies = ""GBP,EUR,JPY""
    live_rates = get_live_rates(base_currency, target_currencies)
    display_live_rates(live_rates, base_currency)

    # Step 2: Convert an amount from one currency to another using the obtained rates
    from_currency = ""EUR""
    to_currency = ""KWD""
    amount_to_convert = 10
    conversion_result = convert_currency(from_currency, to_currency, amount_to_convert)
    display_conversion_result(conversion_result, from_currency, to_currency, amount_to_convert)
",Fail,-1,-1
"Users need a currency conversion service that first retrieves a list of all supported currencies and then converts the specified amount from one base currency to multiple target currencies. Users should be able to select a list of base and target currencies. The service will first obtain the available currency to ensure that a valid currency code is used, and then perform the conversion of the specified amount across the selected target currency, providing detailed conversion rates and amounts.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_supported_currencies():
    url = ""https://currency-converter18.p.rapidapi.com/api/v1/supportedCurrencies""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""currency-converter18.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def multi_currency_convert(base_currency, amount, target_currencies):
    url = ""https://forex-apised1.p.rapidapi.com/multi-convert""
    querystring = {
        ""from"": base_currency,
        ""amount"": amount,
        ""to"": target_currencies
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""forex-apised1.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def display_supported_currencies(data):
    if data:
        print(""\n### Supported Currencies ###"")
        for currency in data:
            print(f""- {currency['name']} (Symbol: {currency['symbol']})"")
    else:
        print(""Failed to retrieve supported currencies."")

def display_conversion_result(data, base_currency, amount):
    if data.get('success'):
        print(f""\n### Conversion Result for {amount} {base_currency} ###"")
        for code, details in data.get('rates', {}).items():
            converted_amount = details['converted_amount']
            rate = details['rate']
            currency_name = details['currency_name']
            print(f""- {currency_name} ({code}):"")
            print(f""  - Conversion Rate: {rate}"")
            print(f""  - Converted Amount: {converted_amount:.2f}"")
            print(""---"")
    else:
        print(""Failed to retrieve conversion data."")

if __name__ == '__main__':
    # Step 1: Retrieve the list of supported currencies
    supported_currencies = get_supported_currencies()
    display_supported_currencies(supported_currencies)

    # Step 2: Convert a specific amount from a base currency to multiple target currencies
    base_currency = ""EUR""
    amount_to_convert = 2
    target_currencies = ""GBP,USD,EUR""
    conversion_result = multi_currency_convert(base_currency, amount_to_convert, target_currencies)
    display_conversion_result(conversion_result, base_currency, amount_to_convert)
","### Supported Currencies ###
- United States Dollar (Symbol: USD)
- Albania Lek (Symbol: ALL)
- Algeria Dinar (Symbol: DZD)
- Angola Kwanza (Symbol: AOA)
- Argentina Peso (Symbol: ARS)
- Armenia Dram (Symbol: AMD)
- Aruba Guilder (Symbol: AWG)
- Australia Dollar (Symbol: AUD)
- Azerbaijan Manat (Symbol: AZN)
- Bahamas Dollar (Symbol: BSD)
- Bahrain Dinar (Symbol: BHD)
- Bangladesh Taka (Symbol: BDT)
- Barbados Dollar (Symbol: BBD)
- Belarus Ruble (Symbol: BYN)
- Belize Dollar (Symbol: BZD)
- Bermuda Dollar (Symbol: BMD)
- Bhutan Ngultrum (Symbol: BTN)
- Bolivia BolÃ­viano (Symbol: BOB)
- Bosnia and Herzegovina Convertible Marka (Symbol: BAM)
- Botswana Pula (Symbol: BWP)
- Brazil Real (Symbol: BRL)
- Brunei Darussalam Dollar (Symbol: BND)
- Bulgaria Lev (Symbol: BGN)
- Burundi Franc (Symbol: BIF)
- Cambodia Riel (Symbol: KHR)
- Canada Dollar (Symbol: CAD)
- Cape Verde Escudo (Symbol: CVE)
- Cayman Islands Dollar (Symbol: KYD)
- Chile Peso (Symbol: CLP)
- China Yuan Renminbi (Symbol: CNY)
- Colombia Peso (Symbol: COP)
- CommunautÃ© FinanciÃ¨re Africaine (BCEAO) Franc (Symbol: XOF)
- CommunautÃ© FinanciÃ¨re Africaine (BEAC) CFA Franc BEAC (Symbol: XAF)
- Comorian Franc (Symbol: KMF)
- Comptoirs FranÃ§ais du Pacifique (CFP) Franc (Symbol: XPF)
- Congo/Kinshasa Franc (Symbol: CDF)
- Costa Rica Colon (Symbol: CRC)
- Croatia Kuna (Symbol: HRK)
- Cuba Peso (Symbol: CUP)
- Czech Republic Koruna (Symbol: CZK)
- Denmark Krone (Symbol: DKK)
- Djibouti Franc (Symbol: DJF)
- Dominican Republic Peso (Symbol: DOP)
- East Caribbean Dollar (Symbol: XCD)
- Egypt Pound (Symbol: EGP)
- El Salvador Colon (Symbol: SVC)
- Ethiopia Birr (Symbol: ETB)
- Euro Member Countries (Symbol: EUR)
- Fiji Dollar (Symbol: FJD)
- Gambia Dalasi (Symbol: GMD)
- Georgia Lari (Symbol: GEL)
- Ghana Cedi (Symbol: GHS)
- Guatemala Quetzal (Symbol: GTQ)
- Guinea Franc (Symbol: GNF)
- Guyana Dollar (Symbol: GYD)
- Haiti Gourde (Symbol: HTG)
- Honduras Lempira (Symbol: HNL)
- Hong Kong Dollar (Symbol: HKD)
- Hungary Forint (Symbol: HUF)
- Iceland Krona (Symbol: ISK)
- India Rupee (Symbol: INR)
- Indonesia Rupiah (Symbol: IDR)
- Iran Rial (Symbol: IRR)
- Iraq Dinar (Symbol: IQD)
- Israel Shekel (Symbol: ILS)
- Jamaica Dollar (Symbol: JMD)
- Japan Yen (Symbol: JPY)
- Jordan Dinar (Symbol: JOD)
- Kazakhstan Tenge (Symbol: KZT)
- Kenya Shilling (Symbol: KES)
- Korea (South) Won (Symbol: KRW)
- Kuwait Dinar (Symbol: KWD)
- Kyrgyzstan Som (Symbol: KGS)
- Laos Kip (Symbol: LAK)
- Lebanon Pound (Symbol: LBP)
- Lesotho Loti (Symbol: LSL)
- Liberia Dollar (Symbol: LRD)
- Libya Dinar (Symbol: LYD)
- Macau Pataca (Symbol: MOP)
- Macedonia Denar (Symbol: MKD)
- Madagascar Ariary (Symbol: MGA)
- Malawi Kwacha (Symbol: MWK)
- Malaysia Ringgit (Symbol: MYR)
- Maldives (Maldive Islands) Rufiyaa (Symbol: MVR)
- Mauritius Rupee (Symbol: MUR)
- Mexico Peso (Symbol: MXN)
- Moldova Leu (Symbol: MDL)
- Morocco Dirham (Symbol: MAD)
- Mozambique Metical (Symbol: MZN)
- Myanmar (Burma) Kyat (Symbol: MMK)
- Namibia Dollar (Symbol: NAD)
- Nepal Rupee (Symbol: NPR)
- Netherlands Antilles Guilder (Symbol: ANG)
- New Zealand Dollar (Symbol: NZD)
- Nicaragua Cordoba (Symbol: NIO)
- Nigeria Naira (Symbol: NGN)
- Norway Krone (Symbol: NOK)
- Oman Rial (Symbol: OMR)
- Pakistan Rupee (Symbol: PKR)
- Panama Balboa (Symbol: PAB)
- Papua New Guinea Kina (Symbol: PGK)
- Paraguay Guarani (Symbol: PYG)
- Peru Sol (Symbol: PEN)
- Philippines Peso (Symbol: PHP)
- Poland Zloty (Symbol: PLN)
- Qatar Riyal (Symbol: QAR)
- Romania Leu (Symbol: RON)
- Russia Ruble (Symbol: RUB)
- Rwanda Franc (Symbol: RWF)
- Saudi Arabia Riyal (Symbol: SAR)
- Serbia Dinar (Symbol: RSD)
- Seychelles Rupee (Symbol: SCR)
- Sierra Leone Leone (Symbol: SLL)
- Singapore Dollar (Symbol: SGD)
- Solomon Islands Dollar (Symbol: SBD)
- Somalia Shilling (Symbol: SOS)
- South Africa Rand (Symbol: ZAR)
- Sri Lanka Rupee (Symbol: LKR)
- Sudan Pound (Symbol: SDG)
- Suriname Dollar (Symbol: SRD)
- Swaziland Lilangeni (Symbol: SZL)
- Sweden Krona (Symbol: SEK)
- Switzerland Franc (Symbol: CHF)
- Taiwan New Dollar (Symbol: TWD)
- Tajikistan Somoni (Symbol: TJS)
- Tanzania Shilling (Symbol: TZS)
- Thailand Baht (Symbol: THB)
- Tonga Pa'anga (Symbol: TOP)
- Trinidad and Tobago Dollar (Symbol: TTD)
- Tunisia Dinar (Symbol: TND)
- Turkey Lira (Symbol: TRY)
- Turkmenistan Manat (Symbol: TMT)
- Uganda Shilling (Symbol: UGX)
- Ukraine Hryvnia (Symbol: UAH)
- United Arab Emirates Dirham (Symbol: AED)
- United Kingdom Pound (Symbol: GBP)
- Uruguay Peso (Symbol: UYU)
- Uzbekistan Som (Symbol: UZS)
- Viet Nam Dong (Symbol: VND)
- Yemen Rial (Symbol: YER)
- Zambia Kwacha (Symbol: ZMW)
- Afghan Afghani (Symbol: AFN)
- Unidad de Fomento (Symbol: CLF)
- Chinese Renminbi Yuan Offshore (Symbol: CNH)
- Belarusian Ruble (Old) (Symbol: BYR)
- Belarusian Ruble (New) (Symbol: BYN)
- Cuban Convertible Peso (Symbol: CUC)
- Estonian Kroon (Symbol: EEK)
- Eritrean Nakfa (Symbol: ERN)
- Falkland Pound (Symbol: FKP)
- Guernsey Pound (Symbol: GGP)
- Gibraltar Pound (Symbol: GIP)
- Isle of Man Pound (Symbol: IMP)
- Jersey Pound (Symbol: JEP)
- Lithuanian Litas (Symbol: LTL)
- Maltese Lira (Symbol: MTL)
- Saint Helenian Pound (Symbol: SHP)
- East Caribbean Dollar (Symbol: XCD)

### Conversion Result for 2 EUR ###
- United Kingdom Pound (GBP):
  - Conversion Rate: 0.8332473675684432
  - Converted Amount: 1.67
---
- United States Dollar (USD):
  - Conversion Rate: 1.0799719207300609
  - Converted Amount: 2.16
---
- Euro Member Countries (EUR):
  - Conversion Rate: 0.9999999999999999
  - Converted Amount: 2.00
---",1,1
"Users need to create a comprehensive commodity market data service by first retrieving a list of supported commodities and currencies. Use this list to get the latest exchange rate for the selected item in the specified base currency. In addition, retrieve real-time or historical market data, including the opening, highest, lowest, previous and current prices of the commodity. The service should allow users to select base currencies and commodities of interest to get the latest exchange rates and detailed market analysis.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_supported_commodities_and_currencies():
    url = ""https://commodities-apised.p.rapidapi.com/v1/supported""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""commodities-apised.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def get_latest_exchange_rates(base_currency, symbols):
    url = ""https://commodities-apised.p.rapidapi.com/v1/latest""
    querystring = {
        ""base"": base_currency,
        ""symbols"": symbols
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""commodities-apised.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_commodity_market_data(base_currency, symbols):
    url = ""https://commodities-apised.p.rapidapi.com/v1/market-data""
    querystring = {
        ""base"": base_currency,
        ""symbols"": symbols
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""commodities-apised.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def display_supported_commodities_and_currencies(data):
    if data.get('success'):
        print(""\n### Supported Commodities ###"")
        for commodity in data.get('supported_commodities', []):
            print(f""- {commodity['commodity_name']} (Code: {commodity['commodity_code']}, Measurement: {commodity['commodity_weight_measurement']})"")

        print(""\n### Supported Currencies ###"")
        for currency in data.get('supported_currencies', []):
            print(f""- {currency['currency_name']} (Code: {currency['currency_code']})"")
    else:
        print(""Failed to retrieve supported commodities and currencies."")

def display_latest_exchange_rates(data, base_currency):
    if data.get('success'):
        print(f""\n### Latest Exchange Rates in {base_currency} ###"")
        for commodity, rate in data.get('rates', {}).items():
            print(f""- {commodity}: {rate} {base_currency}"")
    else:
        print(""Failed to retrieve latest exchange rates."")

def display_commodity_market_data(data, base_currency):
    if data.get('success'):
        print(f""\n### Market Data in {base_currency} ###"")
        for commodity, details in data.get('rates', {}).items():
            print(f""\nCommodity: {commodity}"")
            print(f""- Open: {details['open']} {base_currency}"")
            print(f""- High: {details['high']} {base_currency}"")
            print(f""- Low: {details['low']} {base_currency}"")
            print(f""- Previous Close: {details['prev']} {base_currency}"")
            print(f""- Current: {details['current']} {base_currency}"")
            print(""---"")
    else:
        print(""Failed to retrieve market data."")

if __name__ == '__main__':
    # Step 1: Retrieve supported commodities and currencies
    supported_data = get_supported_commodities_and_currencies()
    display_supported_commodities_and_currencies(supported_data)

    # Step 2: Get latest exchange rates for selected commodities
    base_currency = ""USD""
    commodity_symbols = ""COCOA,COFFEE,CORN,COTTON,GASOLINE,LUMBER""
    latest_rates = get_latest_exchange_rates(base_currency, commodity_symbols)
    display_latest_exchange_rates(latest_rates, base_currency)

    # Step 3: Retrieve market data for selected commodities
    market_data = get_commodity_market_data(base_currency, commodity_symbols)
    display_commodity_market_data(market_data, base_currency)
","### Supported Commodities ###
- Cocoa (Code: COCOA, Measurement: Metric Ton (mt))
- Coffee (Code: COFFEE, Measurement: Pound (lb))
- Corn (Code: CORN, Measurement: Bushel (BU))
- Cotton (Code: COTTON, Measurement: Pound (lb))
- Gasoline (Code: GASOLINE, Measurement: gallon)
- Lumber (Code: LUMBER, Measurement: per 1.000 board feet)
- Natural Gas (Code: NATURALGAS, Measurement: one million British thermal units (MMBtu))
- Oats (Code: OATS, Measurement: Bushel (BU))
- Oil (Code: OIL, Measurement: Barrel (BBL))
- Orange Juice (Code: ORANGEJUICE, Measurement: Pound (lb))
- Soybean (Code: SOYBEAN, Measurement: Metric Ton (mt))
- Sugar (Code: SUGAR, Measurement: Pound (lb))
- Wheat (Code: WHEAT, Measurement: Bushel (BU))

### Supported Currencies ###
- United States Dollar (Code: USD)
- Albania Lek (Code: ALL)
- Algeria Dinar (Code: DZD)
- Angola Kwanza (Code: AOA)
- Argentina Peso (Code: ARS)
- Armenia Dram (Code: AMD)
- Aruba Guilder (Code: AWG)
- Australia Dollar (Code: AUD)
- Azerbaijan Manat (Code: AZN)
- Bahamas Dollar (Code: BSD)
- Bahrain Dinar (Code: BHD)
- Bangladesh Taka (Code: BDT)
- Barbados Dollar (Code: BBD)
- Belarus Ruble (Code: BYN)
- Belize Dollar (Code: BZD)
- Bermuda Dollar (Code: BMD)
- Bhutan Ngultrum (Code: BTN)
- Bolivia BolÃ­viano (Code: BOB)
- Bosnia and Herzegovina Convertible Marka (Code: BAM)
- Botswana Pula (Code: BWP)
- Brazil Real (Code: BRL)
- Brunei Darussalam Dollar (Code: BND)
- Bulgaria Lev (Code: BGN)
- Burundi Franc (Code: BIF)
- Cambodia Riel (Code: KHR)
- Canada Dollar (Code: CAD)
- Cape Verde Escudo (Code: CVE)
- Cayman Islands Dollar (Code: KYD)
- Chile Peso (Code: CLP)
- China Yuan Renminbi (Code: CNY)
- Colombia Peso (Code: COP)
- CommunautÃ© FinanciÃ¨re Africaine (BCEAO) Franc (Code: XOF)
- CommunautÃ© FinanciÃ¨re Africaine (BEAC) CFA Franc BEAC (Code: XAF)
- Comorian Franc (Code: KMF)
- Comptoirs FranÃ§ais du Pacifique (CFP) Franc (Code: XPF)
- Congo/Kinshasa Franc (Code: CDF)
- Costa Rica Colon (Code: CRC)
- Croatia Kuna (Code: HRK)
- Cuba Peso (Code: CUP)
- Czech Republic Koruna (Code: CZK)
- Denmark Krone (Code: DKK)
- Djibouti Franc (Code: DJF)
- Dominican Republic Peso (Code: DOP)
- East Caribbean Dollar (Code: XCD)
- Egypt Pound (Code: EGP)
- El Salvador Colon (Code: SVC)
- Ethiopia Birr (Code: ETB)
- Euro Member Countries (Code: EUR)
- Fiji Dollar (Code: FJD)
- Gambia Dalasi (Code: GMD)
- Georgia Lari (Code: GEL)
- Ghana Cedi (Code: GHS)
- Guatemala Quetzal (Code: GTQ)
- Guinea Franc (Code: GNF)
- Guyana Dollar (Code: GYD)
- Haiti Gourde (Code: HTG)
- Honduras Lempira (Code: HNL)
- Hong Kong Dollar (Code: HKD)
- Hungary Forint (Code: HUF)
- Iceland Krona (Code: ISK)
- India Rupee (Code: INR)
- Indonesia Rupiah (Code: IDR)
- Iran Rial (Code: IRR)
- Iraq Dinar (Code: IQD)
- Israel Shekel (Code: ILS)
- Jamaica Dollar (Code: JMD)
- Japan Yen (Code: JPY)
- Jordan Dinar (Code: JOD)
- Kazakhstan Tenge (Code: KZT)
- Kenya Shilling (Code: KES)
- Korea (South) Won (Code: KRW)
- Kuwait Dinar (Code: KWD)
- Kyrgyzstan Som (Code: KGS)
- Laos Kip (Code: LAK)
- Lebanon Pound (Code: LBP)
- Lesotho Loti (Code: LSL)
- Liberia Dollar (Code: LRD)
- Libya Dinar (Code: LYD)
- Macau Pataca (Code: MOP)
- Macedonia Denar (Code: MKD)
- Madagascar Ariary (Code: MGA)
- Malawi Kwacha (Code: MWK)
- Malaysia Ringgit (Code: MYR)
- Maldives (Maldive Islands) Rufiyaa (Code: MVR)
- Mauritius Rupee (Code: MUR)
- Mexico Peso (Code: MXN)
- Moldova Leu (Code: MDL)
- Morocco Dirham (Code: MAD)
- Mozambique Metical (Code: MZN)
- Myanmar (Burma) Kyat (Code: MMK)
- Namibia Dollar (Code: NAD)
- Nepal Rupee (Code: NPR)
- Netherlands Antilles Guilder (Code: ANG)
- New Zealand Dollar (Code: NZD)
- Nicaragua Cordoba (Code: NIO)
- Nigeria Naira (Code: NGN)
- Norway Krone (Code: NOK)
- Oman Rial (Code: OMR)
- Pakistan Rupee (Code: PKR)
- Panama Balboa (Code: PAB)
- Papua New Guinea Kina (Code: PGK)
- Paraguay Guarani (Code: PYG)
- Peru Sol (Code: PEN)
- Philippines Peso (Code: PHP)
- Poland Zloty (Code: PLN)
- Qatar Riyal (Code: QAR)
- Romania Leu (Code: RON)
- Russia Ruble (Code: RUB)
- Rwanda Franc (Code: RWF)
- Saudi Arabia Riyal (Code: SAR)
- Serbia Dinar (Code: RSD)
- Seychelles Rupee (Code: SCR)
- Sierra Leone Leone (Code: SLL)
- Singapore Dollar (Code: SGD)
- Solomon Islands Dollar (Code: SBD)
- Somalia Shilling (Code: SOS)
- South Africa Rand (Code: ZAR)
- Sri Lanka Rupee (Code: LKR)
- Sudan Pound (Code: SDG)
- Suriname Dollar (Code: SRD)
- Swaziland Lilangeni (Code: SZL)
- Sweden Krona (Code: SEK)
- Switzerland Franc (Code: CHF)
- Taiwan New Dollar (Code: TWD)
- Tajikistan Somoni (Code: TJS)
- Tanzania Shilling (Code: TZS)
- Thailand Baht (Code: THB)
- Tonga Pa'anga (Code: TOP)
- Trinidad and Tobago Dollar (Code: TTD)
- Tunisia Dinar (Code: TND)
- Turkey Lira (Code: TRY)
- Turkmenistan Manat (Code: TMT)
- Uganda Shilling (Code: UGX)
- Ukraine Hryvnia (Code: UAH)
- United Arab Emirates Dirham (Code: AED)
- United Kingdom Pound (Code: GBP)
- Uruguay Peso (Code: UYU)
- Uzbekistan Som (Code: UZS)
- Viet Nam Dong (Code: VND)
- Yemen Rial (Code: YER)
- Zambia Kwacha (Code: ZMW)
- Afghan Afghani (Code: AFN)
- Unidad de Fomento (Code: CLF)
- Chinese Renminbi Yuan Offshore (Code: CNH)
- Belarusian Ruble (Old) (Code: BYR)
- Belarusian Ruble (New) (Code: BYN)
- Cuban Convertible Peso (Code: CUC)
- Estonian Kroon (Code: EEK)
- Eritrean Nakfa (Code: ERN)
- Falkland Pound (Code: FKP)
- Guernsey Pound (Code: GGP)
- Gibraltar Pound (Code: GIP)
- Isle of Man Pound (Code: IMP)
- Jersey Pound (Code: JEP)
- Lithuanian Litas (Code: LTL)
- Maltese Lira (Code: MTL)
- Saint Helenian Pound (Code: SHP)
- East Caribbean Dollar (Code: XCD)

### Latest Exchange Rates in USD ###
- COCOA: 6782 USD
- COFFEE: 247.37 USD
- CORN: 414.43 USD
- COTTON: 70.54 USD
- GASOLINE: 2.0889 USD
- LUMBER: 522.59 USD

### Market Data in USD ###

Commodity: COCOA
- Open: 6696 USD
- High: 6916 USD
- Low: 6421 USD
- Previous Close: 6724 USD
- Current: 6782 USD
---

Commodity: COFFEE
- Open: 246.27 USD
- High: 248.37 USD
- Low: 243.98 USD
- Previous Close: 244.28 USD
- Current: 247.37 USD
---

Commodity: CORN
- Open: 420.72 USD
- High: 420.72 USD
- Low: 413.7 USD
- Previous Close: 420.72 USD
- Current: 414.43 USD
---

Commodity: COTTON
- Open: 71.33 USD
- High: 71.35 USD
- Low: 70.49 USD
- Previous Close: 71.44 USD
- Current: 70.54 USD
---

Commodity: GASOLINE
- Open: 2.0444 USD
- High: 2.0922 USD
- Low: 2.0342 USD
- Previous Close: 2.0464 USD
- Current: 2.0889 USD
---

Commodity: LUMBER
- Open: 522.8 USD
- High: 525.16 USD
- Low: 520.91 USD
- Previous Close: 521.29 USD
- Current: 522.59 USD
---",1,1
"The user first retrieves a comprehensive list of supported currencies, mapping each currency code to its full name. This data is then used to provide a real-time exchange rate relative to the specified base currency, with the ability to choose the response format (JSON or XML). The service should allow users to choose their preferred base currency for conversion and provide up-to-date exchange rates for supported currencies.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_supported_currencies(output_format=""JSON""):
    url = ""https://currencyapi-net.p.rapidapi.com/currencies""
    querystring = {""output"": output_format}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""currencyapi-net.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_exchange_rates(base_currency=""USD"", output_format=""JSON""):
    url = ""https://currencyapi-net.p.rapidapi.com/rates""
    querystring = {
        ""output"": output_format,
        ""base"": base_currency  # Specify the base currency, e.g., 'USD'
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""currencyapi-net.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def format_supported_currencies(data):
    print(""\n### Supported Currencies ###"")
    if data.get('valid'):
        currencies = data.get('currencies', {})
        if currencies:
            for code, name in currencies.items():
                print(f""- {code}: {name}"")
        else:
            print(""No supported currencies available."")
    else:
        print(""Failed to retrieve supported currencies. Response:"", data)

def format_exchange_rates(data, base_currency):
    print(f""\n### Live Exchange Rates relative to {base_currency} ###"")
    if data.get('valid'):
        rates = data.get('rates', {})
        if rates:
            for currency, rate in rates.items():
                print(f""- {currency}: {rate}"")
        else:
            print(""No exchange rate data available."")
    else:
        print(""Failed to retrieve exchange rates. Response:"", data)

if __name__ == '__main__':
    # Step 1: Retrieve a list of supported currencies
    supported_currencies = get_supported_currencies()
    format_supported_currencies(supported_currencies)

    # Step 2: Fetch exchange rates using a specified base currency
    base_currency = ""USD""  # Example base currency
    exchange_rates = get_exchange_rates(base_currency)
    format_exchange_rates(exchange_rates, base_currency)
","### Supported Currencies ###
- AED: United Arab Emirates Dirham
- AFN: Afghan Afghani
- ALL: Albanian Lek
- AMD: Armenian Dram
- ANG: Netherlands Antillean Guilder
- AOA: Angolan Kwanza
- ARS: Argentine Peso
- AUD: Australian Dollar
- AWG: Aruban Florin
- AZN: Azerbaijani Manat
- BAM: Bosnia-Herzegovina Convertible Mark
- BBD: Barbadian Dollar
- BCH: Bitcoin Cash
- BDT: Bangladeshi Taka
- BGN: Bulgarian Lev
- BHD: Bahraini Dinar
- BIF: Burundian Franc
- BMD: Bermudan Dollar
- BND: Brunei Dollar
- BOB: Bolivian Boliviano
- BRL: Brazilian Real
- BSD: Bahamian Dollar
- BTC: Bitcoin
- BTG: Bitcoin Gold
- BWP: Botswanan Pula
- BZD: Belize Dollar
- CAD: Canadian Dollar
- CDF: Congolese Franc
- CHF: Swiss Franc
- CLP: Chilean Peso
- CNH: Chinese Yuan (offshore)
- CNY: Chinese Yuan
- COP: Colombian Peso
- CRC: Costa Rican ColÃ³n
- CUC: Cuban Convertible Peso
- CUP: Cuban Peso
- CVE: Cape Verdean Escudo
- CZK: Czech Republic Koruna
- DASH: DASH
- DJF: Djiboutian Franc
- DKK: Danish Krone
- DOP: Dominican Peso
- DZD: Algerian Dinar
- EGP: Egyptian Pound
- EOS: EOS
- ETB: Ethiopian Birr
- ETH: Ethereum
- EUR: Euro
- FJD: Fijian Dollar
- GBP: British Pound Sterling
- GEL: Georgian Lari
- GHS: Ghanaian Cedi
- GIP: Gibraltar Pound
- GMD: Gambian Dalasi
- GNF: Guinean Franc
- GTQ: Guatemalan Quetzal
- GYD: Guyanaese Dollar
- HKD: Hong Kong Dollar
- HNL: Honduran Lempira
- HRK: Croatian Kuna
- HTG: Haitian Gourde
- HUF: Hungarian Forint
- IDR: Indonesian Rupiah
- ILS: Israeli New Sheqel
- INR: Indian Rupee
- IQD: Iraqi Dinar
- IRR: Iranian Rial
- ISK: Icelandic KrÃ³na
- JMD: Jamaican Dollar
- JOD: Jordanian Dinar
- JPY: Japanese Yen
- KES: Kenyan Shilling
- KGS: Kyrgystani Som
- KHR: Cambodian Riel
- KMF: Comorian Franc
- KRW: South Korean Won
- KWD: Kuwaiti Dinar
- KYD: Cayman Islands Dollar
- KZT: Kazakhstani Tenge
- LAK: Laotian Kip
- LBP: Lebanese Pound
- LKR: Sri Lankan Rupee
- LRD: Liberian Dollar
- LSL: Lesotho Loti
- LTC: Litecoin
- LYD: Libyan Dinar
- MAD: Moroccan Dirham
- MDL: Moldovan Leu
- MKD: Macedonian Denar
- MMK: Myanma Kyat
- MOP: Macanese Pataca
- MUR: Mauritian Rupee
- MVR: Maldivian Rufiyaa
- MWK: Malawian Kwacha
- MXN: Mexican Peso
- MYR: Malaysian Ringgit
- MZN: Mozambican Metical
- NAD: Namibian Dollar
- NGN: Nigerian Naira
- NIO: Nicaraguan CÃ³rdoba
- NOK: Norwegian Krone
- NPR: Nepalese Rupee
- NZD: New Zealand Dollar
- OMR: Omani Rial
- PAB: Panamanian Balboa
- PEN: Peruvian Nuevo Sol
- PGK: Papua New Guinean Kina
- PHP: Philippine Peso
- PKR: Pakistani Rupee
- PLN: Polish Zloty
- PYG: Paraguayan Guarani
- QAR: Qatari Rial
- RON: Romanian Leu
- RSD: Serbian Dinar
- RUB: Russian Ruble
- RWF: Rwandan Franc
- SAR: Saudi Riyal
- SBD: Solomon Islands Dollar
- SCR: Seychellois Rupee
- SDG: Sudanese Pound
- SEK: Swedish Krona
- SGD: Singapore Dollar
- SLL: Sierra Leonean Leone
- SOS: Somali Shilling
- SRD: Surinamese Dollar
- SVC: Salvadoran ColÃ³n
- SZL: Swazi Lilangeni
- THB: Thai Baht
- TJS: Tajikistani Somoni
- TMT: Turkmenistani Manat
- TND: Tunisian Dinar
- TOP: Tongan PaÊ»anga
- TRY: Turkish Lira
- TTD: Trinidad and Tobago Dollar
- TWD: New Taiwan Dollar
- TZS: Tanzanian Shilling
- UAH: Ukrainian Hryvnia
- UGX: Ugandan Shilling
- USD: United States Dollar
- UYU: Uruguayan Peso
- UZS: Uzbekistan Som
- VND: Vietnamese Dong
- XAF: CFA Franc BEAC
- XAG: Silver (troy ounce)
- XAU: Gold (troy ounce)
- XCD: East Caribbean Dollar
- XLM: Stellar Lumens
- XOF: CFA Franc BCEAO
- XRP: Ripple
- YER: Yemeni Rial
- ZAR: South African Rand
- ZMW: Zambian Kwacha

### Live Exchange Rates relative to USD ###
- AED: 3.673
- AFN: 66.69506
- ALL: 91.1980228
- AMD: 385.831049
- ANG: 1.806062
- AOA: 909.051
- ARS: 984.393427
- AUD: 1.51377536
- AWG: 1.79994445
- AZN: 1.7
- BAM: 1.810691
- BBD: 2.02228968
- BCH: 0.002856098818
- BDT: 119.753
- BGN: 1.810985
- BHD: 0.376744184
- BIF: 2910.271
- BMD: 1
- BND: 1.32303483
- BOB: 6.92014225
- BRL: 5.7075
- BSD: 1.002092
- BTC: 1.49187599e-05
- BTG: 0.04765539576
- BWP: 13.3957001
- BZD: 2.01886264
- CAD: 1.3892499
- CDF: 2847.86857
- CHF: 0.8668313
- CLP: 952.380952
- CNH: 7.1343
- CNY: 7.12110218
- COP: 4331.00297
- CRC: 515.809031
- CUC: 1
- CUP: 24.0502207
- CVE: 102.083994
- CZK: 23.43
- DASH: 0.0456
- DJF: 178.351833
- DKK: 6.9113
- DOP: 60.34375
- DZD: 133.489712
- EGP: 48.74983
- EOS: 2.250032932
- ETB: 121.858949
- ETH: 0.0004037104367
- EUR: 0.92595
- FJD: 2.24825
- GBP: 0.7715454
- GEL: 2.72
- GHS: 16.10439
- GIP: 0.7715454
- GMD: 69.5
- GNF: 8641.59337
- GTQ: 7.751773
- GYD: 209.658
- HKD: 7.77035
- HNL: 25.2549104
- HRK: 6.8253273
- HTG: 131.920848
- HUF: 374.28
- IDR: 15695.1
- ILS: 3.7886
- INR: 84.102238
- IQD: 1312.20525
- IRR: 42105
- ISK: 138.12
- JMD: 158.730436
- JOD: 0.7089
- JPY: 152.284998
- KES: 129.27
- KGS: 85.8
- KHR: 4067.73594
- KMF: 454.949532
- KRW: 1388.66
- KWD: 0.306185592
- KYD: 0.834666713
- KZT: 487.4553
- LAK: 22010.1423
- LBP: 89739.2178
- LKR: 294.2369
- LRD: 192.4102
- LSL: 17.69021
- LTC: 0.01459148194
- LYD: 4.82699332
- MAD: 9.88528131
- MDL: 17.8906879
- MKD: 56.96367
- MMK: 3247.83199
- MOP: 8.01587143
- MUR: 46.0883227
- MVR: 15.3593481
- MWK: 1737.645
- MXN: 19.9839
- MYR: 4.33999899
- MZN: 63.9033985
- NAD: 17.68996
- NGN: 1645
- NIO: 36.87485
- NOK: 10.9731987
- NPR: 134.8032
- NZD: 1.67322
- OMR: 0.38499
- PAB: 1.00156705
- PEN: 3.76148004
- PGK: 4.010591
- PHP: 58.565
- PKR: 278.1873
- PLN: 4.016349
- PYG: 8020.257
- QAR: 3.654804
- RON: 4.60739893
- RSD: 108.3564
- RUB: 97.19037
- RWF: 1355.287
- SAR: 3.75588
- SBD: 8.351058
- SCR: 13.1487546
- SDG: 601.5
- SEK: 10.6211
- SGD: 1.3209
- SLL: 19750
- SOS: 572.7035
- SRD: 34.0505
- SVC: 8.768308
- SZL: 17.68382
- THB: 33.6599923
- TJS: 10.66511
- TMT: 3.5
- TND: 3.108764
- TOP: 2.3421
- TRY: 34.2914008
- TTD: 6.808282
- TWD: 32.0788
- TZS: 2727.879
- UAH: 41.4378606
- UGX: 3675.449
- USD: 1
- UYU: 41.6146484
- UZS: 12842.8231
- VND: 25369.9574
- XAF: 607.288802
- XAG: 0.029673942717420982
- XAU: 0.00036388976323512554
- XCD: 2.70255
- XLM: 10.65551307
- XOF: 607.288802
- XRP: 1.954115569
- YER: 250.348798
- ZAR: 17.67098
- ZMW: 26.5307373",1,1
"The user wants to retrieve real-time pricing data for a list of items that use US dollars as the default currency. After obtaining the price of the item, it provides the ability to convert the specified amount from one currency to another. This allows users to see the prices of goods in US dollars and, if needed, to convert those prices into different currencies for further analysis or comparison.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_market_data(symbols=""COCOA,COFFEE,CORN,COTTON,GASOLINE,LUMBER,NATURALGAS,OATS,OIL,ORANGEJUICE,SOYBEAN,SUGAR,WHEAT"", base_currency=""USD""):
    url = ""https://commodities-apised.p.rapidapi.com/v1/market-data""
    querystring = {
        ""symbols"": symbols,
        ""base"": base_currency
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""commodities-apised.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def convert_currency(from_currency=""USD"", to_currency=""EUR"", amount=100):
    url = ""https://currency-converter18.p.rapidapi.com/api/v1/convert""
    querystring = {
        ""from"": from_currency,
        ""to"": to_currency,
        ""amount"": str(amount)
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""currency-converter18.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def format_market_data(data):
    print(""\n### Market Data for Commodities (in USD) ###"")
    rates = data.get('rates', {})
    if rates:
        for commodity, details in rates.items():
            open_price = details.get('open', 'N/A')
            high = details.get('high', 'N/A')
            low = details.get('low', 'N/A')
            prev = details.get('prev', 'N/A')
            current = details.get('current', 'N/A')
            print(f""\nCommodity: {commodity}"")
            print(f""  - Open Price    : {open_price} USD"")
            print(f""  - High Price    : {high} USD"")
            print(f""  - Low Price     : {low} USD"")
            print(f""  - Previous Close: {prev} USD"")
            print(f""  - Current Price : {current} USD"")
    else:
        print(""No market data available. Response:"", data)

def format_currency_conversion(data, from_currency, to_currency, amount):
    print(f""\n### Currency Conversion: {amount} {from_currency} to {to_currency} ###"")
    result = data.get('result', {})
    if result:
        converted_amount = result.get('convertedAmount', 'Unknown')
        print(f""Converted Amount: {amount} {from_currency} = {converted_amount} {to_currency}"")
    else:
        print(""Currency conversion data is not available. Response:"", data)

if __name__ == '__main__':
    # Step 1: Retrieve market data for commodities in USD
    market_data = get_market_data()
    format_market_data(market_data)

    # Step 2: Convert a specified amount from USD to EUR
    conversion_result = convert_currency(from_currency=""USD"", to_currency=""EUR"", amount=150)
    format_currency_conversion(conversion_result, ""USD"", ""EUR"", 150)","### Market Data for Commodities (in USD) ###

Commodity: COCOA
  - Open Price    : 6696 USD
  - High Price    : 6916 USD
  - Low Price     : 6421 USD
  - Previous Close: 6724 USD
  - Current Price : 6782 USD

Commodity: COFFEE
  - Open Price    : 246.27 USD
  - High Price    : 248.37 USD
  - Low Price     : 243.98 USD
  - Previous Close: 244.28 USD
  - Current Price : 247.37 USD

Commodity: CORN
  - Open Price    : 420.72 USD
  - High Price    : 420.72 USD
  - Low Price     : 413.7 USD
  - Previous Close: 420.72 USD
  - Current Price : 414.43 USD

Commodity: COTTON
  - Open Price    : 71.33 USD
  - High Price    : 71.35 USD
  - Low Price     : 70.49 USD
  - Previous Close: 71.44 USD
  - Current Price : 70.54 USD

Commodity: GASOLINE
  - Open Price    : 2.0444 USD
  - High Price    : 2.0922 USD
  - Low Price     : 2.0342 USD
  - Previous Close: 2.0464 USD
  - Current Price : 2.0889 USD

Commodity: LUMBER
  - Open Price    : 522.8 USD
  - High Price    : 525.16 USD
  - Low Price     : 520.91 USD
  - Previous Close: 521.29 USD
  - Current Price : 522.59 USD

Commodity: NATURALGAS
  - Open Price    : 2.982 USD
  - High Price    : 3.079 USD
  - Low Price     : 2.942 USD
  - Previous Close: 2.972 USD
  - Current Price : 3.038 USD

Commodity: OATS
  - Open Price    : 3.8587 USD
  - High Price    : 3.8725 USD
  - Low Price     : 3.8311 USD
  - Previous Close: 3.8638 USD
  - Current Price : 3.8522 USD

Commodity: OIL
  - Open Price    : 70.295 USD
  - High Price    : 71.925 USD
  - Low Price     : 69.96 USD
  - Previous Close: 70.295 USD
  - Current Price : 71.675 USD

Commodity: ORANGEJUICE
  - Open Price    : 4.60363 USD
  - High Price    : 4.72281 USD
  - Low Price     : 4.57388 USD
  - Previous Close: 4.60307 USD
  - Current Price : 4.70631 USD

Commodity: SOYBEAN
  - Open Price    : 1002.5 USD
  - High Price    : 1004.6 USD
  - Low Price     : 992.9 USD
  - Previous Close: 1012.5 USD
  - Current Price : 996.7 USD

Commodity: SUGAR
  - Open Price    : 0.21874 USD
  - High Price    : 0.22004 USD
  - Low Price     : 0.216 USD
  - Previous Close: 0.21824 USD
  - Current Price : 0.21796 USD

Commodity: WHEAT
  - Open Price    : 583.35 USD
  - High Price    : 583.39 USD
  - Low Price     : 565.13 USD
  - Previous Close: 580.15 USD
  - Current Price : 567.41 USD

### Currency Conversion: 150 USD to EUR ###
Converted Amount: 150 USD = 138.8925 EUR",1,1
"Develop a system that allows users to search for recipes based on specific criteria such as ingredients, preparation time, nutritional values, and tags, and then retrieve detailed information about any selected recipe, including ingredients, steps, and nutritional data. This system should provide a seamless interface for filtering recipes and displaying comprehensive recipe details.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def search_recipes(name=None, tags=None, includeIngredients=None, excludeIngredients=None, maxPrepareTime=None,
                   maxCookTime=None, maxCalories=None, maxNetCarbs=None, maxSugar=None, maxAddedSugar=None, limit=10):
    url = ""https://low-carb-recipes.p.rapidapi.com/search""
    querystring = {
        ""name"": name,
        ""tags"": tags,
        ""includeIngredients"": includeIngredients,
        ""excludeIngredients"": excludeIngredients,
        ""maxPrepareTime"": maxPrepareTime,
        ""maxCookTime"": maxCookTime,
        ""maxCalories"": maxCalories,
        ""maxNetCarbs"": maxNetCarbs,
        ""maxSugar"": maxSugar,
        ""maxAddedSugar"": maxAddedSugar,
        ""limit"": limit
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""low-carb-recipes.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params={k: v for k, v in querystring.items() if v is not None})
    return response.json()


def get_recipe_by_id(recipe_id):
    url = f""https://low-carb-recipes.p.rapidapi.com/recipes/{recipe_id}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""low-carb-recipes.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()


def format_search_results(results):
    print(""\n### Recipe Search Results ###"")
    if results:
        for idx, recipe in enumerate(results, 1):
            title = recipe.get('name', 'No title available')
            tags = ', '.join(recipe.get('tags', []))
            calories = recipe.get('caloriesKCal', 'Unknown')
            cook_time = recipe.get('cookTime', 'Unknown')
            net_carbs = recipe.get('netCarbs', 'Unknown')
            print(f""\nRecipe {idx}:"")
            print(f""  - Title       : {title}"")
            print(f""  - Tags        : {tags}"")
            print(f""  - Calories    : {calories} kcal"")
            print(f""  - Cook Time   : {cook_time} minutes"")
            print(f""  - Net Carbs   : {net_carbs}g"")
    else:
        print(""No recipes found based on the search criteria."")


def format_detailed_recipe(recipe):
    print(""\n### Detailed Recipe Information ###"")
    title = recipe.get('name', 'No title available')
    description = recipe.get('description', 'No description available')
    prepare_time = recipe.get('prepareTime', 'Unknown')
    cook_time = recipe.get('cookTime', 'Unknown')
    ingredients = recipe.get('ingredients', [])
    steps = recipe.get('steps', [])
    calories = recipe.get('nutrients', {}).get('caloriesKCal', 'Unknown')
    net_carbs = recipe.get('nutrients', {}).get('netCarbs', 'Unknown')
    image = recipe.get('image', 'No image available')

    print(f""Recipe Title : {title}"")
    print(f""Description  : {description[:100]}..."")  # Show only the first 100 characters
    print(f""Prepare Time : {prepare_time} minutes"")
    print(f""Cook Time    : {cook_time} minutes"")
    print(f""Calories     : {calories} kcal"")
    print(f""Net Carbs    : {net_carbs}g"")

    print(""\nIngredients  :"")
    for ingredient in ingredients:
        name = ingredient.get('name', 'Unknown ingredient')
        desc = ingredient.get('servingSize', {}).get('desc', 'No description')
        qty = ingredient.get('servingSize', {}).get('qty', 'Unknown quantity')
        print(f""  - {name}: {desc} ({qty}g)"")

    print(""\nSteps        :"")
    for idx, step in enumerate(steps, 1):
        print(f""  {idx}. {step}"")

    print(f""\nImage        : {image}"")


if __name__ == '__main__':
    # Step 1: Search for recipes based on specific criteria
    search_results = search_recipes(
        name=""cake"",
        tags=""keto;dairy-free"",
        includeIngredients=""egg;butter"",
        excludeIngredients=""cinnamon"",
        maxPrepareTime=""10"",
        maxCookTime=""20"",
        maxCalories=""500"",
        maxNetCarbs=""5"",
        maxSugar=""3"",
        maxAddedSugar=""0"",
        limit=""5""
    )
    format_search_results(search_results)

    # Step 2: Retrieve details for the first recipe in the search results
    if search_results and 'id' in search_results[0]:
        recipe_id = search_results[0]['id']
        detailed_recipe = get_recipe_by_id(recipe_id)
        format_detailed_recipe(detailed_recipe)
",Fail,-1,-1
"Users navigate the list of Chinese food recipes by displaying basic information such as the title, difficulty level and picture of each recipe. Users can select a specific recipe from the list to see details, including ingredients, preparation time, step-by-step cooking instructions, and other details such as serving size and difficulty.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_chinese_food_list():
    url = ""https://chinese-food-db.p.rapidapi.com/""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""chinese-food-db.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def get_chinese_food_details(recipe_id):
    url = f""https://chinese-food-db.p.rapidapi.com/{recipe_id}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""chinese-food-db.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def format_chinese_food_list(food_list):
    print(""\n### Chinese Food List (First Item Example) ###"")
    if food_list:
        first_item = food_list[0]
        title = first_item.get('title', 'No title available')
        category = first_item.get('category', 'No category available')
        difficulty = first_item.get('difficulty', 'No difficulty level')
        image = first_item.get('image', 'No image available')
        print(f""Recipe Title: {title}"")
        print(f""Category    : {category}"")
        print(f""Difficulty  : {difficulty}"")
        print(f""Image       : {image}"")
    else:
        print(""No Chinese food available."")

def format_chinese_food_details(recipe):
    print(""\n### Detailed Recipe Information ###"")
    title = recipe.get('title', 'No title available')
    description = recipe.get('description', 'No description available')
    ingredients = recipe.get('ingredients', [])
    instructions = recipe.get('instructions', [])
    category = recipe.get('category', 'No category available')
    difficulty = recipe.get('difficulty', 'No difficulty level')
    image = recipe.get('image', 'No image available')

    print(f""Recipe Title : {title}"")
    print(f""Category     : {category}"")
    print(f""Difficulty   : {difficulty}"")
    print(f""Description  : {description[:100]}..."")  # Truncate to the first 100 characters

    print(""\nIngredients  :"")
    for ingredient in ingredients:
        print(f""  - {ingredient}"")

    print(""\nInstructions :"")
    for idx, step in enumerate(instructions, 1):
        print(f""  {idx}. {step}"")

    print(f""\nImage        : {image}"")

if __name__ == '__main__':
    # Step 1: Retrieve and display the list of Chinese recipes
    chinese_food_list = get_chinese_food_list()
    format_chinese_food_list(chinese_food_list)

    # Step 2: Retrieve and display detailed information for the first recipe in the list
    if chinese_food_list and len(chinese_food_list) > 0:
        first_recipe_id = chinese_food_list[0]['id']
        detailed_recipe_info = get_chinese_food_details(first_recipe_id)
        format_chinese_food_details(detailed_recipe_info)
","### Chinese Food List (First Item Example) ###
Recipe Title: Homemade crispy chilli oil
Category    : No category available
Difficulty  : Easy
Image       : https://apipics.s3.amazonaws.com/chinese_recipes_api/1.jpg

### Detailed Recipe Information ###
Recipe Title : Homemade crispy chilli oil
Category     : No category available
Difficulty   : Easy
Description  : This is a full-flavoured, in-your-face crispy chilli oil, with plenty of fire, spice and floral numb...

Ingredients  :
  - 250g vegetable or sunflower oil
  - 2 slices fresh ginger
  - 3 banana shallots, finely sliced
  - 5 garlic cloves, finely sliced
  - 3 tbsp Sichuan peppercorns
  - 3 star anise
  - 1 cinnamon stick
  - 1 tsp fennel seeds
  - 50g gochugaru (Korean red pepper flakes)
  - 25g chilli flakes, or crushed dried chillies
  - Â½ tbsp sweet paprika
  - 2 dried bay leaves
  - 2 tsp salt
  - 1 tsp caster sugar
  - Thermometer
  - Sterilised jar

Instructions :

Image        : https://apipics.s3.amazonaws.com/chinese_recipes_api/1.jpg",1,1
"Users are asked to view a list of vegetarian recipes, including title, difficulty, and a representative image of each recipe. In addition, provide the ability to access random recipes, displaying comprehensive information such as preparation time, cooking time, ingredients, step-by-step instructions, and nutritional details.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def get_vegan_food_list():
    url = ""https://the-vegan-recipes-db.p.rapidapi.com/""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""the-vegan-recipes-db.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()


def get_random_vegan_recipe():
    url = ""https://low-carb-recipes.p.rapidapi.com/random""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""low-carb-recipes.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()


def format_vegan_food_list(food_list):
    print(""\n### Vegan Food List (First Item Example) ###"")
    if food_list:
        first_item = food_list[0]
        title = first_item.get('title', 'No title available')
        difficulty = first_item.get('difficulty', 'No difficulty level')
        image = first_item.get('image', 'No image available')
        print(f""Recipe Title: {title}"")
        print(f""Difficulty   : {difficulty}"")
        print(f""Image        : {image}"")
    else:
        print(""No vegan food available."")


def format_random_vegan_recipe(recipe):
    print(""\n### Random Vegan Recipe ###"")
    title = recipe.get('name', 'No title available')
    tags = ', '.join(recipe.get('tags', []))
    description = recipe.get('description', 'No description available')
    prepare_time = recipe.get('prepareTime', 'Unknown')
    ingredients = recipe.get('ingredients', [])
    steps = recipe.get('steps', [])
    servings = recipe.get('servings', 'Unknown')
    calories = recipe.get('nutrients', {}).get('caloriesKCal', 'Unknown')
    image = recipe.get('image', 'No image available')

    print(f""Recipe Title : {title}"")
    print(f""Tags         : {tags}"")
    print(f""Description  : {description[:100]}..."")  # Show only the first 100 characters
    print(f""Prepare Time : {prepare_time} minutes"")

    print(""\nIngredients  :"")
    for ingredient in ingredients:
        name = ingredient.get('name', 'Unknown ingredient')
        desc = ingredient.get('servingSize', {}).get('desc', 'No description')
        qty = ingredient.get('servingSize', {}).get('qty', 'Unknown quantity')
        print(f""  - {name}: {desc} ({qty}g)"")

    print(""\nSteps        :"")
    for idx, step in enumerate(steps, 1):
        print(f""  {idx}. {step}"")

    print(f""\nServings     : {servings}"")
    print(f""Calories     : {calories} kcal per serving"")
    print(f""Image        : {image}"")


if __name__ == '__main__':
    # Step 1: Retrieve and display the list of vegan recipes
    vegan_food_list = get_vegan_food_list()
    format_vegan_food_list(vegan_food_list)

    # Step 2: Retrieve and display details of a random vegan recipe
    random_vegan_recipe = get_random_vegan_recipe()
    format_random_vegan_recipe(random_vegan_recipe)
","### Vegan Food List (First Item Example) ###
Recipe Title: Dark chocolate bark with sea salt, rosemary, pistachios and candied citrus
Difficulty   : Easy
Image        : https://apipics.s3.amazonaws.com/vegan_api/1.jpg

### Random Vegan Recipe ###
Recipe Title : Keto Coconut Chai Smoothie
Tags         : meal-plan-ok, quick-easy, paleo, vegan, vegetarian, pescatarian, dairy-free, gluten-free, wheat-free, egg-free, peanut-free, soy-free, fish-free, shellfish-free, pork-free, beef-free, chicken-free, lchf, relevant-meal--beverages, snacks, beverages
Description  : If you need a light breakfast or you simply want a drinkable treat, this coconut chai smoothie recip...
Prepare Time : 15 minutes

Ingredients  :
  - Almond Milk, Vanilla Or Other Flavors, Unsweetened: 2 cup (2.0g)
  - Spiced Chai Tea Bags by Trader Joe's: 1 bag (1.0g)
  - Frozen Coconut Chunks By Trader Joe's: 1 ounce (1.0g)

Steps        :
  1. Heat the almond milk over the stove to a strong simmer. Steep the chai tea in the almond milk for 5-10 minutes, depending on how strong you want the flavor to be. After the milk has steeped, discard the tea bag and store the milk in the freezer until it becomes cold.
  2. For the coconut, cut it into cubes if necessary. The coconut must be frozen for this recipe, so have this prepare in advance. When the almond milk is cold, blend it with the frozen coconut cubes.
  3. You may refrigerate your smoothie for a thicker consistency before consuming. Pour it in a tall glass. Enjoy !

Servings     : 1
Calories     : 128.935 kcal per serving
Image        : https://tinyurl.com/2p82zzca/7c3d80d3-c7c7-46ab-9f3c-6d5a77284474.png",1,1
"Users verify the halal status of animal and food additives using their name or designated E number. Users can enter the name of an animal to determine if it is halal, or they can enter an E number to check if a particular food additive meets halal standards. This ensures that users have access to halal information on animal-based products and common food additives.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def check_halal_animal(animal_name):
    url = f""https://halal-food-checker.p.rapidapi.com/check-halal-animal/{animal_name}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""halal-food-checker.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def check_halal_e_number(e_number):
    url = f""https://halal-food-checker.p.rapidapi.com/check-halal-number/{e_number}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""halal-food-checker.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def print_halal_animal_result(result):
    if result and 'animal' in result:
        animal = result['animal']
        halal_status = ""Halal"" if result['is_halal'] else ""Not Halal""
        print(f""Animal Check Result:"")
        print(f""  - Animal: {animal}"")
        print(f""  - Status: {halal_status}\n"")

def print_halal_e_number_result(result):
    if result and 'e_number' in result:
        e_number = result['e_number']
        name = result.get('name', 'Unknown')
        halal_status = ""Halal"" if result['is_halal'] else ""Not Halal""
        print(f""E Number Check Result:"")
        print(f""  - E Number: {e_number}"")
        print(f""  - Name: {name}"")
        print(f""  - Status: {halal_status}\n"")

if __name__ == '__main__':
    # Step 1: Check if a given animal is halal
    animal_to_check = ""Dog""  # Example animal
    animal_halal_status = check_halal_animal(animal_to_check)
    print_halal_animal_result(animal_halal_status)

    # Step 2: Check if a given E number is halal
    e_number_to_check = ""E101""  # Example E number
    e_number_halal_status = check_halal_e_number(e_number_to_check)
    print_halal_e_number_result(e_number_halal_status)
","Animal Check Result:
  - Animal: Dog
  - Status: Not Halal

E Number Check Result:
  - E Number: E101
  - Name: Riboflavin/Lactofavin/Vitamin B2 *
  - Status: Not Halal
",1,1
Give me some hotel information in New York City.,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


# API 1: Search Hotels in New York City
def search_hotels(query=""new york""):
    url = ""https://tripadvisor-scraper.p.rapidapi.com/hotels/search""
    querystring = {""query"": query}

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""tripadvisor-scraper.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        return {""error"": f""Failed to search hotels: {response.status_code}""}


# API 2: List Hotels with Detailed Information
def list_hotels(query=""new york"", page=1):
    url = ""https://tripadvisor-scraper.p.rapidapi.com/hotels/list""
    querystring = {""query"": query, ""page"": str(page)}

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""tripadvisor-scraper.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        return {""error"": f""Failed to list hotels: {response.status_code}""}


if __name__ == '__main__':
    # Step 1: Search for hotels in New York
    search_results = search_hotels()

    if search_results and isinstance(search_results, list) and len(search_results) > 0:
        for result in search_results:

            # If the result type is an accommodation, fetch detailed information
            if result['type'] == 'accommodation':

                # Step 2: List hotels with detailed information (pagination starts from page 1)
                hotel_list = list_hotels(query=result['name'], page=1)

                if 'results' in hotel_list and len(hotel_list['results']) > 0:
                    for hotel in hotel_list['results']:
                        print(""=== Hotel Details ==="")
                        print(f""Name: {hotel['name']}"")
                        print(f""Rating: {hotel['rating']} / 5"")
                        print(f""Reviews: {hotel['reviews']}"")

                        # Check if price range information is available
                        if 'price_range_usd' in hotel:
                            print(
                                f""Price Range: ${hotel['price_range_usd']['min']} - ${hotel['price_range_usd']['max']}"")
                        else:
                            print(""Price Range: Information not available"")

                        # Check if detailed address is available
                        if 'detailed_address' in hotel:
                            address = hotel['detailed_address']
                            street = address.get('street', 'Street info not available')
                            city = address.get('city', 'City info not available')
                            print(f""Address: {street}, {city}"")
                        else:
                            print(""Address: Information not available"")

                        print(f""Phone: {hotel.get('phone', 'Phone number not available')}"")
                        print(f""Featured Image: {hotel.get('featured_image', 'Image not available')}"")
                        print(
                            f""Amenities: {', '.join(hotel['amenities']) if hotel.get('amenities') else 'No amenities listed'}"")
                        print(f""Link to Hotel: {hotel['link']}"")
                        print(""====================\n"")
                else:
                    print(""No detailed hotel information found for the specified query."")
    else:
        print(""No hotels found in the search results."")
","=== Hotel Details ===
Name: The New Yorker, A Wyndham Hotel
Rating: 4 / 5
Reviews: 16534
Price Range: Information not available
Address: Information not available
Phone: None
Featured Image: https://dynamic-media-cdn.tripadvisor.com/media/photo-o/2b/87/f3/7b/welcome-to-the-wyndham.jpg
Amenities: No amenities listed
Link to Hotel: https://www.tripadvisor.com/Hotel_Review-g60763-d122005-Reviews-The_New_Yorker_A_Wyndham_Hotel-New_York_City_New_York.html
====================

=== Hotel Details ===
Name: New York Hilton Midtown
Rating: 4 / 5
Reviews: 14987
Price Range: Information not available
Address: Information not available
Phone: 1 (855) 605-0316
Featured Image: https://dynamic-media-cdn.tripadvisor.com/media/photo-o/2d/dd/2c/6c/guest-room.jpg
Amenities: No amenities listed
Link to Hotel: https://www.tripadvisor.com/Hotel_Review-g60763-d611947-Reviews-New_York_Hilton_Midtown-New_York_City_New_York.html
====================",1,1
Give me some information about restaurants in New York City.,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


# API 1: List Restaurants by Search Query
def list_restaurants_by_query(query=""new york"", page=1):
    url = ""https://tripadvisor-scraper.p.rapidapi.com/restaurants/list""
    querystring = {""query"": query, ""page"": str(page)}

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""tripadvisor-scraper.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        return {""error"": f""Failed to list restaurants by query: {response.status_code}""}


# API 2: List Restaurants by ID
def list_restaurants_by_id(restaurant_id, page=1):
    url = ""https://tripadvisor-scraper.p.rapidapi.com/restaurants/list""
    querystring = {""query"": str(restaurant_id), ""page"": str(page)}

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""tripadvisor-scraper.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        return {""error"": f""Failed to list restaurants by ID: {response.status_code}""}


if __name__ == '__main__':
    # Step 1: List restaurants in New York by a search query
    search_query = ""new york""
    page = 1
    restaurant_list = list_restaurants_by_query(query=search_query, page=page)

    if 'results' in restaurant_list and len(restaurant_list['results']) > 0:
        print(""List of Restaurants:"")
        for restaurant in restaurant_list['results'][:6]:
            print(
                f""Name: {restaurant['name']}, Rating: {restaurant['rating']}, Cuisines: {', '.join(restaurant['cuisines'])}"")
            print(f""Link: {restaurant['link']}"")
            print(f""Featured Image: {restaurant['featured_image']}\n"")

            # Use the restaurant ID to fetch additional details if needed
            restaurant_id = restaurant['id']

            # Step 2: Retrieve detailed restaurant information using the ID
            detailed_info = list_restaurants_by_id(restaurant_id, page=1)

            if 'results' in detailed_info and len(detailed_info['results']) > 0:
                for detailed_restaurant in detailed_info['results']:
                    print(""=== Detailed Restaurant Information ==="")
                    print(f""Name: {detailed_restaurant['name']}"")
                    print(f""Rating: {detailed_restaurant['rating']} / 5"")
                    print(f""Reviews: {detailed_restaurant['reviews']}"")

                    # Handle possible null fields gracefully
                    price_range = detailed_restaurant.get('price_range_usd', 'Price information not available')
                    menu_link = detailed_restaurant.get('menu_link', 'Menu link not available')
                    reservation_link = detailed_restaurant.get('reservation_link', 'Reservation link not available')

                    print(f""Price Range: {price_range}"")
                    print(f""Menu: {menu_link}"")
                    print(f""Reservation: {reservation_link}"")
                    print(f""Has Delivery: {'Yes' if detailed_restaurant['has_delivery'] else 'No'}"")
                    print(f""Latitude: {detailed_restaurant['latitude']}, Longitude: {detailed_restaurant['longitude']}"")
                    print(f""Cuisines: {', '.join(detailed_restaurant['cuisines'])}"")
                    print(f""Link to Restaurant: {detailed_restaurant['link']}"")
                    print(""============================\n"")

",Fail,-1,-1
I want to know some sports data.,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def list_of_sports():
    url = ""https://pinnacle-odds.p.rapidapi.com/kit/v1/sports""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""pinnacle-odds.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def sports():
    url = ""https://odds-api1.p.rapidapi.com/sports""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""odds-api1.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def merge_sports_data():
    list_of_sports_data = list_of_sports()
    sports_data = sports()

    merged_data = []

    # Convert sports_data from object-like to a list of dictionaries for easier processing
    sports_list = [v for k, v in sports_data.items() if k.isdigit()]

    # Match data based on sport name
    for sport in list_of_sports_data:
        matched_sport = next((s for s in sports_list if s['name'].lower() == sport['name'].lower()), None)
        if matched_sport:
            merged_data.append({
                ""ID"": sport.get('id'),
                ""Sport Name"": sport.get('name'),
                ""Slug"": matched_sport.get('slug'),
                ""Last API Call Timestamp"": sport.get('last_call'),
                ""Last Event Timestamp"": sport.get('last'),
                ""Special Event Timestamp"": sport.get('special_last')
            })
        else:
            # Sport from List of Sports not found in Sports data
            merged_data.append({
                ""ID"": sport.get('id'),
                ""Sport Name"": sport.get('name'),
                ""Slug"": ""Not Available"",
                ""Last API Call Timestamp"": sport.get('last_call'),
                ""Last Event Timestamp"": sport.get('last'),
                ""Special Event Timestamp"": sport.get('special_last')
            })

    return merged_data

def display_user_friendly(data):
    for entry in data:
        print(f""Sport ID: {entry['ID']}"")
        print(f""Sport Name: {entry['Sport Name']}"")
        print(f""Slug: {entry['Slug']}"")
        print(f""Last API Call Timestamp: {entry['Last API Call Timestamp']}"")
        print(f""Last Event Timestamp: {entry['Last Event Timestamp']}"")
        print(f""Special Event Timestamp: {entry['Special Event Timestamp']}"")
        print("""")

if __name__ == '__main__':
    unified_sports_data = merge_sports_data()
    display_user_friendly(unified_sports_data)
","Sport ID: 1
Sport Name: Soccer
Slug: soccer
Last API Call Timestamp: 1729957495
Last Event Timestamp: 1729957495
Special Event Timestamp: 1729957475

Sport ID: 2
Sport Name: Tennis
Slug: tennis
Last API Call Timestamp: 1729957496
Last Event Timestamp: 1729957495
Special Event Timestamp: 1729949222

Sport ID: 3
Sport Name: Basketball
Slug: basketball
Last API Call Timestamp: 1729957496
Last Event Timestamp: 1729957489
Special Event Timestamp: 1729957401

Sport ID: 4
Sport Name: Hockey
Slug: Not Available
Last API Call Timestamp: 1729957490
Last Event Timestamp: 1729957490
Special Event Timestamp: 1729957476

Sport ID: 5
Sport Name: Volleyball
Slug: volleyball
Last API Call Timestamp: 1729957490
Last Event Timestamp: 1729957478
Special Event Timestamp: None

Sport ID: 6
Sport Name: Handball
Slug: handball
Last API Call Timestamp: 1729957490
Last Event Timestamp: 1729957471
Special Event Timestamp: None

Sport ID: 7
Sport Name: American Football
Slug: american-football
Last API Call Timestamp: 1729957490
Last Event Timestamp: 1729957490
Special Event Timestamp: 1729957477

Sport ID: 8
Sport Name: Mixed Martial Arts
Slug: Not Available
Last API Call Timestamp: 1729957490
Last Event Timestamp: 1729957465
Special Event Timestamp: 1729957327

Sport ID: 9
Sport Name: Baseball
Slug: baseball
Last API Call Timestamp: 1729957490
Last Event Timestamp: 1729957264
Special Event Timestamp: 1729955407",1,1
Tell me about some sports.,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_sports():
    url = ""https://odds-api1.p.rapidapi.com/sports""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""odds-api1.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def get_sports_list():
    url = ""https://global-data.p.rapidapi.com/sport/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""global-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def merge_sports_data():
    sports_data = get_sports()
    sports_list_data = get_sports_list()

    merged_data = []

    # Convert sports_data to a list of dictionaries
    sports_list = [v for k, v in sports_data.items() if k.isdigit()]

    # Merge data from both sources
    for sport in sports_list:
        sport_name = sport.get('name')
        sport_slug = sport.get('slug')

        # Find matching data in sports_list_data based on sport name
        matching_sport = next((item for item in sports_list_data if item['name'].lower() == sport_name.lower()), None)

        if matching_sport:
            merged_data.append({
                ""Full Name"": matching_sport.get('name'),
                ""Slug"": sport_slug,
                ""Short Name"": matching_sport.get('shortName', 'N/A'),
                ""Sport ID"": matching_sport.get('id')
            })
        else:
            # If no match found, use available data
            merged_data.append({
                ""Full Name"": sport_name,
                ""Slug"": sport_slug,
                ""Short Name"": ""N/A"",
                ""Sport ID"": ""Not Available""
            })

    return merged_data

def display_user_friendly_data(data):
    if not data:
        print(""No data to display."")
        return

    for entry in data[:10]:
        print(""==================================="")
        print(f""Full Name: {entry['Full Name']}"")
        print(f""Slug: {entry['Slug']}"")
        print(f""Short Name: {entry['Short Name']}"")
        print(f""Sport ID: {entry['Sport ID']}"")
        print(""===================================\n"")

if __name__ == '__main__':
    unified_sports_data = merge_sports_data()
    display_user_friendly_data(unified_sports_data)
",Fail,-1,-1
I want to know all about the New York Knicks team in the NBA.,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_team_details(team_id):
    url = ""https://nba-api-free-data.p.rapidapi.com/nba-team-info/v1/data""
    querystring = {""id"": team_id}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""nba-api-free-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_team_players(team_id):
    url = ""https://nba-api-free-data.p.rapidapi.com/nba-player-listing/v1/data""
    querystring = {""id"": team_id}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""nba-api-free-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def merge_team_and_player_data(team_id):
    team_details = get_team_details(team_id)
    player_details = get_team_players(team_id)

    merged_data = {
        ""Team Information"": {
            ""Name"": team_details['team'].get('displayName'),
            ""Location"": team_details['team'].get('location'),
            ""Abbreviation"": team_details['team'].get('abbreviation'),
            ""Record"": team_details['team'].get('recordSummary'),
            ""Standing"": team_details['team'].get('standingSummary'),
            ""Season"": team_details['season'].get('displayName'),
            ""Coach"": [
                {
                    ""Name"": f""{coach.get('firstName')} {coach.get('lastName')}"",
                    ""Experience"": coach.get('experience')
                } for coach in team_details.get('coach', [])
            ],
            ""Team Logo"": team_details['team'].get('logo'),
            ""Team Color"": team_details['team'].get('color')
        },
        ""Players"": []
    }

    # Add player details if available
    if 'athletes' in player_details:
        for player in player_details['athletes']:
            player_info = {
                ""Full Name"": player.get('fullName'),
                ""Position"": player.get('position', {}).get('displayName', 'N/A'),
                ""Jersey Number"": player.get('jersey', 'N/A'),
                ""Height"": player.get('displayHeight', 'N/A'),
                ""Weight"": player.get('displayWeight', 'N/A'),
                ""Age"": player.get('age', 'N/A'),
                ""Experience"": player.get('experience', {}).get('years', 'N/A'),
                ""Contract Status"": {
                    ""Active"": player.get('contract', {}).get('active', False),
                    ""Salary"": player.get('contract', {}).get('salary', 'N/A'),
                    ""Years Remaining"": player.get('contract', {}).get('yearsRemaining', 'N/A')
                },
                ""Headshot"": player.get('headshot', {}).get('href', 'N/A')
            }
            merged_data['Players'].append(player_info)

    return merged_data

def display_user_friendly_data(data):
    if not data:
        print(""No data to display."")
        return

    print(""==================================="")
    print(""Team Information:"")
    print(f""Name: {data['Team Information']['Name']}"")
    print(f""Location: {data['Team Information']['Location']}"")
    print(f""Abbreviation: {data['Team Information']['Abbreviation']}"")
    print(f""Record: {data['Team Information']['Record']}"")
    print(f""Standing: {data['Team Information']['Standing']}"")
    print(f""Season: {data['Team Information']['Season']}"")
    print(""Coach Information:"")
    for coach in data['Team Information']['Coach']:
        print(f""  - Name: {coach['Name']}, Experience: {coach['Experience']} years"")
    print(f""Team Logo: {data['Team Information']['Team Logo']}"")
    print(f""Team Color: {data['Team Information']['Team Color']}"")
    print(""\nPlayers:"")
    for player in data['Players']:
        print(""-----------------------------------"")
        print(f""Full Name: {player['Full Name']}"")
        print(f""Position: {player['Position']}"")
        print(f""Jersey Number: {player['Jersey Number']}"")
        print(f""Height: {player['Height']}"")
        print(f""Weight: {player['Weight']}"")
        print(f""Age: {player['Age']}"")
        print(f""Experience: {player['Experience']} years"")
        print(""Contract Status:"")
        print(f""  - Active: {player['Contract Status']['Active']}"")
        print(f""  - Salary: {player['Contract Status']['Salary']}"")
        print(f""  - Years Remaining: {player['Contract Status']['Years Remaining']}"")
        print(f""Headshot URL: {player['Headshot']}"")
        print(""-----------------------------------"")
    print(""==================================="")

if __name__ == '__main__':
    team_id = ""18""  # Example Team ID
    unified_team_data = merge_team_and_player_data(team_id)
    display_user_friendly_data(unified_team_data)
","===================================
Team Information:
Name: New York Knicks
Location: New York
Abbreviation: NY
Record: 1-1
Standing: 2nd in Atlantic Division
Season: 2024-25
Coach Information:
  - Name: Tom Thibodeau, Experience: 12 years
Team Logo: https://a.espncdn.com/i/teamlogos/nba/500/ny.png
Team Color: 1d428a

Players:
-----------------------------------
Full Name: Precious Achiuwa
Position: Power Forward
Jersey Number: 5
Height: 6' 8""
Weight: 243 lbs
Age: 25
Experience: 4 years
Contract Status:
  - Active: True
  - Salary: 6000000
  - Years Remaining: 1
Headshot URL: https://a.espncdn.com/i/headshots/nba/players/full/4431679.png
-----------------------------------
-----------------------------------
Full Name: OG Anunoby
Position: Small Forward
Jersey Number: 8
Height: 6' 7""
Weight: 240 lbs
Age: 27
Experience: 7 years
Contract Status:
  - Active: True
  - Salary: 36637931
  - Years Remaining: 5
Headshot URL: https://a.espncdn.com/i/headshots/nba/players/full/3934719.png
-----------------------------------
-----------------------------------
Full Name: Mikal Bridges
Position: Small Forward
Jersey Number: 25
Height: 6' 6""
Weight: 209 lbs
Age: 28
Experience: 6 years
Contract Status:
  - Active: True
  - Salary: 23300000
  - Years Remaining: 2
Headshot URL: https://a.espncdn.com/i/headshots/nba/players/full/3147657.png
-----------------------------------
-----------------------------------
Full Name: Jalen Brunson
Position: Point Guard
Jersey Number: 11
Height: 6' 2""
Weight: 190 lbs
Age: 28
Experience: 6 years
Contract Status:
  - Active: True
  - Salary: 24960001
  - Years Remaining: 5
Headshot URL: https://a.espncdn.com/i/headshots/nba/players/full/3934672.png
-----------------------------------
-----------------------------------
Full Name: Pacome Dadiet
Position: Guard
Jersey Number: 4
Height: 6' 8""
Weight: 210 lbs
Age: 19
Experience: 0 years
Contract Status:
  - Active: True
  - Salary: 1808080
  - Years Remaining: 4
Headshot URL: https://a.espncdn.com/i/headshots/nba/players/full/5211983.png
-----------------------------------
-----------------------------------
Full Name: Josh Hart
Position: Shooting Guard
Jersey Number: 3
Height: 6' 4""
Weight: 215 lbs
Age: 29
Experience: 7 years
Contract Status:
  - Active: True
  - Salary: 18144000
  - Years Remaining: 4
Headshot URL: https://a.espncdn.com/i/headshots/nba/players/full/3062679.png
-----------------------------------
-----------------------------------
Full Name: Ariel Hukporti
Position: Center
Jersey Number: 55
Height: 6' 11""
Weight: 246 lbs
Age: 22
Experience: 0 years
Contract Status:
  - Active: True
  - Salary: 0
  - Years Remaining: 1
Headshot URL: https://a.espncdn.com/i/headshots/nba/players/full/4871141.png
-----------------------------------
-----------------------------------
Full Name: Tyler Kolek
Position: Guard
Jersey Number: 13
Height: 6' 3""
Weight: 195 lbs
Age: 23
Experience: 0 years
Contract Status:
  - Active: True
  - Salary: 2087519
  - Years Remaining: 4
Headshot URL: https://a.espncdn.com/i/headshots/nba/players/full/4433225.png
-----------------------------------
-----------------------------------
Full Name: Miles McBride
Position: Point Guard
Jersey Number: 2
Height: 6' 1""
Weight: 195 lbs
Age: 24
Experience: 3 years
Contract Status:
  - Active: True
  - Salary: 4710144
  - Years Remaining: 3
Headshot URL: https://a.espncdn.com/i/headshots/nba/players/full/4431823.png
-----------------------------------
-----------------------------------
Full Name: Kevin McCullar Jr.
Position: Forward
Jersey Number: 9
Height: 6' 6""
Weight: 210 lbs
Age: 23
Experience: 0 years
Contract Status:
  - Active: True
  - Salary: 0
  - Years Remaining: 1
Headshot URL: https://a.espncdn.com/i/headshots/nba/players/full/4411057.png
-----------------------------------
-----------------------------------
Full Name: Cameron Payne
Position: Point Guard
Jersey Number: 1
Height: 6' 3""
Weight: 183 lbs
Age: 30
Experience: 9 years
Contract Status:
  - Active: True
  - Salary: 2087519
  - Years Remaining: 1
Headshot URL: https://a.espncdn.com/i/headshots/nba/players/full/3064230.png
-----------------------------------
-----------------------------------
Full Name: Mitchell Robinson
Position: Center
Jersey Number: 23
Height: 7' 0""
Weight: 240 lbs
Age: 26
Experience: 6 years
Contract Status:
  - Active: True
  - Salary: 14318182
  - Years Remaining: 2
Headshot URL: https://a.espncdn.com/i/headshots/nba/players/full/4351852.png
-----------------------------------
-----------------------------------
Full Name: Jericho Sims
Position: Center
Jersey Number: 20
Height: 6' 10""
Weight: 250 lbs
Age: 26
Experience: 3 years
Contract Status:
  - Active: True
  - Salary: 2092344
  - Years Remaining: 1
Headshot URL: https://a.espncdn.com/i/headshots/nba/players/full/4277922.png
-----------------------------------
-----------------------------------
Full Name: Jacob Toppin
Position: Forward
Jersey Number: 00
Height: 6' 8""
Weight: 200 lbs
Age: 24
Experience: 1 years
Contract Status:
  - Active: True
  - Salary: 0
  - Years Remaining: 1
Headshot URL: https://a.espncdn.com/i/headshots/nba/players/full/4592461.png
-----------------------------------
-----------------------------------
Full Name: Karl-Anthony Towns
Position: Center
Jersey Number: 32
Height: 7' 0""
Weight: 248 lbs
Age: 28
Experience: 9 years
Contract Status:
  - Active: True
  - Salary: 49205800
  - Years Remaining: 4
Headshot URL: https://a.espncdn.com/i/headshots/nba/players/full/3136195.png
-----------------------------------
===================================",1,1
I want logos for all NBA teams.,"import requests
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
def get_all_nba_teams():
    url = ""https://nba-api-free-data.p.rapidapi.com/nba-team-list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""nba-api-free-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def get_team_logo(team_id):
    url = ""https://nba-api-free-data.p.rapidapi.com/football-team-logo""
    querystring = {""teamid"": team_id}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""nba-api-free-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

if __name__ == '__main__':
    # Step 1: Get all NBA teams
    nba_teams = get_all_nba_teams()
    teams = nba_teams.get('teams', [])

    # Step 2: For each team, get the logo and display it along with the team details
    for team in teams:
        team_id = team['id']
        team_logo_data = get_team_logo(team_id)

        # Extract the logo URL if available
        if team_logo_data.get('status') == 'success':
            team_logo_url = team_logo_data['response']['url']
        else:
            team_logo_url = ""No logo available""

        # Print team information including the logo URL
        print(f""Team Name: {team['displayName']}"")
        print(f""Abbreviation: {team['abbreviation']}"")
        print(f""Location: {team['location']}"")
        print(f""Logo URL: {team_logo_url}\n"")
",Fail,-1,-1
I would like to know holiday information in Brazil,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
def get_country_list():
    url = ""https://global-data.p.rapidapi.com/country/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""global-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()


def get_city_holidays(cidade, estado, ano):
    url = ""https://feriados-brasileiros1.p.rapidapi.com/read""
    querystring = {""cidade"": cidade, ""estado"": estado, ""ano"": ano}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""feriados-brasileiros1.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


if __name__ == '__main__':
    # Step 1: Get the list of countries
    country_list = get_country_list()

    # Step 2: Print out the country list for user to identify if Brazil is present
    brazil_id = None
    for country in country_list:
        country_name = country.get(""name"")
        country_short_name = country.get(""shortName"")
        country_id = country.get(""id"")

        # Checking if the country is Brazil
        if country_name.lower() == ""brazil"":
            brazil_id = country_id


    # Step 3: If Brazil is in the list, proceed to get holidays for a city in Brazil
    if brazil_id is not None:
        # For demonstration purposes, let's assume we want holidays for Belo Horizonte, Minas Gerais, in 2023
        cidade = ""belo_horizonte""
        estado = ""MG""
        ano = ""2023""
        holidays = get_city_holidays(cidade, estado, ano)

        # Step 4: Display the holidays in a user-friendly format
        print(""\n*** Holidays in Belo Horizonte, Minas Gerais, Brazil for 2023 ***\n"")
        for holiday in holidays:
            holiday_date = holiday.get(""data"")
            holiday_name = holiday.get(""feriado"")
            holiday_type = holiday.get(""tipo"")
            holiday_type_desc = ""Mandatory Holiday"" if holiday_type == ""feriado"" else ""Optional Holiday""
            print(f""Date: {holiday_date} - Holiday: {holiday_name} ({holiday_type_desc})"")

        print(""\n*** End of Holidays Information ***"")
    else:
        print(""\nBrazil is not in the country list. Unable to retrieve holiday information.\n"")
","*** Holidays in Belo Horizonte, Minas Gerais, Brazil for 2023 ***

Date: 01/01/2023 - Holiday: Ano Novo (Mandatory Holiday)
Date: 07/04/2023 - Holiday: Sexta-Feira Santa (Mandatory Holiday)
Date: 07/04/2023 - Holiday: Sexta-feira Santa (Mandatory Holiday)
Date: 21/04/2023 - Holiday: Dia de Tiradentes (Mandatory Holiday)
Date: 01/05/2023 - Holiday: Dia do Trabalho (Mandatory Holiday)
Date: 08/06/2023 - Holiday: Corpus Christi (Mandatory Holiday)
Date: 15/08/2023 - Holiday: Assuncao de Nossa Senhora (Mandatory Holiday)
Date: 07/09/2023 - Holiday: Independencia do Brasil (Mandatory Holiday)
Date: 12/10/2023 - Holiday: Nossa Senhora Aparecida (Mandatory Holiday)
Date: 02/11/2023 - Holiday: Dia de Finados (Mandatory Holiday)
Date: 15/11/2023 - Holiday: Proclamacao da Republica (Mandatory Holiday)
Date: 08/12/2023 - Holiday: Imaculada Conceicao (Mandatory Holiday)
Date: 25/12/2023 - Holiday: Natal (Mandatory Holiday)
Date: 20/02/2023 - Holiday: Carnaval (Optional Holiday)
Date: 21/02/2023 - Holiday: Carnaval (Optional Holiday)
Date: 21/02/2023 - Holiday: Terca de Carnaval (Optional Holiday)
Date: 22/02/2023 - Holiday: Carnaval (Optional Holiday)
Date: 08/06/2023 - Holiday: Corpus Christi (Optional Holiday)
Date: 15/10/2023 - Holiday: Dia do Professor (Optional Holiday)
Date: 28/10/2023 - Holiday: Dia do Servidor Publico (Optional Holiday)

*** End of Holidays Information ***",1,1
I would like to know comprehensive information about airports worldwide and the corresponding airline logos.,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def get_airports_list():
    """"""
    Fetches a comprehensive list of airports worldwide, including details like location, timezone, and country.

    Returns:
        JSON response containing the list of airports and relevant information.
    """"""
    url = ""https://flight-radar1.p.rapidapi.com/airports/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""flight-radar1.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers)
    return response.json()


def get_airline_logos():
    """"""
    Retrieves detailed information about airline logos, including metadata and URLs for accessing the logos.

    Returns:
        JSON response containing airline logos metadata.
    """"""
    url = ""https://flight-radar1.p.rapidapi.com/airlines/get-logos""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""flight-radar1.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers)
    return response.json()


def display_airports_and_logos(airports_data, logos_data):
    """"""
    Displays a user-friendly view of airports and associated airline logos.

    Parameters:
        airports_data (dict): JSON response containing the list of airports.
        logos_data (dict): JSON response containing the list of airline logos.
    """"""
    if 'rows' not in airports_data or not airports_data['rows']:
        print(""No airport data available."")
        return

    print(""\nList of Airports:"")
    print(""-"" * 60)
    for airport in airports_data['rows'][:10]:  # Limit to 10 airports for display
        print(f""Airport: {airport['name']} ({airport['iata']}/{airport['icao']})"")
        print(f""Location: {airport['city']}, {airport['country']}"")
        print(f""Coordinates: ({airport['lat']}, {airport['lon']})"")
        print(f""Timezone: {airport['timezone']['name']} (UTC {airport['timezone']['offsetHours']})"")
        print(""-"" * 60)

    print(""\nAirline Logos:"")
    print(""-"" * 60)
    airlines = logos_data.get('result', {}).get('response', {}).get('airlines', {}).get('logotypes', [])
    for airline in airlines[:10]:  # Limit to 10 logos for display
        logo = airline['file']
        print(f""Airline Logo: {logo['name']}"")
        print(f""Logo URL: {logo['url']}"")
        print(""-"" * 60)


if __name__ == '__main__':
    # Step 1: Retrieve the list of worldwide airports
    print(""Fetching the list of worldwide airports..."")
    airports_data = get_airports_list()

    # Step 2: Retrieve the airline logos information
    print(""Fetching airline logos..."")
    logos_data = get_airline_logos()

    # Step 3: Display the airports and airline logos in a user-friendly way
    display_airports_and_logos(airports_data, logos_data)
","List of Airports:
------------------------------------------------------------
Airport: A Coruna Airport (LCG/LECO)
Location: A Coruna, Spain
Coordinates: (43.302059, -8.37725)
Timezone: Europe/Madrid (UTC 2:00)
------------------------------------------------------------
Airport: Aachen Merzbruck Airport (AAH/EDKA)
Location: Aachen, Germany
Coordinates: (50.821899, 6.184759)
Timezone: Europe/Berlin (UTC 2:00)
------------------------------------------------------------
Airport: Aalborg Airport (AAL/EKYT)
Location: Aalborg, Denmark
Coordinates: (57.095112, 9.855172)
Timezone: Europe/Copenhagen (UTC 2:00)
------------------------------------------------------------
Airport: Aarhus Airport (AAR/EKAH)
Location: Aarhus, Denmark
Coordinates: (56.303295, 10.619129)
Timezone: Europe/Copenhagen (UTC 2:00)
------------------------------------------------------------
Airport: Aarhus Sea Airport (QEA/EKAC)
Location: Aarhus, Denmark
Coordinates: (56.151993, 10.247725)
Timezone: Europe/Copenhagen (UTC 2:00)
------------------------------------------------------------
Airport: Aasiaat Airport (JEG/BGAA)
Location: Aasiaat, Greenland
Coordinates: (68.72184, -52.784698)
Timezone: America/Godthab (UTC -1:00)
------------------------------------------------------------
Airport: Abadan Airport (ABD/OIAA)
Location: Abadan, Iran
Coordinates: (30.371111, 48.228329)
Timezone: Asia/Tehran (UTC 3:30)
------------------------------------------------------------
Airport: Abakan International Airport (ABA/UNAA)
Location: Abakan, Russia
Coordinates: (53.740002, 91.385002)
Timezone: Asia/Krasnoyarsk (UTC 7:00)
------------------------------------------------------------
Airport: Abbotsford International Airport (YXX/CYXX)
Location: Abbotsford, Canada
Coordinates: (49.025269, -122.360001)
Timezone: America/Vancouver (UTC -7:00)
------------------------------------------------------------
Airport: Abemama Airport (AEA/NGTB)
Location: Abemama Atoll, Kiribati
Coordinates: (0.490833, 173.828598)
Timezone: Pacific/Tarawa (UTC 12:00)
------------------------------------------------------------

Airline Logos:
------------------------------------------------------------
Airline Logo: A0_EFW.png
Logo URL: https://s3.eu-central-1.amazonaws.com/images.flightradar24.com/assets/airlines/logotypes/A0_EFW.png?_=1678974401
------------------------------------------------------------
Airline Logo: A3_AEE.png
Logo URL: https://s3.eu-central-1.amazonaws.com/images.flightradar24.com/assets/airlines/logotypes/A3_AEE.png?_=1652880694
------------------------------------------------------------
Airline Logo: A6_OTC.png
Logo URL: https://s3.eu-central-1.amazonaws.com/images.flightradar24.com/assets/airlines/logotypes/A6_OTC.png?_=1699531755
------------------------------------------------------------
Airline Logo: AB_ABC.png
Logo URL: https://s3.eu-central-1.amazonaws.com/images.flightradar24.com/assets/airlines/logotypes/AB_ABC.png?_=1699444240
------------------------------------------------------------
Airline Logo: AB_BNZ.png
Logo URL: https://s3.eu-central-1.amazonaws.com/images.flightradar24.com/assets/airlines/logotypes/AB_BNZ.png?_=1678972803
------------------------------------------------------------
Airline Logo: AE_MDA.png
Logo URL: https://s3.eu-central-1.amazonaws.com/images.flightradar24.com/assets/airlines/logotypes/AE_MDA.png?_=1699644572
------------------------------------------------------------
Airline Logo: AI_AIC.png
Logo URL: https://s3.eu-central-1.amazonaws.com/images.flightradar24.com/assets/airlines/logotypes/AI_AIC.png?_=1699532075
------------------------------------------------------------
Airline Logo: AJ_AZY.png
Logo URL: https://s3.eu-central-1.amazonaws.com/images.flightradar24.com/assets/airlines/logotypes/AJ_AZY.png?_=1653302677
------------------------------------------------------------
Airline Logo: AK_AXM.png
Logo URL: https://s3.eu-central-1.amazonaws.com/images.flightradar24.com/assets/airlines/logotypes/AK_AXM.png?_=1652883979
------------------------------------------------------------
Airline Logo: AL_MAY.png
Logo URL: https://s3.eu-central-1.amazonaws.com/images.flightradar24.com/assets/airlines/logotypes/AL_MAY.png?_=1643833358
------------------------------------------------------------
",1,1
I would like to know the vehicle information with trim id 3421.,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def get_trim_details(trim_id):
    """"""
    Retrieves comprehensive vehicle specifications including pricing, mileage, engine details, and make/model information.

    Parameters:
        trim_id (int): The unique identifier for the vehicle trim.

    Returns:
        JSON response containing detailed vehicle specifications.
    """"""
    url = f""https://car-api2.p.rapidapi.com/api/trims/{trim_id}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""car-api2.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers)
    return response.json()


def get_body_details(make_model_trim_id):
    """"""
    Retrieves detailed information about the vehicle body, including dimensions, weight, and cargo capacity.

    Parameters:
        make_model_trim_id (int): The unique identifier for the vehicle make, model, and trim.

    Returns:
        JSON response containing detailed body specifications of the vehicle.
    """"""
    url = ""https://car-api2.p.rapidapi.com/api/bodies""
    querystring = {
        ""make_model_trim_id"": str(make_model_trim_id),
        ""verbose"": ""yes"",
        ""sort"": ""id"",
        ""direction"": ""asc""
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""car-api2.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def display_vehicle_info(trim_data, body_data):
    """"""
    Displays detailed information of the vehicle including general specifications and body details.

    Parameters:
        trim_data (dict): JSON response data containing vehicle trim information.
        body_data (dict): JSON response data containing vehicle body information.
    """"""
    print(""\nVehicle Specifications:"")
    print(""-"" * 60)
    print(f""Vehicle Name: {trim_data.get('name')}"")
    print(f""Year: {trim_data.get('year')}"")
    print(f""MSRP: ${trim_data.get('msrp')}"")
    print(f""Description: {trim_data.get('description')}"")

    engine = trim_data.get('make_model_trim_engine', {})
    if engine:
        print(""\nEngine Specifications:"")
        print(f""Engine Type: {engine.get('engine_type')}"")
        print(f""Fuel Type: {engine.get('fuel_type')}"")
        print(f""Horsepower: {engine.get('horsepower_hp')} HP"")
        print(f""Torque: {engine.get('torque_ft_lbs')} ft-lbs"")

    if body_data and 'data' in body_data and len(body_data['data']) > 0:
        body = body_data['data'][0]
        print(""\nBody Specifications:"")
        print(f""Type: {body.get('type')}"")
        print(f""Doors: {body.get('doors')}"")
        print(f""Length: {body.get('length')}"")
        print(f""Width: {body.get('width')}"")
        print(f""Height: {body.get('height')}"")
        print(f""Seats: {body.get('seats')}"")
        print(f""Cargo Capacity: {body.get('cargo_capacity')}"")
        print(f""Curb Weight: {body.get('curb_weight')} lbs"")
        print(""-"" * 60)
    else:
        print(""No body specifications available for this vehicle."")
    print(""-"" * 60)


if __name__ == '__main__':
    # Step 1: Retrieve general vehicle specifications using the Trim View API
    trim_id = ""3421""

    if trim_id.isdigit():
        trim_data = get_trim_details(int(trim_id))

        # Step 2: Use make_model_trim_id to get detailed body specifications
        make_model_trim_id = trim_data.get('make_model_id')
        if make_model_trim_id:
            body_data = get_body_details(make_model_trim_id)

            # Step 3: Display the vehicle and body information
            display_vehicle_info(trim_data, body_data)
        else:
            print(""Invalid or missing Make/Model/Trim ID."")
    else:
        print(""Invalid Trim ID. Please enter a numeric value."")
","Vehicle Specifications:
------------------------------------------------------------
Vehicle Name: * (hidden)
Year: 2022
MSRP: $27990
Description: ********************************** (hidden)

Engine Specifications:
Engine Type: *** (engine type hidden)
Fuel Type: *** (fuel_type hidden)
Horsepower: 184 HP
Torque: 221 ft-lbs

Body Specifications:
Type: *** (body type hidden)
Doors: 5
Length: 169.7
Width: 71.7
Height: 61.3
Seats: 5
Cargo Capacity: 17.6
Curb Weight: 3618 lbs
------------------------------------------------------------
------------------------------------------------------------",1,1
Give me some information about the airport.,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_airports_list():
    """"""
    Fetches a list of airports with detailed information including names, codes, location, altitude, and size.

    Returns:
        JSON response containing the list of airports.
    """"""
    url = ""https://flightradar24-com.p.rapidapi.com/v2/airports/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""flightradar24-com.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers)
    return response.json()

def get_airlines_list():
    """"""
    Retrieves a list of airlines, including their codes, status, and logo information.

    Returns:
        JSON response containing the list of airlines and their metadata.
    """"""
    url = ""https://flightradar24-com.p.rapidapi.com/v2/airlines/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""flightradar24-com.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers)
    return response.json()

def display_airports_and_airlines(airports_data, airlines_data):
    """"""
    Displays a user-friendly list of airports and associated airlines, including airline logos.

    Parameters:
        airports_data (dict): JSON response containing airport information.
        airlines_data (dict): JSON response containing airline information.
    """"""
    if 'rows' not in airports_data or not airports_data['rows']:
        print(""No airport data available."")
        return

    if 'rows' not in airlines_data or not airlines_data['rows']:
        print(""No airline data available."")
        return

    print(""\nList of Airports:"")
    print(""-"" * 60)
    for airport in airports_data['rows'][:10]:  # Displaying the first 10 airports for clarity
        print(f""Airport: {airport['name']} ({airport['iata']}/{airport['icao']})"")
        print(f""Location: {airport['city']}, {airport['country']}"")
        print(f""Coordinates: ({airport['lat']}, {airport['lon']})"")
        print(f""Altitude: {airport['alt']} ft, Size: {airport['size']}"")
        print(""-"" * 60)

    print(""\nList of Airlines with Logos:"")
    print(""-"" * 60)
    for airline in airlines_data['rows'][:10]:  # Displaying the first 10 airlines for clarity
        print(f""Airline: {airline['name']} ({airline['iata']}/{airline['icao']})"")
        if airline['metadata'] and airline['metadata']['images'] and airline['metadata']['images']['logotypes']:
            logo_info = airline['metadata']['images']['logotypes']['default']['file']
            print(f""Logo Name: {logo_info['name']}"")
            print(f""Logo URL: {logo_info['source']}"")
        else:
            print(""No logo information available."")
        print(""-"" * 60)

if __name__ == '__main__':
    # Step 1: Retrieve the list of worldwide airports
    print(""Fetching the list of worldwide airports..."")
    airports_data = get_airports_list()

    # Step 2: Retrieve the list of airlines
    print(""Fetching the list of airlines and their logos..."")
    airlines_data = get_airlines_list()

    # Step 3: Display the airports and airlines with their logos in a user-friendly way
    display_airports_and_airlines(airports_data, airlines_data)
",Fail,-1,-1
Give me some information about the airline.,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def get_basic_airlines_list():
    """"""
    Retrieves a basic list of airlines, including their names, codes, and ICAO codes.

    Returns:
        JSON response containing the list of basic airline data.
    """"""
    url = ""https://flight-radar1.p.rapidapi.com/airlines/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""flight-radar1.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers)
    return response.json()


def get_detailed_airlines_list():
    """"""
    Retrieves a comprehensive list of airline data, including activity status, hub, IATA/ICAO codes,
    and additional metadata with logotype information.

    Returns:
        JSON response containing the detailed airline data.
    """"""
    url = ""https://flightradar24-com.p.rapidapi.com/v2/airlines/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""flightradar24-com.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers)
    return response.json()


def display_airlines_information(basic_data, detailed_data):
    """"""
    Displays a user-friendly list of airlines with both basic and detailed information, including logos.

    Parameters:
        basic_data (dict): JSON response containing basic airline data.
        detailed_data (dict): JSON response containing detailed airline data.
    """"""
    if 'rows' not in basic_data or not basic_data['rows']:
        print(""No basic airline data available."")
        return

    if 'rows' not in detailed_data or not detailed_data['rows']:
        print(""No detailed airline data available."")
        return

    print(""\nAirline List (Basic Information):"")
    print(""-"" * 60)
    for airline in basic_data['rows'][:10]:  # Display the first 10 airlines for clarity
        print(f""Airline: {airline['Name']} | Code: {airline['Code']} | ICAO: {airline['ICAO']}"")
    print(""-"" * 60)

    print(""\nAirline List (Detailed Information):"")
    print(""-"" * 60)
    for airline in detailed_data['rows'][:10]:  # Display the first 10 airlines for clarity
        print(f""Airline: {airline['name']} | IATA: {airline['iata']} | ICAO: {airline['icao']}"")
        print(f""Active: {'Yes' if airline['active'] else 'No'} | Hub: {airline['hub'] or 'N/A'}"")
        if airline['metadata'] and airline['metadata']['images'] and airline['metadata']['images']['logotypes']:
            logo_info = airline['metadata']['images']['logotypes']['default']['file']
            print(f""Logo File: {logo_info['name']}, URL: {logo_info['source']}"")
        else:
            print(""No logo information available."")
        print(""-"" * 60)


if __name__ == '__main__':
    # Step 1: Retrieve the basic airline list
    basic_data = get_basic_airlines_list()

    # Step 2: Retrieve the detailed airline list including logos
    detailed_data = get_detailed_airlines_list()

    # Step 3: Display the combined airline information in a user-friendly format
    display_airlines_information(basic_data, detailed_data)
","Airline List (Basic Information):
------------------------------------------------------------
Airline: 21 Air | Code: 2I | ICAO: CSB
Airline: 247 Aviation | Code:  | ICAO: EMC
Airline: 2Excel Aviation | Code:  | ICAO: BRO
Airline: 4 Airways | Code:  | ICAO: DAK
Airline: 40-Mile Air | Code: Q5 | ICAO: MLA
Airline: 748 Air Services | Code: FE | ICAO: IHO
Airline: 9 Air | Code: AQ | ICAO: JYH
Airline: Abakan Air | Code: S5 | ICAO: NKP
Airline: ABS Jets | Code:  | ICAO: ABP
Airline: Abu Dhabi Aviation | Code:  | ICAO: BAR
------------------------------------------------------------

Airline List (Detailed Information):
------------------------------------------------------------
Airline: 21 Air | IATA: 2I | ICAO: CSB
Active: Yes | Hub: N/A
Logo File: 1545.png, URL: https://images.flightradar24.com/assets/airlines/logotypes/1545.png
------------------------------------------------------------
Airline: 247 Aviation | IATA: None | ICAO: EMC
Active: Yes | Hub: N/A
Logo File: 2566.png, URL: https://images.flightradar24.com/assets/airlines/logotypes/2566.png
------------------------------------------------------------
Airline: 2Excel Aviation | IATA: None | ICAO: BRO
Active: Yes | Hub: N/A
Logo File: 1914.png, URL: https://images.flightradar24.com/assets/airlines/logotypes/1914.png
------------------------------------------------------------
Airline: 4 Airways | IATA: None | ICAO: DAK
Active: Yes | Hub: N/A
No logo information available.
------------------------------------------------------------
Airline: 40-Mile Air | IATA: Q5 | ICAO: MLA
Active: Yes | Hub: N/A
Logo File: MLA_logo0.png, URL: https://www.flightradar24.com/static/images/data/operators/MLA_logo0.png
------------------------------------------------------------
Airline: 748 Air Services | IATA: FE | ICAO: IHO
Active: Yes | Hub: N/A
Logo File: 1757.png, URL: https://images.flightradar24.com/assets/airlines/logotypes/1757.png
------------------------------------------------------------
Airline: 9 Air | IATA: AQ | ICAO: JYH
Active: Yes | Hub: N/A
Logo File: 280.png, URL: https://images.flightradar24.com/assets/airlines/logotypes/280.png
------------------------------------------------------------
Airline: Abakan Air | IATA: S5 | ICAO: NKP
Active: Yes | Hub: N/A
No logo information available.
------------------------------------------------------------
Airline: ABS Jets | IATA: None | ICAO: ABP
Active: Yes | Hub: N/A
No logo information available.
------------------------------------------------------------
Airline: Abu Dhabi Aviation | IATA: None | ICAO: BAR
Active: Yes | Hub: N/A
No logo information available.
------------------------------------------------------------",1,1
I would like to know more about United States Airlines.,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def search_flights(query, limit=25):
    """"""
    Searches for flights using a query, which can be an aircraft number, flight code, or airline code.

    Parameters:
        query (str): The search query, e.g., flight number or airline code.
        limit (int): The number of items per response, default is 25.

    Returns:
        JSON response containing a list of matching flight details.
    """"""
    url = ""https://flightradar24-com.p.rapidapi.com/v2/flights/search""
    querystring = {""query"": query, ""limit"": limit}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""flightradar24-com.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def get_flight_details(flight_id):
    """"""
    Retrieves detailed information about a specific flight.

    Parameters:
        flight_id (str): The unique identifier of the flight.

    Returns:
        JSON response containing detailed flight information.
    """"""
    url = ""https://flightradar24-com.p.rapidapi.com/v2/flights/detail""
    querystring = {""flight"": flight_id}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""flightradar24-com.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def display_search_results(search_data):
    """"""
    Displays the flight search results in a user-friendly way.

    Parameters:
        search_data (dict): JSON response containing flight search results.
    """"""
    if 'results' not in search_data or not search_data['results']:
        print(""No flight search results available."")
        return

    print(""\nFlight Search Results:"")
    print(""-"" * 60)
    for result in search_data['results'][:10]:  # Display the first 10 results for clarity
        flight_detail = result.get('detail', {})
        print(f""Flight Name: {result.get('name')}"")
        print(f""Flight Number: {flight_detail.get('flight', 'N/A')}"")
        print(f""Operator: {flight_detail.get('operator', 'N/A')}"")
        print(f""Logo URL: {flight_detail.get('logo', 'No logo available')}"")
        print(f""Callsign: {flight_detail.get('callsign', 'N/A')}"")
        print(""-"" * 60)


def display_flight_details(flight_data):
    """"""
    Displays detailed information of a specific flight.

    Parameters:
        flight_data (dict): JSON response containing detailed flight information.
    """"""
    print(""\nDetailed Flight Information:"")
    print(""-"" * 60)
    print(f""Flight ID: {flight_data.get('id')}"")
    print(f""Flight Name: {flight_data.get('name')}"")
    print(f""Created At: {flight_data.get('created_at')}"")
    print(f""Updated At: {flight_data.get('updated_at')}"")

    details = flight_data.get('details', {})
    print(f""Description: {details.get('description', 'No description available')}"")
    print(f""Status: {details.get('status', 'N/A')}"")

    tags = flight_data.get('tags', [])
    if tags:
        print(f""Tags: {', '.join(tags)}"")
    else:
        print(""No tags associated."")
    print(""-"" * 60)


if __name__ == '__main__':
    # Step 1: Search for flights using a specific query
    search_query = ""AA""

    print(f""\nSearching for flights with query '{search_query}'..."")
    search_results = search_flights(search_query)

    # Step 2: Display the search results
    display_search_results(search_results)

    # Step 3: Ask the user to choose a flight for detailed information
    selected_flight_id = ""AA""

    # Step 4: Retrieve and display detailed information for the selected flight
    if selected_flight_id:
        print(f""\nFetching detailed information for Flight ID {selected_flight_id}..."")
        flight_details = get_flight_details(selected_flight_id)
        display_flight_details(flight_details)
    else:
        print(""No Flight ID entered. Exiting."")
","Flight Search Results:
------------------------------------------------------------
Flight Name: American Airlines
Flight Number: N/A
Operator: N/A
Logo URL: https://images.flightradar24.com/assets/airlines/logotypes/30.png
Callsign: N/A
------------------------------------------------------------

Fetching detailed information for Flight ID AA...

Detailed Flight Information:
------------------------------------------------------------
Flight ID: None
Flight Name: None
Created At: None
Updated At: None
Description: No description available
Status: N/A
No tags associated.
------------------------------------------------------------",1,1
I would like to know New York city and airport information.,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def search_airport(query, locale='en-US'):
    """"""
    Searches for airports and related cities based on a specified location query.

    Parameters:
        query (str): The name of the location to search for airports.
        locale (str): The language code for localization, default is 'en-US'.

    Returns:
        JSON response containing a list of airports and related cities.
    """"""
    url = ""https://sky-scrapper.p.rapidapi.com/api/v1/flights/searchAirport""
    querystring = {""query"": query, ""locale"": locale}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""sky-scrapper.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def search_location(query):
    """"""
    Searches for cities and airports based on a specified location query, returning hierarchical information.

    Parameters:
        query (str): The location name to search for, such as a city or airport.

    Returns:
        JSON response containing a list of cities and airports with hierarchical information.
    """"""
    url = ""https://sky-scrapper.p.rapidapi.com/api/v1/cars/searchLocation""
    querystring = {""query"": query}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""sky-scrapper.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def display_airport_search_results(search_data):
    """"""
    Displays the search results for airports and related cities in a user-friendly format.

    Parameters:
        search_data (dict): JSON response containing search results for airports and cities.
    """"""
    if 'data' not in search_data or not search_data['data']:
        print(""No airport data available."")
        return

    print(""\nAirport and City Search Results:"")
    print(""-"" * 60)
    for entry in search_data['data'][:10]:  # Display the first 10 results for clarity
        presentation = entry.get('presentation', {})
        navigation = entry.get('navigation', {})
        print(f""Title: {presentation.get('title')}"")
        print(f""Suggestion: {presentation.get('suggestionTitle')}"")
        print(f""Subtitle: {presentation.get('subtitle')}"")
        print(f""Entity Type: {navigation.get('entityType')}, Localized Name: {navigation.get('localizedName')}"")
        print(""-"" * 60)


def display_location_search_results(search_data):
    """"""
    Displays the search results for cities and airports with hierarchical information.

    Parameters:
        search_data (dict): JSON response containing search results for cities and airports.
    """"""
    if 'data' not in search_data or not search_data['data']:
        print(""No location data available."")
        return

    print(""\nHierarchical Location Search Results:"")
    print(""-"" * 60)
    for entry in search_data['data'][:10]:  # Display the first 10 results for clarity
        print(f""Entity Name: {entry['entity_name']} | Type: {entry['class']}"")
        print(f""Location Coordinates: {entry['location']}"")
        print(f""Hierarchy: {entry['hierarchy']}"")
        print(f""Entity ID: {entry['entity_id']}"")
        print(""-"" * 60)


if __name__ == '__main__':
    # Step 1: Ask the user for a location to search
    location_query = ""New York""

    # Step 2: Search for airports using the specified query
    airport_search_results = search_airport(location_query)

    # Step 3: Display the search results from the searchAirport API
    display_airport_search_results(airport_search_results)

    # Step 4: Search for cities and airports with hierarchical information using the searchLocation API
    location_search_results = search_location(location_query)

    # Step 5: Display the search results from the searchLocation API
    display_location_search_results(location_search_results)
","Airport and City Search Results:
------------------------------------------------------------
Title: New York
Suggestion: New York (Any)
Subtitle: United States
Entity Type: CITY, Localized Name: New York
------------------------------------------------------------
Title: New York John F. Kennedy
Suggestion: New York John F. Kennedy (JFK)
Subtitle: United States
Entity Type: AIRPORT, Localized Name: New York John F. Kennedy
------------------------------------------------------------
Title: New York Newark
Suggestion: New York Newark (EWR)
Subtitle: United States
Entity Type: AIRPORT, Localized Name: New York Newark
------------------------------------------------------------
Title: New York LaGuardia
Suggestion: New York LaGuardia (LGA)
Subtitle: United States
Entity Type: AIRPORT, Localized Name: New York LaGuardia
------------------------------------------------------------
Title: Stewart International
Suggestion: Stewart International (SWF)
Subtitle: United States
Entity Type: AIRPORT, Localized Name: Stewart International
------------------------------------------------------------
Title: Albany
Suggestion: Albany (ALB)
Subtitle: United States
Entity Type: AIRPORT, Localized Name: Albany
------------------------------------------------------------
Title: Buffalo Niagara
Suggestion: Buffalo Niagara (BUF)
Subtitle: United States
Entity Type: AIRPORT, Localized Name: Buffalo Niagara
------------------------------------------------------------
Title: Syracuse
Suggestion: Syracuse (SYR)
Subtitle: United States
Entity Type: AIRPORT, Localized Name: Syracuse
------------------------------------------------------------

Hierarchical Location Search Results:
------------------------------------------------------------
Entity Name: New York John F. Kennedy (JFK) | Type: Airport
Location Coordinates: 40.6412215, -73.7781176
Hierarchy: New York|New York|United States
Entity ID: 95565058
------------------------------------------------------------
Entity Name: New York | Type: City
Location Coordinates: 40.6940959901, -73.9282670243
Hierarchy: New York|United States
Entity ID: 27537542
------------------------------------------------------------
Entity Name: New York Newark (EWR) | Type: Airport
Location Coordinates: 40.695833, -74.171667
Hierarchy: New York|New York|United States
Entity ID: 95565059
------------------------------------------------------------
Entity Name: New York LaGuardia (LGA) | Type: Airport
Location Coordinates: 40.779444, -73.875833
Hierarchy: New York|New York|United States
Entity ID: 95565057
------------------------------------------------------------
Entity Name: Newyork | Type: City
Location Coordinates: 56.2499999918, -5.2833331469
Hierarchy: Argyll and Bute|Scotland|United Kingdom
Entity ID: 46422841
------------------------------------------------------------
Entity Name: New York | Type: City
Location Coordinates: 32.1679320926, -95.6691276413
Hierarchy: Henderson County|Texas|United States
Entity ID: 46834056
------------------------------------------------------------
Entity Name: New York | Type: City
Location Coordinates: 30.8385201937, -87.2008047958
Hierarchy: Santa Rosa County|Florida|United States
Entity ID: 46772434
------------------------------------------------------------
Entity Name: New York | Type: City
Location Coordinates: 53.0789712548, -0.1400758425
Hierarchy: Lincolnshire|England|United Kingdom
Entity ID: 46422842
------------------------------------------------------------
Entity Name: New York Mills | Type: City
Location Coordinates: 46.5180160916, -95.376146406
Hierarchy: Otter Tail County|Minnesota|United States
Entity ID: 46863744
------------------------------------------------------------
Entity Name: New York Mills | Type: City
Location Coordinates: 43.1053474928, -75.2912777901
Hierarchy: Oneida County|New York|United States
Entity ID: 46872082
------------------------------------------------------------",1,1
"I need detailed information about the Paris I'm searching for, including their location hierarchy and geographic coordinates. Also, I'd like to see a randomly selected popular photo related to a trip or destination with metadata such as the photo URL, the username of the uploader, and the hosting platform. Please provide at least three search results for the places I designate and always include a random photo related to your trip for inspiration.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
API_HOST_LOCATIONS = 'skyscanner80.p.rapidapi.com'
API_HOST_PHOTOS = 'nicaraguawallpaper.p.rapidapi.com'


def API_search_location(query, market='US', locale='en-US'):
    url = f""https://{API_HOST_LOCATIONS}/api/v1/cars/search-location""

    querystring = {
        ""query"": query,
        ""market"": market,
        ""locale"": locale
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST_LOCATIONS
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        data = response.json()
        if data.get('status'):
            return data['data'][:3]  # Limit to the first 3 results
        else:
            print(""Error in location search"")
    else:
        print(""Request failed for location search:"", response.status_code)

    return None


def API_get_random_photo():
    url = f""https://{API_HOST_PHOTOS}/api/getdemorandomphoto""

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST_PHOTOS
    }

    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        data = response.json()
        if data.get('success'):
            return data['dataResponse']
        else:
            print(""Error retrieving random photo"")
    else:
        print(""Request failed for random photo:"", response.status_code)

    return None


if __name__ == '__main__':
    # Example location search query
    search_query = ""Paris""

    # Retrieve detailed information for a location
    location_data = API_search_location(search_query)
    if location_data:
        print(""Search Location Results:"")
        for item in location_data:
            print(f""Entity Name: {item['entity_name']}"")
            print(f""Entity ID: {item['entity_id']}"")
            print(f""Location: {item['location']}"")
            print(f""Hierarchy: {item['hierarchy']}"")
            print(f""Class: {item['class']}"")
            print(""\n"")

    # Retrieve a random popular travel-related photo
    random_photo = API_get_random_photo()
    if random_photo:
        print(""Random Travel Photo:"")
        print(f""Photo ID: {random_photo['id']}"")
        print(f""Image URL: {random_photo['imageUrl']}"")
        print(f""Photo Link: {random_photo['photoLink']}"")
        print(f""Uploader Username: {random_photo['username']}"")
        print(f""Platform: {random_photo['platform']}"")
        print(f""Uploader Profile URL: {random_photo['profile_url']}"")
        print(""\n"")
","Search Location Results:
Entity Name: Paris Charles de Gaulle (CDG)
Entity ID: 95565041
Location: 49.0096176, 2.5481698
Hierarchy: Paris|ÃŽle-de-France|France
Class: Airport


Entity Name: Paris
Entity ID: 27539733
Location: 48.85662237511698, 2.3428759930239886
Hierarchy: ÃŽle-de-France|France
Class: City


Entity Name: Paris Orly (ORY)
Entity ID: 95565040
Location: 48.728889, 2.357222
Hierarchy: Paris|ÃŽle-de-France|France
Class: Airport


Random Travel Photo:
Photo ID: popular_02
Image URL: https://live.staticflickr.com/3670/32419322614_499fc708f0_b.jpg
Photo Link: https://flic.kr/p/RoMv9b
Uploader Username: Dieter Vynckier
Platform: Flickr
Uploader Profile URL: https://www.flickr.com/photos/dietervynckier/",1,1
"I want to know about different countries, including their market codes, currency details, locale settings, and any associated website URLs. Additionally, I'd like to see some visually appealing photos from a curated list, specifically three promoted and three popular photos, along with information about the photo's uploader and the platform where it's hosted. The results should be presented together, with a focus on countries and captivating photos related to travel.
","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
API_HOST_CONFIG = 'skyscanner80.p.rapidapi.com'
API_HOST_PHOTOS = 'nicaraguawallpaper.p.rapidapi.com'


def API_get_config():
    url = f""https://{API_HOST_CONFIG}/api/v1/get-config""

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST_CONFIG
    }

    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        data = response.json()
        if data.get('status'):
            return data['data']
        else:
            print(""Error retrieving config data:"", data.get('message'))
    else:
        print(""Request failed for config retrieval:"", response.status_code)

    return None


def API_get_demo_all_photos():
    url = f""https://{API_HOST_PHOTOS}/api/getdemoallphotos""

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST_PHOTOS
    }

    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        data = response.json()
        if data.get('success'):
            return data['dataResponse']['dataResponse']
        else:
            print(""Error retrieving photos"")
    else:
        print(""Request failed for photos retrieval:"", response.status_code)

    return None


if __name__ == '__main__':
    # Retrieve country configuration details
    config_data = API_get_config()
    if config_data:
        print(""Country Configuration Data:"")
        for country in config_data[:3]:  # Limit to the first 3 countries
            print(f""Country: {country['country']}"")
            print(f""Market Code: {country['market']}"")
            print(f""Locale: {country['locale']}"")
            print(f""Currency: {country['currency']} ({country['currencySymbol']})"")
            print(f""Website: {country['site']}"")
            print(""\n"")

    # Retrieve a list of curated promoted and popular photos
    photo_data = API_get_demo_all_photos()
    if photo_data:
        print(""Promoted Photos:"")
        for item in photo_data['promotion']['items'][:3]:  # Limit to 3 promoted photos
            print(f""Photo ID: {item['id']}"")
            print(f""Image URL: {item['imageUrl']}"")
            print(f""Photo Link: {item['photoLink']}"")
            print(f""Uploader: {item['username']}"")
            print(f""Platform: {item['platform']}"")
            print(f""Profile URL: {item['profile_url']}"")
            print(""\n"")

        print(""Popular Photos:"")
        for item in photo_data['popular']['items'][:3]:  # Limit to 3 popular photos
            print(f""Photo ID: {item['id']}"")
            print(f""Image URL: {item['imageUrl']}"")
            print(f""Photo Link: {item['photoLink']}"")
            print(f""Uploader: {item['username']}"")
            print(f""Platform: {item['platform']}"")
            print(f""Profile URL: {item['profile_url']}"")
            print(""\n"")
","Country Configuration Data:
Country: Andorra
Market Code: AD
Locale: es-ES
Currency: EUR (â‚¬)
Website: www.skyscanner.net


Country: United Arab Emirates
Market Code: AE
Locale: en-US
Currency: AED (AED)
Website: www.skyscanner.ae


Country: Afghanistan
Market Code: AF
Locale: en-GB
Currency: AFN (AFN)
Website: www.skyscanner.net


Promoted Photos:
Photo ID: promotion_01
Image URL: https://live.staticflickr.com/3681/13654859084_c3ddd8848e_o_d.jpg
Photo Link: https://flic.kr/p/mNCHSU
Uploader: Nava writz
Platform: Flickr
Profile URL: https://www.flickr.com/photos/navawritz/


Popular Photos:
Photo ID: popular_01
Image URL: https://live.staticflickr.com/4069/4467659984_c68c183360_b.jpg
Photo Link: https://flic.kr/p/7NMVEq
Uploader: Shannon
Platform: Flickr
Profile URL: https://www.flickr.com/photos/trailofdead/


Photo ID: popular_02
Image URL: https://live.staticflickr.com/3670/32419322614_499fc708f0_b.jpg
Photo Link: https://flic.kr/p/RoMv9b
Uploader: Dieter Vynckier
Platform: Flickr
Profile URL: https://www.flickr.com/photos/dietervynckier/


Photo ID: popular_03
Image URL: https://live.staticflickr.com/49/129153533_4bb1dc653b_b.jpg
Photo Link: https://flic.kr/p/cpWR4
Uploader: Daniela Cordero
Platform: Flickr
Profile URL: https://www.flickr.com/photos/danielacordero/",1,1
"I need a detailed overview of New York's airports and related cities, including their titles and navigation details such as localized names and identifiers. Also, I want to know about its available rental car options, focusing on at least three available offers with information on car types, prices, and supplier ratings. Responses should provide concise and comprehensive information to help inform travel plans, combining airport data with car rental options in the same area.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
API_HOST_AIRPORTS = 'sky-scrapper.p.rapidapi.com'
API_HOST_CARS = 'skyscanner80.p.rapidapi.com'


def API_search_airport(query, locale='en-US'):
    url = f""https://{API_HOST_AIRPORTS}/api/v1/flights/searchAirport""

    querystring = {
        ""query"": query,
        ""locale"": locale
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST_AIRPORTS
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        data = response.json()
        if data.get('status'):
            return data['data'][:3]  # Limit to the first 3 results
        else:
            print(""Error retrieving airport data"")
    else:
        print(""Request failed for airport search:"", response.status_code)

    return None


def API_search_cars(pick_up_entity_id, pick_up_date='2024-03-12', pick_up_time='10:00', currency='USD', market='US',
                    locale='en-US'):
    url = f""https://{API_HOST_CARS}/api/v1/cars/search-cars""

    querystring = {
        ""pickUpEntityId"": pick_up_entity_id,
        ""pickUpDate"": pick_up_date,
        ""pickUpTime"": pick_up_time,
        ""currency"": currency,
        ""market"": market,
        ""locale"": locale
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST_CARS
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        data = response.json()
        if data.get('status'):
            return data['data']['quotes'][:3]  # Limit to the first 3 quotes
        else:
            print(""Error retrieving car rental data"")
    else:
        print(""Request failed for car rental search:"", response.status_code)

    return None


if __name__ == '__main__':
    # Search for airports in a specific location
    search_location = ""New York""
    airport_data = API_search_airport(search_location)

    if airport_data:
        print(""Airports and Related Cities:"")
        for airport in airport_data:
            print(f""Title: {airport['presentation']['title']}"")
            print(f""Suggestion Title: {airport['presentation']['suggestionTitle']}"")
            print(f""Country: {airport['presentation']['subtitle']}"")
            print(f""Entity ID: {airport['navigation']['entityId']}"")
            print(f""Localized Name: {airport['navigation']['localizedName']}"")
            print(f""Entity Type: {airport['navigation']['entityType']}"")
            print(""\n"")

        # Assume the first airport entity for car rental search
        first_airport_id = airport_data[0]['navigation']['entityId']

        # Search for car rentals at the first found airport location
        car_quotes = API_search_cars(pick_up_entity_id=first_airport_id)

        if car_quotes:
            print(""Car Rental Options:"")
            for quote in car_quotes:
                print(f""Car Name: {quote['car_name']}"")
                print(f""Vendor: {quote['vndr']}"")
                print(f""Price: ${quote['price']}"")
                print(f""Fuel Type: {quote['fuel_type']}"")
                print(f""Unlimited Mileage: {'Yes' if quote['adds']['unlim_mlg'] else 'No'}"")
                print(f""Free Cancellation: {'Yes' if quote['adds']['free_cancel'] else 'No'}"")
                print(f""Vendor Rating: {quote['vndr_rating']['overall_rating']} / 5"")
                print(f""Deep Link: {quote['dplnk']}"")
                print(""\n"")
","Airports and Related Cities:
Title: New York
Suggestion Title: New York (Any)
Country: United States
Entity ID: 27537542
Localized Name: New York
Entity Type: CITY


Title: New York John F. Kennedy
Suggestion Title: New York John F. Kennedy (JFK)
Country: United States
Entity ID: 95565058
Localized Name: New York John F. Kennedy
Entity Type: AIRPORT


Title: New York Newark
Suggestion Title: New York Newark (EWR)
Country: United States
Entity ID: 95565059
Localized Name: New York Newark
Entity Type: AIRPORT


Error retrieving car rental data",1,1
"I want a comprehensive weather and air quality report for a specific location in the National Capital Region of the Philippines. I need to know the detailed weather forecast for the next few days, including temperature, humidity, and wind conditions, as well as the air quality, with information on pollutants like carbon monoxide, nitrogen dioxide, and particulate matter. The report should cover at least three upcoming forecast intervals and provide the current air quality index (AQI) and pollutant concentrations.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
API_HOST_WEATHER = 'weather-data-api1.p.rapidapi.com'

def API_check_forecast(lat='14.5995', lon='120.9842'):
    url = f""https://{API_HOST_WEATHER}/check-forecast""

    querystring = {
        ""lat"": lat,
        ""lon"": lon
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST_WEATHER
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        data = response.json()
        if data.get('list'):
            return data['list'][:3]  # Limit to the first 3 forecast intervals
        else:
            print(""Error retrieving weather forecast"")
    else:
        print(""Request failed for weather forecast:"", response.status_code)

    return None

def API_check_air_pollution(lat='14.5995', lon='120.9842'):
    url = f""https://{API_HOST_WEATHER}/air_pollution""

    querystring = {
        ""lat"": lat,
        ""lon"": lon
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST_WEATHER
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        data = response.json()
        if data.get('list'):
            return data['list'][0]  # Get the most recent air quality data
        else:
            print(""Error retrieving air pollution data"")
    else:
        print(""Request failed for air pollution data:"", response.status_code)

    return None

if __name__ == '__main__':
    # Retrieve weather forecast data for the National Capital Region
    forecast_data = API_check_forecast()
    if forecast_data:
        print(""Weather Forecast:"")
        for forecast in forecast_data:
            print(f""Date & Time: {forecast['dt_txt']}"")
            print(f""Temperature: {forecast['main']['temp']}Â°C"")
            print(f""Feels Like: {forecast['main']['feels_like']}Â°C"")
            print(f""Humidity: {forecast['main']['humidity']}%"")
            print(f""Weather: {forecast['weather'][0]['description'].capitalize()}"")
            print(f""Wind Speed: {forecast['wind']['speed']} m/s"")
            print(f""Cloudiness: {forecast['clouds']['all']}%"")
            if 'rain' in forecast and '3h' in forecast['rain']:
                print(f""Rain Volume (last 3 hrs): {forecast['rain']['3h']} mm"")
            print(""\n"")

    # Retrieve air quality data for the National Capital Region
    air_quality_data = API_check_air_pollution()
    if air_quality_data:
        print(""Air Quality Report:"")
        print(f""Air Quality Index (AQI): {air_quality_data['main']['aqi']}"")
        print(f""Carbon Monoxide (CO): {air_quality_data['components']['co']} Âµg/mÂ³"")
        print(f""Nitric Oxide (NO): {air_quality_data['components']['no']} Âµg/mÂ³"")
        print(f""Nitrogen Dioxide (NO2): {air_quality_data['components']['no2']} Âµg/mÂ³"")
        print(f""Ozone (O3): {air_quality_data['components']['o3']} Âµg/mÂ³"")
        print(f""Sulfur Dioxide (SO2): {air_quality_data['components']['so2']} Âµg/mÂ³"")
        print(f""PM2.5: {air_quality_data['components']['pm2_5']} Âµg/mÂ³"")
        print(f""PM10: {air_quality_data['components']['pm10']} Âµg/mÂ³"")
        print(f""Ammonia (NH3): {air_quality_data['components']['nh3']} Âµg/mÂ³"")
","Weather Forecast:
Date & Time: 2024-10-26 21:00:00
Temperature: 299Â°C
Feels Like: 299.99Â°C
Humidity: 90%
Weather: Moderate rain
Wind Speed: 5.93 m/s
Cloudiness: 89%
Rain Volume (last 3 hrs): 8.27 mm


Date & Time: 2024-10-27 00:00:00
Temperature: 299.31Â°C
Feels Like: 299.31Â°C
Humidity: 86%
Weather: Moderate rain
Wind Speed: 6.49 m/s
Cloudiness: 93%
Rain Volume (last 3 hrs): 3.69 mm


Date & Time: 2024-10-27 03:00:00
Temperature: 300.72Â°C
Feels Like: 303.7Â°C
Humidity: 76%
Weather: Light rain
Wind Speed: 7.5 m/s
Cloudiness: 100%
Rain Volume (last 3 hrs): 1.26 mm


Air Quality Report:
Air Quality Index (AQI): 2
Carbon Monoxide (CO): 347.14 Âµg/mÂ³
Nitric Oxide (NO): 0 Âµg/mÂ³
Nitrogen Dioxide (NO2): 4.54 Âµg/mÂ³
Ozone (O3): 70.1 Âµg/mÂ³
Sulfur Dioxide (SO2): 2.86 Âµg/mÂ³
PM2.5: 9.8 Âµg/mÂ³
PM10: 16.31 Âµg/mÂ³
Ammonia (NH3): 1.11 Âµg/mÂ³
",1,1
"I would like to know the exact location of Manila including latitude, longitude, country code and region details. After getting the geographic information, I want to know the air quality of the place, providing the air quality index (AQI) and the concentration of major pollutants such as carbon monoxide, nitrogen dioxide, and particulate matter. Make sure the response covers at least one specific location and includes air quality details.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
API_HOST_LOCATION = 'weather-data-api1.p.rapidapi.com'
API_HOST_AIR = 'weather-data-api1.p.rapidapi.com'


def API_find_location(location_name='Manila', limit=1):
    url = f""https://{API_HOST_LOCATION}/find-location""

    querystring = {
        ""q"": location_name,
        ""limit"": str(limit)
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST_LOCATION
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        data = response.json()
        if data:
            return data[0]  # Get the first matching location
        else:
            print(""Error retrieving location data"")
    else:
        print(""Request failed for location data:"", response.status_code)

    return None


def API_check_air_pollution(lat, lon):
    url = f""https://{API_HOST_AIR}/air_pollution""

    querystring = {
        ""lat"": str(lat),
        ""lon"": str(lon)
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST_AIR
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        data = response.json()
        if data.get('list'):
            return data['list'][0]  # Get the most recent air quality data
        else:
            print(""Error retrieving air pollution data"")
    else:
        print(""Request failed for air pollution data:"", response.status_code)

    return None


if __name__ == '__main__':
    # Retrieve geographical information for a specific location
    location_name = 'Manila'  # Example location
    location_data = API_find_location(location_name=location_name)

    if location_data:
        print(""Location Information:"")
        print(f""Location Name: {location_data['name']}"")
        print(f""Latitude: {location_data['lat']}"")
        print(f""Longitude: {location_data['lon']}"")
        print(f""Country Code: {location_data['country']}"")
        if 'state' in location_data:
            print(f""State: {location_data['state']}"")
        print(""\n"")

        # Retrieve air quality data using the found latitude and longitude
        air_quality_data = API_check_air_pollution(lat=location_data['lat'], lon=location_data['lon'])

        if air_quality_data:
            print(""Air Quality Report:"")
            print(f""Air Quality Index (AQI): {air_quality_data['main']['aqi']}"")
            print(f""Carbon Monoxide (CO): {air_quality_data['components']['co']} Âµg/mÂ³"")
            print(f""Nitric Oxide (NO): {air_quality_data['components']['no']} Âµg/mÂ³"")
            print(f""Nitrogen Dioxide (NO2): {air_quality_data['components']['no2']} Âµg/mÂ³"")
            print(f""Ozone (O3): {air_quality_data['components']['o3']} Âµg/mÂ³"")
            print(f""Sulfur Dioxide (SO2): {air_quality_data['components']['so2']} Âµg/mÂ³"")
            print(f""PM2.5: {air_quality_data['components']['pm2_5']} Âµg/mÂ³"")
            print(f""PM10: {air_quality_data['components']['pm10']} Âµg/mÂ³"")
            print(f""Ammonia (NH3): {air_quality_data['components']['nh3']} Âµg/mÂ³"")
",Fail,-1,-1
"I want information about cars for specific years, including key details about models available during those years. Please provide data for the most recent three years and include details such as model names, manufacturers, and any other relevant specifications.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
API_HOST = 'car-api2.p.rapidapi.com'


def get_years_data():
    url = f""https://{API_HOST}/api/years""

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST
    }

    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        data = response.json()
        if isinstance(data, list):
            # Assume the data is a list of year objects, get the most recent 3 years
            sorted_years = sorted(data, reverse=True)[:3]  # Sort in descending order and take the top 3
            return sorted_years
        else:
            print(""Unexpected data format received from Years API"")
    else:
        print(f""Failed to fetch years data: {response.status_code}"")

    return None


def get_models_for_year(year):
    url = f""https://{API_HOST}/api/models""

    querystring = {
        ""year"": str(year),  # Ensure the year is passed as a string
        ""sort"": ""id"",
        ""direction"": ""asc"",
        ""verbose"": ""yes""
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        data = response.json()
        return data['data'][:3]  # Limit to the first 3 models for simplicity
    else:
        print(f""Failed to fetch models for year {year}: {response.status_code}"")

    return None


if __name__ == '__main__':
    # Step 1: Get the most recent 3 years
    years = get_years_data()

    if years:
        for year in years:
            print(f""\nFetching data for cars from the year {year}...\n"")

            # Step 2: Get car models for each of the recent 3 years
            models_data = get_models_for_year(year)

            if models_data:
                print(f""Top 3 Car Models for {year}:"")
                for model in models_data:
                    print(f""Model Name: {model['name']}"")
                    print(f""Manufacturer: {model['make']['name']}"")
                    print(f""Model ID: {model['id']}"")
                    print(f""Make ID: {model['make']['id']}"")
                    print(f""Link: {model.get('collection', {}).get('url', 'N/A')}\n"")
","Fetching data for cars from the year 2025...

Top 3 Car Models for 2025:
Model Name: ******* (hidden)
Manufacturer: Acura
Model ID: 1
Make ID: 1
Link: N/A

Model Name: *** (hidden)
Manufacturer: Acura
Model ID: 2
Make ID: 1
Link: N/A

Model Name: ******** (hidden)
Manufacturer: BMW
Model ID: 9
Make ID: 3
Link: N/A


Fetching data for cars from the year 2024...

Top 3 Car Models for 2024:
Model Name: ******* (hidden)
Manufacturer: Acura
Model ID: 1
Make ID: 1
Link: N/A

Model Name: *** (hidden)
Manufacturer: Acura
Model ID: 2
Make ID: 1
Link: N/A

Model Name: ** (hidden)
Manufacturer: Audi
Model ID: 3
Make ID: 2
Link: N/A


Fetching data for cars from the year 2023...

Top 3 Car Models for 2023:
Model Name: ******* (hidden)
Manufacturer: Acura
Model ID: 1
Make ID: 1
Link: N/A

Model Name: *** (hidden)
Manufacturer: Acura
Model ID: 2
Make ID: 1
Link: N/A

Model Name: ** (hidden)
Manufacturer: Audi
Model ID: 3
Make ID: 2
Link: N/A
",0,0
"I need detailed information about automobile data for the latest three years, including specific manufacturers and their details. ","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
API_HOST = 'car-api2.p.rapidapi.com'


def get_recent_years():
    url = f""https://{API_HOST}/api/years""

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST
    }

    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        data = response.json()
        if isinstance(data, list):
            # Assume the response is a list of years, get the most recent 3
            sorted_years = sorted(data, reverse=True)[:3]
            return sorted_years
        else:
            print(""Unexpected data format received from Years API"")
    else:
        print(f""Failed to fetch years data: {response.status_code}"")

    return None


def get_makes_for_year(year):
    url = f""https://{API_HOST}/api/makes""

    querystring = {
        ""year"": str(year),  # Ensure the year is passed as a string
        ""sort"": ""id"",
        ""direction"": ""asc"",
        ""limit"": ""3""  # Limit to the first 3 makes for each year
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        data = response.json()
        return data['data'][:3]  # Take the first 3 manufacturers
    else:
        print(f""Failed to fetch makes for year {year}: {response.status_code}"")

    return None


if __name__ == '__main__':
    # Step 1: Get the most recent 3 years
    recent_years = get_recent_years()

    if recent_years:
        for year in recent_years:
            print(f""\nFetching car makes for the year {year}...\n"")

            # Step 2: Get car makes for each of the recent 3 years
            makes_data = get_makes_for_year(year)

            if makes_data:
                print(f""Top 3 Car Makes for {year}:"")
                for make in makes_data:
                    print(f""Make Name: {make['name']}"")
                    print(f""Make ID: {make['id']}\n"")
","Fetching car makes for the year 2025...

Top 3 Car Makes for 2025:
Make Name: Acura
Make ID: 1

Make Name: Audi
Make ID: 2

Make Name: BMW
Make ID: 3


Fetching car makes for the year 2024...

Top 3 Car Makes for 2024:
Make Name: Acura
Make ID: 1

Make Name: Audi
Make ID: 2

Make Name: BMW
Make ID: 3


Fetching car makes for the year 2023...

Top 3 Car Makes for 2023:
Make Name: Acura
Make ID: 1

Make Name: Audi
Make ID: 2

Make Name: BMW
Make ID: 3
",1,1
I need a summary of recent soccer matches and the profiles of three players from the La Liga league. ,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
ARCHIVE_API_HOST = 'pinnacle-odds.p.rapidapi.com'
PLAYER_API_HOST = 'la-liga2.p.rapidapi.com'

def get_recent_soccer_events():
    url = f""https://{ARCHIVE_API_HOST}/kit/v1/archive""

    querystring = {
        ""sport_id"": ""1"",  # Assuming 1 is the ID for soccer
        ""page_num"": ""1""   # Get the first page of events
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ARCHIVE_API_HOST
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        data = response.json()
        if 'events' in data:
            # Return the first three events
            return data['events'][:3]
        else:
            print(""Unexpected data format received from Archive Events API"")
    else:
        print(f""Failed to fetch events data: {response.status_code}"")

    return None

def get_player_details(player_id):
    url = f""https://{PLAYER_API_HOST}/players/{player_id}""

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": PLAYER_API_HOST
    }

    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        return response.json()
    else:
        print(f""Failed to fetch player data for ID {player_id}: {response.status_code}"")

    return None

if __name__ == '__main__':
    # Step 1: Get recent soccer events
    recent_events = get_recent_soccer_events()

    if recent_events:
        print(""\nRecent Soccer Events:"")
        player_ids = []

        for event in recent_events:
            print(f""Match: {event['home']} vs {event['away']}"")
            print(f""League: {event['league_name']}"")
            print(f""Start Time: {event['starts']}"")
            print(f""Home Score: {event['period_results'][0]['team_1_score']}, Away Score: {event['period_results'][0]['team_2_score']}\n"")

            # Collect player IDs if available
            if 'parent_id' in event and event['parent_id']:
                player_ids.append(event['parent_id'])

        # Ensure we have exactly 3 player IDs
        if len(player_ids) >= 3:
            player_ids = player_ids[:3]

            print(""\nDetailed Player Information:"")
            for player_id in player_ids:
                player_info = get_player_details(player_id)
                if player_info:
                    print(f""Player Name: {player_info['name']}"")
                    print(f""Team: {player_info['team']['name']}"")
                    print(f""Position: {player_info['position']}"")
                    print(f""Age: {player_info['age']}"")
                    print(f""Goals: {player_info['goals']}"")
                    print(f""Assists: {player_info['assists']}"")
                    print(f""Yellow Cards: {player_info['yellowCards']}"")
                    print(f""Red Cards: {player_info['redCards']}\n"")
",Fail,-1,-1
I need to find detailed geographical information and the local time for three specific cities. ,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
LOCATION_API_HOST = 'weather-data-api1.p.rapidapi.com'
TIMEZONE_API_HOST = 'weatherapi-com.p.rapidapi.com'


def get_location_data(city_name):
    url = f""https://{LOCATION_API_HOST}/find-location""

    querystring = {
        ""q"": city_name,
        ""limit"": ""1""  # Limit to the first matching result
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": LOCATION_API_HOST
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        data = response.json()
        if data:
            # Return the first match's latitude and longitude
            return data[0]  # Assume the first item is the desired result
        else:
            print(""No location data found."")
    else:
        print(f""Failed to fetch location data for {city_name}: {response.status_code}"")

    return None


def get_time_zone(lat, lon):
    url = f""https://{TIMEZONE_API_HOST}/timezone.json""

    querystring = {
        ""q"": f""{lat},{lon}""  # Use latitude and longitude for the query
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": TIMEZONE_API_HOST
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(f""Failed to fetch timezone data for coordinates ({lat}, {lon}): {response.status_code}"")

    return None


if __name__ == '__main__':
    # Define the cities you want to get data for
    cities = [""New York"", ""Tokyo"", ""London""]

    for city in cities:
        # Step 1: Get geographical data for the city
        location_data = get_location_data(city)

        if location_data:
            print(f""\nLocation Data for {city}:"")
            print(f""Name: {location_data['name']}"")
            print(f""Country: {location_data['country']}"")
            print(f""Latitude: {location_data['lat']}, Longitude: {location_data['lon']}"")

            # Step 2: Use latitude and longitude to get local time
            timezone_data = get_time_zone(location_data['lat'], location_data['lon'])

            if timezone_data:
                print(f""Local Time Information for {city}:"")
                print(f""Time Zone ID: {timezone_data['location']['tz_id']}"")
                print(f""Local Time: {timezone_data['location']['localtime']}"")
","Location Data for New York:
Name: New York
Country: US
Latitude: 40.7127281, Longitude: -74.0060152
Local Time Information for New York:
Time Zone ID: America/New_York
Local Time: 2024-10-26 14:24

Location Data for Tokyo:
Name: Tokyo
Country: JP
Latitude: 35.6828387, Longitude: 139.7594549
Local Time Information for Tokyo:
Time Zone ID: Asia/Tokyo
Local Time: 2024-10-27 03:24

Location Data for London:
Name: London
Country: GB
Latitude: 51.5073219, Longitude: -0.1276474
Local Time Information for London:
Time Zone ID: Europe/London
Local Time: 2024-10-26 19:24",1,1
"I need to get information about NBA teams, including their players and details. First, gather a list of all NBA teams.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
NBA_API_HOST = 'nba-api-free-data.p.rapidapi.com'

def get_all_nba_teams():
    url = f""https://{NBA_API_HOST}/nba-team-list""

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": NBA_API_HOST
    }

    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        return response.json()  # Return list of NBA teams
    else:
        print(f""Failed to fetch NBA teams: {response.status_code}"")
        return None

def get_nba_players_by_team_id(team_id):
    url = f""https://{NBA_API_HOST}/nba-player-listing/v1/data""

    querystring = {""id"": team_id}

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": NBA_API_HOST
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()  # Return players data for a specific team
    else:
        print(f""Failed to fetch players for team ID {team_id}: {response.status_code}"")
        return None

def get_nba_team_details(team_id):
    url = f""https://{NBA_API_HOST}/nba-team-info/v1/data""

    querystring = {""id"": team_id}

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": NBA_API_HOST
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()  # Return detailed info for a specific team
    else:
        print(f""Failed to fetch team details for team ID {team_id}: {response.status_code}"")
        return None

if __name__ == '__main__':
    # Step 1: Retrieve the list of all NBA teams
    nba_teams = get_all_nba_teams()

    if nba_teams and 'teams' in nba_teams:
        print(""\nList of NBA Teams:"")
        selected_teams = nba_teams['teams'][:3]  # Select the first 3 teams for detailed analysis

        for team in selected_teams:
            print(f""Team ID: {team['id']}, Name: {team['displayName']}"")

            # Step 2: Fetch players for each selected team
            players = get_nba_players_by_team_id(team['id'])

            if players and 'athletes' in players:
                print(f""\nPlayers for {team['displayName']}:"")
                for player in players['athletes'][:3]:  # Display first 3 players
                    print(f""Player Name: {player['fullName']}, Position: {player['position']['displayName']}"")

            # Step 3: Fetch detailed team info for each selected team
            team_details = get_nba_team_details(team['id'])

            if team_details and 'team' in team_details:
                team_info = team_details['team']
                print(f""\nDetailed Info for {team_info['displayName']}:"")
                print(f""Location: {team_info['location']}, Abbreviation: {team_info['abbreviation']}"")
                print(f""Record: {team_info['recordSummary']}, Standing: {team_info['standingSummary']}"")
    else:
        print(""No NBA teams found."")
","List of NBA Teams:
Team ID: 1, Name: Atlanta Hawks

Players for Atlanta Hawks:
Player Name: Dominick Barlow, Position: Forward
Player Name: Bogdan Bogdanovic, Position: Shooting Guard
Player Name: Kobe Bufkin, Position: Guard

Detailed Info for Atlanta Hawks:
Location: Atlanta, Abbreviation: ATL
Record: 2-0, Standing: 1st in Southeast Division
Team ID: 2, Name: Boston Celtics

Players for Boston Celtics:
Player Name: Jaylen Brown, Position: Shooting Guard
Player Name: JD Davison, Position: Shooting Guard
Player Name: Sam Hauser, Position: Small Forward

Detailed Info for Boston Celtics:
Location: Boston, Abbreviation: BOS
Record: 2-0, Standing: 1st in Atlantic Division
Team ID: 17, Name: Brooklyn Nets

Players for Brooklyn Nets:
Player Name: Bojan Bogdanovic, Position: Small Forward
Player Name: Nic Claxton, Position: Center
Player Name: Noah Clowney, Position: Forward

Detailed Info for Brooklyn Nets:
Location: Brooklyn, Abbreviation: BKN
Record: 0-2, Standing: 4th in Atlantic Division",1,1
" I want to gather detailed information about a specific movie and also retrieve related anime titles. First, provide comprehensive details of a movie including its synopsis, genres, release date, streaming sources, and recommendations for similar movies. Then, fetch anime titles with similar genres and content from a large anime database, with at least three results from both categories.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
MOVIE_API_HOST = 'movies-api14.p.rapidapi.com'
ANIME_API_HOST = 'anime-db.p.rapidapi.com'

def get_movie_details(movie_id):
    url = f""https://{MOVIE_API_HOST}/movie/{movie_id}""

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": MOVIE_API_HOST
    }

    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        return response.json()  # Return detailed information of a movie
    else:
        print(f""Failed to fetch movie details for movie ID {movie_id}: {response.status_code}"")
        return None

def get_anime_titles(page, size, genres):
    url = f""https://{ANIME_API_HOST}/anime""

    querystring = {
        ""page"": page,
        ""size"": size,
        ""genres"": genres,
        ""sortBy"": ""ranking"",
        ""sortOrder"": ""asc""
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ANIME_API_HOST
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()  # Return anime titles based on criteria
    else:
        print(f""Failed to fetch anime titles: {response.status_code}"")
        return None

if __name__ == '__main__':
    # Step 1: Fetch detailed movie information
    movie_id = ""27205""  # Example movie ID
    movie_details = get_movie_details(movie_id)

    if movie_details and 'movie' in movie_details:
        movie_info = movie_details['movie']
        print(""\nMovie Information:"")
        print(f""Title: {movie_info['title']}"")
        print(f""Overview: {movie_info['overview']}"")
        print(f""Release Date: {movie_info['release_date']}"")
        print(f""Genres: {', '.join(movie_info['genres'])}"")
        print(f""Rating: {movie_info['vote_average']} ({movie_info['vote_count']} votes)"")
        print(""\nStreaming Sources:"")
        for source in movie_info['sources'][:3]:  # Display the first 3 streaming sources
            print(f""Platform: {source['display_name']}, Type: {source['type']}, URL: {source['link']}"")

        print(""\nSimilar Movies:"")
        for similar in movie_details['similarMovies'][:3]:  # Display first 3 similar movies
            print(f""Title: {similar['title']}, Poster URL: {similar['poster_path']}"")

        # Step 2: Use movie genres to fetch related anime titles
        movie_genres = ','.join(movie_info['genres'])
        anime_data = get_anime_titles(page=""1"", size=""3"", genres=movie_genres)

        if anime_data and 'data' in anime_data:
            print(""\nRelated Anime Titles:"")
            for anime in anime_data['data']:
                print(f""Title: {anime['title']}, Genre: {', '.join(anime['genres'])}"")
                print(f""Synopsis: {anime['synopsis']}"")
                print(f""Link: {anime['link']}"")
    else:
        print(""No movie details found."")
","Movie Information:
Title: Inception
Overview: Cobb, a skilled thief who commits corporate espionage by infiltrating the subconscious of his targets is offered a chance to regain his old life as payment for a task considered to be impossible: ""inception"", the implantation of another person's idea into a target's subconscious.
Release Date: 2010-07-15
Genres: Action & Adventure, Sci-Fi & Fantasy, Action & Adventure
Rating: 8.362 (33655 votes)

Streaming Sources:
Platform: HBO Max, Type: subscription, URL: https://play.hbomax.com/feature/urn:hbo:feature:GYDbfcQGssU-dEAEAAAAI
Platform: Netflix, Type: subscription, URL: https://www.netflix.com/watch/70131314
Platform: Apple TV, Type: purchase, URL: https://tv.apple.com/us/movie/inception/umc.cmc.6loas01ow0w4lkatxxloz7a6e

Similar Movies:
Title: Strange Days, Poster URL: https://image.tmdb.org/t/p/original/dwstlGA74LujvsLpGGaxtBvgx67.jpg
Title: Beverly Hills Cop III, Poster URL: https://image.tmdb.org/t/p/original/7vIpCkgGEfk5LZwm78nMxswLJRH.jpg
Title: The Matrix, Poster URL: https://image.tmdb.org/t/p/original/f89U3ADr1oiB1s9GkdPOEpXUk5H.jpg

Related Anime Titles:",1,1
"I want retrieve details of three trending TV shows and three popular movies, including their titles, genres, release dates, and short descriptions. Display the information separately for movies and TV shows, along with the poster URLs","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

# Fetch trending TV shows
def get_shows():
    url = ""https://movies-api14.p.rapidapi.com/shows""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""movies-api14.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    data = response.json()

    # Retrieve top 3 TV shows
    shows = data.get(""movies"", [])[:3]
    result = []
    for show in shows:
        result.append({
            ""title"": show.get(""title""),
            ""genres"": show.get(""genres""),
            ""first_aired"": show.get(""first_aired""),
            ""overview"": show.get(""overview""),
            ""poster_path"": show.get(""poster_path"")
        })
    return result

# Fetch popular movies
def get_movies():
    url = ""https://movies-api14.p.rapidapi.com/movies""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""movies-api14.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    data = response.json()

    # Retrieve top 3 movies
    movies = data.get(""movies"", [])[:3]
    result = []
    for movie in movies:
        result.append({
            ""title"": movie.get(""title""),
            ""genres"": movie.get(""genres""),
            ""release_date"": movie.get(""release_date""),
            ""overview"": movie.get(""overview""),
            ""poster_path"": movie.get(""poster_path"")
        })
    return result

# Main function to output results
if __name__ == '__main__':
    shows = get_shows()
    movies = get_movies()

    # Display TV show information
    print(""TV Shows:"")
    for show in shows:
        print(f""Title: {show['title']}"")
        print(f""Genres: {', '.join(show['genres'])}"")
        print(f""First Aired: {show['first_aired']}"")
        print(f""Overview: {show['overview']}"")
        print(f""Poster URL: {show['poster_path']}"")
        print(""\n"" + ""=""*40 + ""\n"")

    # Display movie information
    print(""Movies:"")
    for movie in movies:
        print(f""Title: {movie['title']}"")
        print(f""Genres: {', '.join(movie['genres'])}"")
        print(f""Release Date: {movie['release_date']}"")
        print(f""Overview: {movie['overview']}"")
        print(f""Poster URL: {movie['poster_path']}"")
        print(""\n"" + ""=""*40 + ""\n"")
","TV Shows:
Title: Law & Order: Special Victims Unit
Genres: Crime, Drama, Mystery
First Aired: 1999-09-20
Overview: In the criminal justice system, sexually-based offenses are considered especially heinous. In New York City, the dedicated detectives who investigate these vicious felonies are members of an elite squad known as the Special Victims Unit. These are their stories.
Poster URL: https://image.tmdb.org/t/p/original/ywBt4WKADdMVgxTR1rS2uFwMYTH.jpg

========================================

Title: The Simpsons
Genres: Family, Animation, Comedy
First Aired: 1989-12-17
Overview: Set in Springfield, the average American town, the show focuses on the antics and everyday adventures of the Simpson family; Homer, Marge, Bart, Lisa and Maggie, as well as a virtual cast of thousands. Since the beginning, the series has been a pop culture icon, attracting hundreds of celebrities to guest star. The show has also made name for itself in its fearless satirical take on politics, media and American life in general.
Poster URL: https://image.tmdb.org/t/p/original/zI3E2a3WYma5w8emI35mgq5Iurx.jpg

========================================

Title: The Last of Us
Genres: Drama
First Aired: 2023-01-15
Overview: Twenty years after modern civilization has been destroyed, Joel, a hardened survivor, is hired to smuggle Ellie, a 14-year-old girl, out of an oppressive quarantine zone. What starts as a small job soon becomes a brutal, heartbreaking journey, as they both must traverse the United States and depend on each other for survival.
Poster URL: https://image.tmdb.org/t/p/original/uKvVjHNqB5VmOrdxqAt2F7J78ED.jpg

========================================

Movies:
Title: Ghosted
Genres: Action & Adventure, Comedy, Romance
Release Date: 2023-04-18
Overview: Salt-of-the-earth Cole falls head over heels for enigmatic Sadie â€” but then makes the shocking discovery that sheâ€™s a secret agent. Before they can decide on a second date, Cole and Sadie are swept away on an international adventure to save the world.
Poster URL: https://image.tmdb.org/t/p/original/liLN69YgoovHVgmlHJ876PKi5Yi.jpg

========================================

Title: AKA
Genres: Thriller, Action & Adventure, Crime
Release Date: 2023-04-28
Overview: A steely special ops agent finds his morality put to the test when he infiltrates a crime syndicate and unexpectedly bonds with the boss' young son.
Poster URL: https://image.tmdb.org/t/p/original/3BSxAjiporlwQTWzaHZ9Yrl5C9D.jpg

========================================

Title: Scream VI
Genres: Horror, Mystery, Thriller
Release Date: 2023-03-08
Overview: Following the latest Ghostface killings, the four survivors leave Woodsboro behind and start a fresh chapter.
Poster URL: https://image.tmdb.org/t/p/original/wDWwtvkRRlgTiUr6TyLSMX8FCuZ.jpg

========================================

",1,1
"I want find three popular movies and then perform a search for these movies based on their titles to retrieve additional details. Display each movie's title, genre, release date, synopsis, and available streaming platforms. Provide a poster link for each movie as well.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

# Fetch list of popular movies
def get_movies():
    url = ""https://movies-api14.p.rapidapi.com/movies""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""movies-api14.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    data = response.json()

    # Return top 3 movies with basic info
    movies = data.get(""movies"", [])[:3]
    return [{""title"": movie.get(""title""), ""id"": movie.get(""_id"")} for movie in movies]

# Perform a search for additional details on each movie
def search_movie(query):
    url = ""https://movies-api14.p.rapidapi.com/search""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""movies-api14.p.rapidapi.com""
    }
    params = {""query"": query}
    response = requests.get(url, headers=headers, params=params)
    return response.json()

# Main function to retrieve and display movie details
if __name__ == '__main__':
    # Fetch top 3 movies
    movies = get_movies()

    # Fetch additional details for each movie
    for movie in movies:
        details = search_movie(movie[""title""])

        # Extract and display movie details
        if details.get(""contents""):
            movie_detail = details[""contents""][0]
            title = movie_detail.get(""title"")
            genres = movie_detail.get(""genres"", [])
            release_date = movie_detail.get(""release_date"")
            overview = movie_detail.get(""overview"")
            poster_path = movie_detail.get(""poster_path"")
            streaming_sources = [
                f""{source['display_name']} - {source['link']}""
                for source in movie_detail.get(""sources"", [])
            ]

            # Output movie information
            print(f""Title: {title}"")
            print(f""Genres: {', '.join(genres)}"")
            print(f""Release Date: {release_date}"")
            print(f""Overview: {overview}"")
            print(f""Poster URL: {poster_path}"")
            print(""Available on:"")
            for source in streaming_sources:
                print(f"" - {source}"")
            print(""\n"" + ""=""*40 + ""\n"")
",Fail,-1,-1
"I want retrieve a list of available movie genres and use them to filter trending movies from different categories. For each genre, get three trending or new release movies, showing the title, genre, release date, and a short description, along with poster images.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

# Retrieve available genres for filtering
def get_genres():
    url = ""https://ott-details.p.rapidapi.com/getParams""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""ott-details.p.rapidapi.com""
    }
    params = {""param"": ""genre""}
    response = requests.get(url, headers=headers, params=params)
    return response.json()

# Retrieve trending and new release movies in specific categories
def get_trending_movies():
    url = ""https://movies-api14.p.rapidapi.com/home""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""movies-api14.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    data = response.json()

    # Limit to top 3 movies per category
    movies_by_category = []
    for category in data[:3]:  # Limit to 3 categories for simplicity
        category_movies = category[""movies""][:3]
        movies_by_category.append({
            ""category_title"": category[""title""],
            ""movies"": [{
                ""title"": movie[""title""],
                ""genres"": movie[""genres""],
                ""release_date"": movie[""release_date""],
                ""overview"": movie[""overview""],
                ""poster_path"": movie[""poster_path""]
            } for movie in category_movies]
        })
    return movies_by_category

# Main function to display genres and movies
if __name__ == '__main__':
    genres = get_genres()
    trending_movies = get_trending_movies()

    # Display available genres
    print(""Available Genres:"")
    for genre in genres:
        print(f""- {genre}"")
    print(""\n"" + ""=""*40 + ""\n"")

    # Display trending movies by category
    for category in trending_movies:
        print(f""Category: {category['category_title']}"")
        for movie in category[""movies""]:
            print(f""Title: {movie['title']}"")
            print(f""Genres: {', '.join(movie['genres'])}"")
            print(f""Release Date: {movie['release_date']}"")
            print(f""Overview: {movie['overview']}"")
            print(f""Poster URL: {movie['poster_path']}"")
            print(""\n"" + ""-""*20 + ""\n"")
        print(""\n"" + ""=""*40 + ""\n"")
","Available Genres:
- Action
- Adult
- Adventure
- Animation
- Biography
- Comedy
- Crime
- Documentary
- Drama
- Family
- Fantasy
- Game-Show
- History
- Horror
- Music
- Musical
- Mystery
- News
- Reality-TV
- Romance
- Sci-Fi
- Short
- Sport
- Talk-Show
- Thriller
- War
- Western

========================================

Category: Trending Movies
Title: Ghosted
Genres: Action & Adventure, Comedy, Romance
Release Date: 2023-04-18
Overview: Salt-of-the-earth Cole falls head over heels for enigmatic Sadie â€” but then makes the shocking discovery that sheâ€™s a secret agent. Before they can decide on a second date, Cole and Sadie are swept away on an international adventure to save the world.
Poster URL: https://image.tmdb.org/t/p/original/liLN69YgoovHVgmlHJ876PKi5Yi.jpg

--------------------

Title: AKA
Genres: Thriller, Action & Adventure, Crime
Release Date: 2023-04-28
Overview: A steely special ops agent finds his morality put to the test when he infiltrates a crime syndicate and unexpectedly bonds with the boss' young son.
Poster URL: https://image.tmdb.org/t/p/original/3BSxAjiporlwQTWzaHZ9Yrl5C9D.jpg

--------------------

Title: Scream VI
Genres: Horror, Mystery, Thriller
Release Date: 2023-03-08
Overview: Following the latest Ghostface killings, the four survivors leave Woodsboro behind and start a fresh chapter.
Poster URL: https://image.tmdb.org/t/p/original/wDWwtvkRRlgTiUr6TyLSMX8FCuZ.jpg

--------------------


========================================

Category: New Movies
Title: Sam Bahadur
Genres: History, War
Release Date: 2023-12-01
Overview: A biography on Sam Manekshaw
Poster URL: https://image.tmdb.org/t/p/original/uQiG1gSsV18eTFmflsF2NWQMTgR.jpg

--------------------

Title: The Hunger Games: The Ballad of Songbirds & Snakes
Genres: Action & Adventure, Action & Adventure, Sci-Fi & Fantasy
Release Date: 2023-11-15
Overview: Years before he becomes the tyrannical president of Panem, 18-year-old Coriolanus Snow sees a chance for a change in fortunes when he mentors Lucy Gray Baird, the female tribute from District 12.
Poster URL: https://image.tmdb.org/t/p/original/jFXj9rh8EoSAHcYTBJYv1ir6X3A.jpg

--------------------

Title: After Everything
Genres: Romance, Drama
Release Date: 2023-09-13
Overview: The sequel to 'After Ever Happy' (2022), which was based on the 2015 novel of same title by Anna Todd, wherein Hardin and Tessa are separated.
Poster URL: https://image.tmdb.org/t/p/original/d0qw9hiNwhRmfw4yAc44pep6vhz.jpg

--------------------


========================================

Category: Best Animation Movies
Title: Puss in Boots: The Last Wish
Genres: Animation, Family, Sci-Fi & Fantasy, Action & Adventure, Comedy
Release Date: 2022-12-07
Overview: Puss in Boots discovers that his passion for adventure has taken its toll: He has burned through eight of his nine lives, leaving him with only one life left. Puss sets out on an epic journey to find the mythical Last Wish and restore his nine lives.
Poster URL: https://image.tmdb.org/t/p/original/kuf6dutpsT0vSVehic3EZIqkOBt.jpg

--------------------

Title: Marcel the Shell with Shoes On
Genres: Animation, Comedy, Family, Drama, Action & Adventure
Release Date: 2022-06-24
Overview: Marcel is an adorable one-inch-tall shell who ekes out a colorful existence with his grandmother Connie and their pet lint, Alan. Once part of a sprawling community of shells, they now live alone as the sole survivors of a mysterious tragedy. But when a documentary filmmaker discovers them amongst the clutter of his Airbnb, the short film he posts online brings Marcel millions of passionate fans, as well as unprecedented dangers and a new hope at finding his long-lost family.
Poster URL: https://image.tmdb.org/t/p/original/jaYmP4Ct8YLnxWAW2oYkUjeXtzm.jpg

--------------------

Title: Stand by Me Doraemon
Genres: Animation, Family, Sci-Fi & Fantasy, Sci-Fi & Fantasy
Release Date: 2014-08-08
Overview: Sewashi and Doraemon find themselves way back in time and meet Nobita. It is up to Doraemon to take care of Nobita or else he will not return to the present.
Poster URL: https://image.tmdb.org/t/p/original/wc7XQbfx6EIQqCuvmBMt3aisb2Y.jpg

--------------------


========================================
",1,1
"I want retrieve a list of three recent action movies with an IMDb rating between 6 and 8. For each movie, get additional details, including the plot summary, release date, genres, and available streaming sources. Display each movieâ€™s title, genres, release date, IMDb rating, plot, streaming platform information, and a poster image","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

# Perform an advanced search for action movies with specific IMDb rating range
def advanced_search():
    url = ""https://ott-details.p.rapidapi.com/advancedsearch""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""ott-details.p.rapidapi.com""
    }
    params = {
        ""start_year"": ""2010"",
        ""end_year"": ""2022"",
        ""min_imdb"": ""6"",
        ""max_imdb"": ""8"",
        ""genre"": ""action"",
        ""language"": ""english"",
        ""type"": ""movie"",
        ""sort"": ""latest"",
        ""page"": ""1""
    }
    response = requests.get(url, headers=headers, params=params)
    data = response.json()

    # Retrieve top 3 movies based on the search
    results = data.get(""results"", [])[:3]
    return [{""title"": movie[""title""], ""id"": movie[""imdbid""]} for movie in results]

# Fetch detailed information for a specific movie
def get_movie_details(movie_id):
    url = f""https://movies-api14.p.rapidapi.com/movie/{movie_id}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""movies-api14.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

# Main function to get and display movie details
if __name__ == '__main__':
    # Get the list of action movies from the search
    movies = advanced_search()

    # Fetch and display details for each movie
    for movie in movies:
        details = get_movie_details(movie[""id""])
        movie_data = details.get(""movie"", {})

        title = movie_data.get(""title"")
        genres = movie_data.get(""genres"", [])
        release_date = movie_data.get(""release_date"")
        overview = movie_data.get(""overview"")
        imdb_rating = movie_data.get(""vote_average"")
        poster_path = movie_data.get(""poster_path"")
        streaming_sources = [
            f""{source['display_name']} - {source['link']}""
            for source in movie_data.get(""sources"", [])
        ]

        # Output movie information
        print(f""Title: {title}"")
        print(f""Genres: {', '.join(genres)}"")
        print(f""Release Date: {release_date}"")
        print(f""IMDb Rating: {imdb_rating}"")
        print(f""Overview: {overview}"")
        print(f""Poster URL: {poster_path}"")
        print(""Available on:"")
        for source in streaming_sources:
            print(f"" - {source}"")
        print(""\n"" + ""=""*40 + ""\n"")
",Fail,-1,-1
"I want retrieve a list of available genres and use it to search for top-ranked anime from selected genres. Retrieve three anime titles per genre, showing details including title, ranking, episode count, synopsis, status, and poster image. Display the information sorted by ranking for each genre.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

# Retrieve a list of genres
def get_genres():
    url = ""https://advanced-movie-search.p.rapidapi.com/genre/movie/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""advanced-movie-search.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    data = response.json()

    # Select top 3 genres to use in search (for demonstration purposes)
    genres = [genre[""name""] for genre in data.get(""genres"", [])[:3]]
    return genres

# Search for top-ranked anime in selected genres
def get_anime_by_genre(genre):
    url = ""https://anime-db.p.rapidapi.com/anime""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""anime-db.p.rapidapi.com""
    }
    params = {
        ""page"": ""1"",
        ""size"": ""3"",  # Limit to top 3 anime for each genre
        ""genres"": genre,
        ""sortBy"": ""ranking"",
        ""sortOrder"": ""asc""
    }
    response = requests.get(url, headers=headers, params=params)
    return response.json()

# Main function to get and display anime details by genre
if __name__ == '__main__':
    # Get list of genres
    genres = get_genres()

    # Retrieve and display top anime for each genre
    for genre in genres:
        print(f""Genre: {genre}"")
        anime_data = get_anime_by_genre(genre)

        for anime in anime_data.get(""data"", []):
            title = anime.get(""title"")
            ranking = anime.get(""ranking"")
            episodes = anime.get(""episodes"")
            synopsis = anime.get(""synopsis"")
            status = anime.get(""status"")
            poster_path = anime.get(""image"")

            # Display anime details
            print(f""Title: {title}"")
            print(f""Ranking: {ranking}"")
            print(f""Episodes: {episodes}"")
            print(f""Status: {status}"")
            print(f""Synopsis: {synopsis}"")
            print(f""Poster URL: {poster_path}"")
            print(""\n"" + ""-""*20 + ""\n"")
        print(""\n"" + ""=""*40 + ""\n"")
","Genre: Action
Title: Fullmetal Alchemist: Brotherhood
Ranking: 2
Episodes: 64
Status: Finished Airing
Synopsis: After a horrific alchemy experiment goes wrong in the Elric household, brothers Edward and Alphonse are left in a catastrophic new reality. Ignoring the alchemical principle banning human transmutation, the boys attempted to bring their recently deceased mother back to life. Instead, they suffered brutal personal loss: Alphonse's body disintegrated while Edward lost a leg and then sacrificed an arm to keep Alphonse's soul in the physical realm by binding it to a hulking suit of armor.

The brothers are rescued by their neighbor Pinako Rockbell and her granddaughter Winry. Known as a bio-mechanical engineering prodigy, Winry creates prosthetic limbs for Edward by utilizing ""automail,"" a tough, versatile metal used in robots and combat armor. After years of training, the Elric brothers set off on a quest to restore their bodies by locating the Philosopher's Stoneâ€”a powerful gem that allows an alchemist to defy the traditional laws of Equivalent Exchange.

As Edward becomes an infamous alchemist and gains the nickname ""Fullmetal,"" the boys' journey embroils them in a growing conspiracy that threatens the fate of the world.

[Written by MAL Rewrite]
Poster URL: https://cdn.myanimelist.net/images/anime/1208/94745.webp

--------------------

Title: GintamaÂ°
Ranking: 4
Episodes: 51
Status: Finished Airing
Synopsis: Gintoki, Shinpachi, and Kagura return as the fun-loving but broke members of the Yorozuya team! Living in an alternate-reality Edo, where swords are prohibited and alien overlords have conquered Japan, they try to thrive on doing whatever work they can get their hands on. However, Shinpachi and Kagura still haven't been paid... Does Gin-chan really spend all that cash playing pachinko?

Meanwhile, when Gintoki drunkenly staggers home one night, an alien spaceship crashes nearby. A fatally injured crew member emerges from the ship and gives Gintoki a strange, clock-shaped device, warning him that it is incredibly powerful and must be safeguarded. Mistaking it for his alarm clock, Gintoki proceeds to smash the device the next morning and suddenly discovers that the world outside his apartment has come to a standstill. With Kagura and Shinpachi at his side, he sets off to get the device fixed; though, as usual, nothing is ever that simple for the Yorozuya team.

Filled with tongue-in-cheek humor and moments of heartfelt emotion, Gintama's fourth season finds Gintoki and his friends facing both their most hilarious misadventures and most dangerous crises yet.

[Written by MAL Rewrite]
Poster URL: https://cdn.myanimelist.net/images/anime/3/72078.webp

--------------------

Title: Shingeki no Kyojin Season 3 Part 2
Ranking: 5
Episodes: 10
Status: Finished Airing
Synopsis: Seeking to restore humanity's diminishing hope, the Survey Corps embark on a mission to retake Wall Maria, where the battle against the merciless ""Titans"" takes the stage once again.

Returning to the tattered Shiganshina District that was once his home, Eren Yeager and the Corps find the town oddly unoccupied by Titans. Even after the outer gate is plugged, they strangely encounter no opposition. The mission progresses smoothly until Armin Arlert, highly suspicious of the enemy's absence, discovers distressing signs of a potential scheme against them.

Shingeki no Kyojin Season 3 Part 2 follows Eren as he vows to take back everything that was once his. Alongside him, the Survey Corps striveâ€”through countless sacrificesâ€”to carve a path towards victory and uncover the secrets locked away in the Yeager family's basement.

[Written by MAL Rewrite]
Poster URL: https://cdn.myanimelist.net/images/anime/1517/100633.webp

--------------------


========================================

Genre: Adventure
Title: Sousou no Frieren
Ranking: 1
Episodes: 0
Status: Not yet aired
Synopsis: The demon king has been defeated, and the victorious hero party returns home before disbanding. The fourâ€”mage Frieren, hero Himmel, priest Heiter, and warrior Eisenâ€”reminisce about their decade-long journey as the moment to bid each other farewell arrives. But the passing of time is different for elves, thus Frieren witnesses her companions slowly pass away one by one.

Before his death, Heiter manages to foist a young human apprentice called Fern onto Frieren. Driven by the elf's passion for collecting a myriad of magic spells, the pair embarks on a seemingly aimless journey, revisiting the places that the heroes of yore had visited. Along their travels, Frieren slowly confronts her regrets of missed opportunities to form deeper bonds with her now-deceased comrades.

[Written by MAL Rewrite]
Poster URL: https://cdn.myanimelist.net/images/anime/1015/138006.webp

--------------------

Title: Fullmetal Alchemist: Brotherhood
Ranking: 2
Episodes: 64
Status: Finished Airing
Synopsis: After a horrific alchemy experiment goes wrong in the Elric household, brothers Edward and Alphonse are left in a catastrophic new reality. Ignoring the alchemical principle banning human transmutation, the boys attempted to bring their recently deceased mother back to life. Instead, they suffered brutal personal loss: Alphonse's body disintegrated while Edward lost a leg and then sacrificed an arm to keep Alphonse's soul in the physical realm by binding it to a hulking suit of armor.

The brothers are rescued by their neighbor Pinako Rockbell and her granddaughter Winry. Known as a bio-mechanical engineering prodigy, Winry creates prosthetic limbs for Edward by utilizing ""automail,"" a tough, versatile metal used in robots and combat armor. After years of training, the Elric brothers set off on a quest to restore their bodies by locating the Philosopher's Stoneâ€”a powerful gem that allows an alchemist to defy the traditional laws of Equivalent Exchange.

As Edward becomes an infamous alchemist and gains the nickname ""Fullmetal,"" the boys' journey embroils them in a growing conspiracy that threatens the fate of the world.

[Written by MAL Rewrite]
Poster URL: https://cdn.myanimelist.net/images/anime/1208/94745.webp

--------------------

Title: Hunter x Hunter (2011)
Ranking: 8
Episodes: 148
Status: Finished Airing
Synopsis: Hunters devote themselves to accomplishing hazardous tasks, all from traversing the world's uncharted territories to locating rare items and monsters. Before becoming a Hunter, one must pass the Hunter Examinationâ€”a high-risk selection process in which most applicants end up handicapped or worse, deceased.

Ambitious participants who challenge the notorious exam carry their own reason. What drives 12-year-old Gon Freecss is finding Ging, his father and a Hunter himself. Believing that he will meet his father by becoming a Hunter, Gon takes the first step to walk the same path.

During the Hunter Examination, Gon befriends the medical student Leorio Paladiknight, the vindictive Kurapika, and ex-assassin Killua Zoldyck. While their motives vastly differ from each other, they band together for a common goal and begin to venture into a perilous world.

[Written by MAL Rewrite]
Poster URL: https://cdn.myanimelist.net/images/anime/1337/99013.webp

--------------------


========================================

Genre: Animation

========================================
",1,1
"I want retrieve information about recently added movies available for streaming in the US, including the title, genre, IMDb rating, release year, synopsis, and streaming platforms. Additionally, fetch basic biographical details about Robert Downey Jr., including his birth name, birth date, and notable works. Finally, get details of three top-ranked anime titles, including their title, genre, ranking, and a brief synopsis.etrieve information about recently added movies available for streaming in the US, including the title, genre, IMDb rating, release year, synopsis, and streaming platforms. Additionally, fetch basic biographical details about Robert Downey Jr., including his birth name, birth date, and notable works. Finally, get details of three top-ranked anime titles, including their title, genre, ranking, and a brief synopsis.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

# Retrieve recently added movies with US streaming availability
def get_new_arrivals():
    url = ""https://ott-details.p.rapidapi.com/getnew""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""ott-details.p.rapidapi.com""
    }
    params = {
        ""region"": ""US"",
        ""page"": ""1""
    }
    response = requests.get(url, headers=headers, params=params)
    return response.json()

# Retrieve basic information about Robert Downey Jr.
def get_actor_info():
    url = ""https://ott-details.p.rapidapi.com/getcastDetails""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""ott-details.p.rapidapi.com""
    }
    params = {""peopleid"": ""nm0000375""}  # IMDb ID for Robert Downey Jr.
    response = requests.get(url, headers=headers, params=params)
    return response.json()

# Retrieve top-ranked anime titles
def get_top_anime():
    url = ""https://anime-db.p.rapidapi.com/anime""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""anime-db.p.rapidapi.com""
    }
    params = {
        ""page"": ""1"",
        ""size"": ""3"",  # Retrieve top 3 anime
        ""sortBy"": ""ranking"",
        ""sortOrder"": ""asc""
    }
    response = requests.get(url, headers=headers, params=params)
    return response.json()

# Main function to get and display all required details
if __name__ == '__main__':
    # Step 1: Get new movie arrivals
    new_arrivals = get_new_arrivals()
    print(""Recently Added Movies:"")
    for movie in new_arrivals.get(""results"", [])[:3]:  # Limiting to 3 movies
        title = movie.get(""title"")
        genres = movie.get(""genre"", [])
        imdb_rating = movie.get(""imdbrating"")
        release_year = movie.get(""released"")
        synopsis = movie.get(""synopsis"")
        streaming_info = movie.get(""streamingAvailability"", {}).get(""country"", {}).get(""US"", [])

        print(f""Title: {title}"")
        print(f""Genres: {', '.join(genres)}"")
        print(f""IMDb Rating: {imdb_rating}"")
        print(f""Release Year: {release_year}"")
        print(f""Synopsis: {synopsis}"")
        print(""Available on:"")
        for platform in streaming_info:
            print(f"" - {platform['platform']}: {platform['url']}"")
        print(""\n"" + ""-""*20 + ""\n"")
    print(""\n"" + ""=""*40 + ""\n"")

    # Step 2: Get Robert Downey Jr.'s details
    actor_info = get_actor_info()
    print(""Actor Bio:"")
    print(f""Name: {actor_info.get('name')}"")
    print(f""Birth Name: {actor_info.get('birthName')}"")
    print(f""Born: {actor_info.get('born')}"")
    print(""Notable Works:"")
    for title_id in actor_info.get(""knownForTitles"", []):
        print(f"" - {title_id}"")
    print(""\n"" + ""=""*40 + ""\n"")

    # Step 3: Retrieve top anime
    anime_list = get_top_anime().get(""data"", [])
    print(""Top-Ranked Anime:"")
    for anime in anime_list:
        title = anime.get(""title"")
        ranking = anime.get(""ranking"")
        genres = anime.get(""genres"", [])
        synopsis = anime.get(""synopsis"")

        print(f""Title: {title}"")
        print(f""Ranking: {ranking}"")
        print(f""Genres: {', '.join(genres)}"")
        print(f""Synopsis: {synopsis}"")
        print(""\n"" + ""-""*20 + ""\n"")
","Recently Added Movies:

========================================

Actor Bio:
Name: Robert Downey Jr.
Birth Name: Robert John Downey Jr
Born: April 4,      1965      inÂ Manhattan, New York City, New York, USA
Notable Works:
 - tt0371746
 - tt1300854
 - tt4154796
 - tt0988045

========================================

Top-Ranked Anime:
Title: Sousou no Frieren
Ranking: 1
Genres: Adventure, Drama, Fantasy
Synopsis: The demon king has been defeated, and the victorious hero party returns home before disbanding. The fourâ€”mage Frieren, hero Himmel, priest Heiter, and warrior Eisenâ€”reminisce about their decade-long journey as the moment to bid each other farewell arrives. But the passing of time is different for elves, thus Frieren witnesses her companions slowly pass away one by one.

Before his death, Heiter manages to foist a young human apprentice called Fern onto Frieren. Driven by the elf's passion for collecting a myriad of magic spells, the pair embarks on a seemingly aimless journey, revisiting the places that the heroes of yore had visited. Along their travels, Frieren slowly confronts her regrets of missed opportunities to form deeper bonds with her now-deceased comrades.

[Written by MAL Rewrite]

--------------------

Title: Fullmetal Alchemist: Brotherhood
Ranking: 2
Genres: Action, Adventure, Drama, Fantasy
Synopsis: After a horrific alchemy experiment goes wrong in the Elric household, brothers Edward and Alphonse are left in a catastrophic new reality. Ignoring the alchemical principle banning human transmutation, the boys attempted to bring their recently deceased mother back to life. Instead, they suffered brutal personal loss: Alphonse's body disintegrated while Edward lost a leg and then sacrificed an arm to keep Alphonse's soul in the physical realm by binding it to a hulking suit of armor.

The brothers are rescued by their neighbor Pinako Rockbell and her granddaughter Winry. Known as a bio-mechanical engineering prodigy, Winry creates prosthetic limbs for Edward by utilizing ""automail,"" a tough, versatile metal used in robots and combat armor. After years of training, the Elric brothers set off on a quest to restore their bodies by locating the Philosopher's Stoneâ€”a powerful gem that allows an alchemist to defy the traditional laws of Equivalent Exchange.

As Edward becomes an infamous alchemist and gains the nickname ""Fullmetal,"" the boys' journey embroils them in a growing conspiracy that threatens the fate of the world.

[Written by MAL Rewrite]

--------------------

Title: Steins;Gate
Ranking: 3
Genres: Drama, Sci-Fi, Suspense
Synopsis: Eccentric scientist Rintarou Okabe has a never-ending thirst for scientific exploration. Together with his ditzy but well-meaning friend Mayuri Shiina and his roommate Itaru Hashida, Rintarou founds the Future Gadget Laboratory in the hopes of creating technological innovations that baffle the human psyche. Despite claims of grandeur, the only notable ""gadget"" the trio have created is a microwave that has the mystifying power to turn bananas into green goo.

However, when Rintarou decides to attend neuroscientist Kurisu Makise's conference on time travel, he experiences a series of strange events that lead him to believe that there is more to the ""Phone Microwave"" gadget than meets the eye. Apparently able to send text messages into the past using the microwave, Rintarou dabbles further with the ""time machine,"" attracting the ire and attention of the mysterious organization SERN.

Due to the novel discovery, Rintarou and his friends find themselves in an ever-present danger. As he works to mitigate the damage his invention has caused to the timeline, he is not only fighting a battle to save his loved ones, but also one against his degrading sanity.

[Written by MAL Rewrite]

--------------------",1,1
"I need to get a list of countries and, based on that, retrieve customer reviews for a specific product. Show me the product's top three reviews, including user details, review date, rating, content, and any images provided by the reviewer. Include the name of the country where each review was submitted if available.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

# Retrieve the list of countries
def get_countries():
    url = ""https://unofficial-shein.p.rapidapi.com/countries/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""unofficial-shein.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

# Retrieve customer reviews for a specific product
def get_reviews(goods_spu):
    url = ""https://unofficial-shein.p.rapidapi.com/reviews/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""unofficial-shein.p.rapidapi.com""
    }
    params = {
        ""goods_spu"": goods_spu,
        ""language"": ""en"",
        ""country"": ""US"",
        ""limit"": ""3"",
        ""page"": ""1""
    }
    response = requests.get(url, headers=headers, params=params)
    return response.json()

# Main function to retrieve countries and display product reviews
if __name__ == '__main__':
    # Step 1: Get the list of countries
    countries_data = get_countries()
    countries = countries_data.get(""info"", {}).get(""country"", {}).get(""item_cates"", [])
    print(""Countries List:"")
    for country in countries:
        print(f""{country['value']} - {country['country']}"")
    print(""\n"" + ""=""*40 + ""\n"")

    # Step 2: Retrieve reviews for a specific product
    reviews_data = get_reviews(""m22022854841"")  # Example goods_spu
    print(""Customer Reviews:"")
    for review in reviews_data.get(""info"", {}).get(""comment_info"", []):
        user_name = review.get(""user_name"")
        review_date = review.get(""commentTimeNewStyle"")
        rating = review.get(""comment_rank"")
        content = review.get(""content"")
        country_code = review.get(""language_flag"")
        images = [img.get(""member_image_small"") for img in review.get(""comment_image"", [])]

        # Display review information
        print(f""User: {user_name}"")
        print(f""Date: {review_date}"")
        print(f""Rating: {rating}"")
        print(f""Review: {content}"")
        if country_code:
            print(f""Country Code: {country_code}"")
        if images:
            print(""Images:"")
            for img in images:
                print(f"" - {img}"")
        print(""\n"" + ""-""*20 + ""\n"")
","Countries List:
AL - Albania
AD - Andorra
AO - Angola
AI - Anguilla
AG - Antigua and Barbuda
AR - Argentina
AM - Armenia
AW - Aruba
AU - Australia
AT - Austria
AZ - Azerbaijan
BS - Bahamas
BH - Bahrain
BB - Barbados
BE - Belgium
BZ - Belize
BJ - Benin
BM - Bermuda
BT - Bhutan
BO - Bolivia
BW - Botswana
BR - Brazil
BN - Brunei Darussalam
BG - Bulgaria
BF - Burkina Faso
BI - Burundi
KH - Cambodia
CM - Cameroon
CA - Canada
CV - Cape Verde
KY - Cayman Islands
CF - Central African Republic
TD - Chad
CL - Chile
CO - Colombia
KM - Comoros
CG - Congo
CK - Cook Islands
CR - Costa Rica
HR - Croatia (local name: Hrvatska)
CY - Cyprus
CZ - Czech Republic
DK - Denmark
DJ - Djibouti
DM - Dominica
DO - Dominican Republic
TL - East Timor
EC - Ecuador
EG - Egypt
SV - El Salvador
GQ - Equatorial Guinea
ER - Eritrea
EE - Estonia
ET - Ethiopia
FJ - Fiji
FI - Finland
FR - France
GA - Gabon
GM - Gambia
GE - Georgia
DE - Germany
GH - Ghana
GI - Gibraltar
GR - Greece
GL - Greenland
GD - Grenada
GP - Guadeloupe
GU - Guam
GN - Guinea
GW - Guinea-Bissau
GY - Guyana
HT - Haiti
HN - Honduras
HU - Hungary
IS - Iceland
ID - Indonesia
IE - Ireland
IL - Israel
IT - Italy
JM - Jamaica
JP - Japan
JO - Jordan
KZ - Kazakhstan
KE - Kenya
KW - Kuwait
KG - Kyrgyzstan
LA - Lao People's Democratic Republic
LV - Latvia
LB - Lebanon
LS - Lesotho
LR - Liberia
LI - Liechtenstein
LT - Lithuania
LU - Luxembourg
MO - Macau
MG - Madagascar
MW - Malawi
MY - Malaysia
MV - Maldives
ML - Mali
MT - Malta
MQ - Martinique
MU - Mauritius
YT - Mayotte
MX - Mexico
MD - Moldova
MN - Mongolia
ME - Montenegro
MA - Morocco
MZ - Mozambique
MM - Myanmar
NA - Namibia
NP - Nepal
NL - Netherlands
NC - New Caledonia
NZ - New Zealand
NG - Nigeria
NF - Norfolk Island
NO - Norway
OM - Oman
PK - Pakistan
PS - Palestine
PA - Panama
PG - Papua New Guinea
PY - Paraguay
PE - Peru
PH - Philippines
PL - Poland
PT - Portugal
PR - Puerto Rico
QA - Qatar
RE - Reunion
RO - Romania
RW - Rwanda
LC - Saint Lucia
SM - San Marino
SA - Saudi Arabia
RS - Serbia
SN - Senegal
SC - Seychelles
SL - Sierra Leone
SG - Singapore
SK - Slovakia (Slovak Republic)
SI - Slovenia
SO - Somalia
ZA - South Africa
KR - South Korea
ES - Spain
LK - Sri Lanka
SR - Suriname
SZ - Swaziland
SE - Sweden
CH - Switzerland
TW - Taiwan
TJ - Tajikistan
TZ - Tanzania
TH - Thailand
TG - Togo
TT - Trinidad and Tobago
TR - Turkey
TC - Turks and Caicos Islands
UG - Uganda
UA - Ukraine
AE - United Arab Emirates
GB - United Kingdom
US - United States
UZ - Uzbekistan
VU - Vanuatu
VA - Vatican City State (Holy See)
VN - Vietnam
VI - Virgin Islands (U.S.)
ZM - Zambia
ZW - Zimbabwe

========================================

Customer Reviews:
User: A***y
Date: 1 year ago
Rating: 5
Review:
Country Code: en
Images:
 - images3/2023/03/28/16799400683fd3568c434d692b7b43d73654558792_thumbnail_80x120.webp
 - images3/2023/03/28/167994007052833fb422a619aec64a2dcb5ee5635c_thumbnail_80x120.webp
 - images3/2023/03/28/1679940073e3e75b58fb7dfec40b2e42725ce7f87a_thumbnail_80x120.webp

--------------------

User: n***0
Date: 1 year ago
Rating: 5
Review: Love the shirt, very funny. Good material
Country Code: en
Images:
 - images3/2023/03/28/16799350097f7670592a47de5874da4f9ef287e901_thumbnail_80x120.webp

--------------------

User: T***a
Date: 1 year ago
Rating: 5
Review:
Country Code: en
Images:
 - images3/2023/03/22/1679450297277064458a47827a110c115354016325_thumbnail_80x120.webp

--------------------
",1,1
"I need to view a list of countries and select specific ones to retrieve detailed information, including supported languages and currencies. Display the country's full name, supported languages, and currency details for each selected country.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

# Retrieve the list of countries
def get_countries():
    url = ""https://unofficial-shein.p.rapidapi.com/countries/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""unofficial-shein.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

# Retrieve detailed information about a specific country
def get_country_details(region):
    url = ""https://unofficial-shein.p.rapidapi.com/countries/detail""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""unofficial-shein.p.rapidapi.com""
    }
    params = {""region"": region}
    response = requests.get(url, headers=headers, params=params)
    return response.json()

# Main function to retrieve countries and display details for selected ones
if __name__ == '__main__':
    # Step 1: Get the list of countries
    countries_data = get_countries()
    countries = countries_data.get(""info"", {}).get(""country"", {}).get(""item_cates"", [])

    print(""Countries List:"")
    for country in countries[:3]:  # Limiting to the first three countries for demonstration
        print(f""{country['value']} - {country['country']}"")

    print(""\n"" + ""=""*40 + ""\n"")

    # Step 2: Retrieve and display details for selected countries
    for country in countries[:3]:  # Retrieve details for the first three countries
        region_code = country[""value""]
        details = get_country_details(region_code)

        print(f""Country: {details.get('info', {}).get('current_country_full_name')}"")

        # Display supported languages
        print(""Supported Languages:"")
        for lang in details.get(""info"", {}).get(""support_language_list"", []):
            print(f"" - {lang['languageTip']} ({lang['language']})"")

        # Display currency details
        print(""Supported Currencies:"")
        for currency in details.get(""info"", {}).get(""suppport_currency_list"", []):
            print(f"" - {currency['code']} ({currency['symbol_left']}{currency['symbol_right']}), Exchange Rate: {currency['value']}"")

        print(""\n"" + ""-""*20 + ""\n"")
",Fail,-1,-1
"I need to view a list of navigation tabs with detailed information, including each tabâ€™s ID, name, associated channel, and category IDs. Then, based on a specified tabâ€™s channel ID, retrieve the root categories within that channel to see each categoryâ€™s name, ID, and sub-categories.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

# Task 1: Retrieve navigation tabs
def get_navigation_tabs():
    url = ""https://unofficial-shein.p.rapidapi.com/navigations/get-tabs""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""unofficial-shein.p.rapidapi.com""
    }
    params = {
        ""language"": ""en"",
        ""country"": ""US"",
        ""currency"": ""USD""
    }
    response = requests.get(url, headers=headers, params=params)
    return response.json()

# Task 2: Retrieve root categories based on a specific channel type
def get_root_categories(channel_type):
    url = ""https://unofficial-shein.p.rapidapi.com/navigations/get-root""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""unofficial-shein.p.rapidapi.com""
    }
    params = {
        ""language"": ""en"",
        ""country"": ""US"",
        ""channelType"": channel_type,
        ""currency"": ""USD""
    }
    response = requests.get(url, headers=headers, params=params)
    return response.json()

if __name__ == '__main__':
    # Step 1: Get and display navigation tabs
    navigation_data = get_navigation_tabs()
    tabs = navigation_data.get(""info"", {}).get(""tabs"", [])

    print(""Navigation Tabs List:"")
    for tab in tabs[:3]:  # Displaying the first three tabs for demonstration
        print(f""Tab ID: {tab['id']}"")
        print(f""Tab Name: {tab['name']}"")
        print(f""Channel Name: {tab['channelName']}"")
        print(f""Category IDs: {tab['cat_id']}"")
        print(""\n"" + ""-""*20 + ""\n"")

    # Example: Using the first tabâ€™s ID as the channelType parameter for the next API call
    channel_type = tabs[0]['id'] if tabs else None  # Checking if there's at least one tab

    if channel_type:
        # Step 2: Get and display root categories for the specified channel type
        root_data = get_root_categories(channel_type)
        categories = root_data.get(""info"", {}).get(""content"", [])

        print(""Root Categories List:"")
        for category in categories[:3]:  # Displaying the first three root categories
            print(f""Category Name: {category.get('name', 'N/A')}"")
            print(f""Category ID: {category.get('id', 'N/A')}"")
            print(""Sub-categories:"")
            for child in category.get(""child"", [])[:3]:  # Limiting to 3 sub-categories
                print(f"" - {child.get('name', 'Unnamed')} (ID: {child.get('navNodeId', 'N/A')})"")
            print(""\n"" + ""-""*20 + ""\n"")
",Fail,-1,-1
"I need a list of all available countries, categorized into featured and standard countries, and their identifiers. Additionally, I want the navigation tabs with details for each tab, including their names, associated channels, and category IDs. Please provide results for three countries and three tabs.","API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

import requests

def get_countries_list():
    url = ""https://unofficial-shein.p.rapidapi.com/countries/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""unofficial-shein.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json().get(""info"", {}).get(""country"", {})

def get_navigation_tabs(language=""en"", country=""US"", currency=""USD""):
    url = ""https://unofficial-shein.p.rapidapi.com/navigations/get-tabs""
    querystring = {""language"": language, ""country"": country, ""currency"": currency}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""unofficial-shein.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json().get(""info"", {}).get(""tabs"", [])

if __name__ == '__main__':
    # Step 1: Retrieve the list of countries
    countries_info = get_countries_list()

    # Display first three results from featured and all countries
    featured_countries = countries_info.get(""hotcountry"", [])[:3]
    all_countries = countries_info.get(""item_cates"", [])[:3]

    print(""Featured Countries:"")
    for country in featured_countries:
        print(f""- {country['country']} (Code: {country['value']}, ID: {country['id']})"")

    print(""\nAll Countries:"")
    for country in all_countries:
        print(f""- {country['country']} (Code: {country['value']}, ID: {country['id']})"")

    # Step 2: Retrieve navigation tabs
    navigation_tabs = get_navigation_tabs()

    print(""\nNavigation Tabs:"")
    for tab in navigation_tabs[:3]:  # Display first three tabs
        print(f""- Tab Name: {tab['name']}, Channel: {tab['channelName']}, Category IDs: {tab['cat_id']}"")
","Featured Countries:
- United States (Code: US, ID: 226)

All Countries:
- Albania (Code: AL, ID: 2)
- Andorra (Code: AD, ID: 5)
- Angola (Code: AO, ID: 6)

Navigation Tabs:
- Tab Name: All, Channel: All, Category IDs: 1864,2026,2031,2032,2038,2273,2297,2400,3195,3224,3631,3634,3636,3637,3650,3657,4327,4328,4436,4083
- Tab Name: Women, Channel: Women, Category IDs: 4436,2041,2042,3644,7469,4816,4815,4813,4811,4808,4806,4804,3863,3694,3686,2475,4451,4801,4802,2031,2032,4314,4295,4293,4292,3812,3627,3626,3625,2347,2346,2208,2273,2297,2400,1894,2491,4329,4331,4335,4350,4366,4379,4396,4409,4413,4422,4424,5106,5247,5290,5301,5348,5796,5797,6376,7152,3224,1765,5806,5826,5906,5907,6219,6217,4656,4438,4219,3632,3035,3029,1745,4803,5928,2043,4099,4258,4268,4458,3650,3657,4083,4327,4328,2462,2463,2464,2465
- Tab Name: Curve, Channel: PlusSize, Category IDs: 1888,3734,2346,2347,2491,3613
",1,1
"I need a list of countries, including their featured and full lists with each countryâ€™s unique identifiers and short codes. Then, for three specific countries from this list, I need to retrieve additional details, including supported languages and currency settings. Finally, I want to view the top reviews for a product based on available product identifiers, including user information, product details, and review comments. Please provide at least three reviews for context.","API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

import requests

def get_countries_list():
    url = ""https://unofficial-shein.p.rapidapi.com/countries/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""unofficial-shein.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json().get(""info"", {}).get(""country"", {})

def get_country_detail(region):
    url = ""https://unofficial-shein.p.rapidapi.com/countries/detail""
    querystring = {""region"": region}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""unofficial-shein.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json().get(""info"", {})

def get_reviews_list(goods_spu, cat_id, limit=3):
    url = ""https://unofficial-shein.p.rapidapi.com/reviews/list""
    querystring = {
        ""language"": ""en"",
        ""country"": ""US"",
        ""currency"": ""USD"",
        ""goods_spu"": goods_spu,
        ""cat_id"": cat_id,
        ""sort"": ""default"",
        ""limit"": str(limit),
        ""page"": ""1""
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""unofficial-shein.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json().get(""info"", {}).get(""comment_info"", [])

if __name__ == '__main__':
    # Step 1: Retrieve the list of countries
    countries_info = get_countries_list()

    # Display featured and all countries
    featured_countries = countries_info.get(""hotcountry"", [])[:3]
    all_countries = countries_info.get(""item_cates"", [])[:3]

    print(""Featured Countries:"")
    for country in featured_countries:
        print(f""- {country['country']} (Code: {country['value']}, ID: {country['id']})"")

    print(""\nAll Countries:"")
    for country in all_countries:
        print(f""- {country['country']} (Code: {country['value']}, ID: {country['id']})"")

    # Step 2: Retrieve details for each featured country
    print(""\nCountry Details:"")
    for country in featured_countries:
        country_detail = get_country_detail(country['value'])
        print(f""\nDetails for {country['country']}:"")
        print(f""  - Supported Languages: {[lang['languageTip'] for lang in country_detail.get('support_language_list', [])]}"")
        print(f""  - Currencies: {[currency['code'] for currency in country_detail.get('suppport_currency_list', [])]}"")
        print(f""  - Full Name: {country_detail.get('current_country_full_name')}"")

    # Step 3: Retrieve and display reviews for a specific product
    goods_spu = ""m22022854841""  # Example product ID
    cat_id = ""1980""             # Example category ID
    reviews = get_reviews_list(goods_spu, cat_id)

    print(""\nProduct Reviews:"")
    for review in reviews[:3]:  # Displaying the top 3 reviews
        print(f""\nReview by {review['user_name']}:"")
        print(f""  - Rating: {review['comment_rank']}"")
        print(f""  - Comment: {review['content']}"")
        if review.get('comment_image'):
            print(""  - Images:"")
            for img in review['comment_image']:
                print(f""    - {img['member_image_small']}"")
","Featured Countries:
- United States (Code: US, ID: 226)

All Countries:
- Albania (Code: AL, ID: 2)
- Andorra (Code: AD, ID: 5)
- Angola (Code: AO, ID: 6)

Country Details:

Details for United States:
  - Supported Languages: ['English', 'EspaÃ±ol']
  - Currencies: ['USD', 'EUR', 'GBP', 'CHF', 'CAD', 'MXN']
  - Full Name: United States

Product Reviews:

Review by A***y:
  - Rating: 5
  - Comment:
  - Images:
    - images3/2023/03/28/16799400683fd3568c434d692b7b43d73654558792_thumbnail_80x120.webp
    - images3/2023/03/28/167994007052833fb422a619aec64a2dcb5ee5635c_thumbnail_80x120.webp
    - images3/2023/03/28/1679940073e3e75b58fb7dfec40b2e42725ce7f87a_thumbnail_80x120.webp

Review by n***0:
  - Rating: 5
  - Comment: Love the shirt, very funny. Good material
  - Images:
    - images3/2023/03/28/16799350097f7670592a47de5874da4f9ef287e901_thumbnail_80x120.webp

Review by T***a:
  - Rating: 5
  - Comment:
  - Images:
    - images3/2023/03/22/1679450297277064458a47827a110c115354016325_thumbnail_80x120.webp
",1,1
"I need to retrieve the latest news articles from a specified source along with their publication details, images, and URLs. Additionally, I want a list of general recent news articles from various sources, including titles, publication dates, and brief previews. Please provide as many results as possible, with at least three articles from each category.","API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

import requests

def get_news_from_source(source_id=""bbc""):
    url = f""https://biztoc.p.rapidapi.com/news/source/{source_id}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""biztoc.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()[:3]  # Retrieve the first 3 articles

def get_latest_news():
    url = ""https://biztoc.p.rapidapi.com/news/latest""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""biztoc.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()[:3]  # Retrieve the first 3 articles

if __name__ == '__main__':
    # Step 1: Retrieve the latest news from a specific source (e.g., BBC)
    source_news = get_news_from_source(""bbc"")
    print(""Latest News from Source (BBC):"")
    for article in source_news:
        print(f""\nTitle: {article['title']}"")
        print(f""Published Date: {article['published']}"")
        print(f""URL: {article['url']}"")
        print(f""Image (Small): {article['img']['s']}"")
        print(f""Image (Square): {article['img']['sq']}"")
        print(f""Summary: {article['body']}"")

    # Step 2: Retrieve the latest general news
    latest_news = get_latest_news()
    print(""\nLatest General News:"")
    for article in latest_news:
        print(f""\nTitle: {article['title']}"")
        print(f""Source: {article['domain']}"")
        print(f""Published Date: {article['published']}"")
        print(f""URL: {article['url']}"")
        if article.get('img'):
            print(f""Image (Small): {article['img']['s']}"")
            print(f""Image (Square): {article['img']['sq']}"")
        print(f""Preview: {article['body_preview'] if 'body_preview' in article else article['body']}"")
","Latest News from Source (BBC):

Title: Firms must prevent sexual harassment under new law
Published Date: Fri, 25 Oct 2024 23:43:11 GMT
URL: https://www.bbc.co.uk/news/business-65311346?ref=biztoc.com
Image (Small): https://cdn.biztoc.com/1a0da8ad8878aa79_s.webp
Image (Square): https://cdn.biztoc.com/1a0da8ad8878aa79_sq.webp
Summary: Employers must take steps to prevent sexual harassment in the workplace, under a new law.
The Worker Protection Act also states employers must discipline or hold accountable those who are found guilty of sexual harassment.
The new rules from 26 October include harassment via social media and alsoâ€¦

Title: Employers' National Insurance hike to raise Â£20bn
Published Date: Fri, 25 Oct 2024 21:57:05 GMT
URL: https://www.bbc.com/news/articles/c9wrkngvyx4o?ref=biztoc.com
Image (Small): https://cdn.biztoc.com/3263d1f20185ed35_s.webp
Image (Square): https://cdn.biztoc.com/3263d1f20185ed35_sq.webp
Summary: Employers' National Insurance hike to raise Â£20bn
The chancellor is set to increase the amount employers in the UK pay in National Insurance to raise Â£20bn for public services at the Budget.
Rachel Reeves is also expected to lower the threshold for when employers start paying the tax, but is notâ€¦

Title: 'I can't run a business like this': Why the WordPress row matters
Published Date: Fri, 25 Oct 2024 21:55:34 GMT
URL: https://www.bbc.com/news/articles/cm2y7eyp3zpo?ref=biztoc.com
Image (Small): https://cdn.biztoc.com/48d5475d43274c4e_s.webp
Image (Square): https://cdn.biztoc.com/48d5475d43274c4e_sq.webp
Summary: 'I can't run a business like this': Why the WordPress row matters
One of the world's biggest web publishing platforms - used by a large chunk of the internet - is locked in a spat which is affecting thousands of businesses worldwide.
While most of the work WordPress does is not seen by internetâ€¦

Latest General News:

Title: Capcom Co., Ltd. (OTCMKTS:CCOEY) Short Interest Down 63.1% in October
Source: marketbeat.com
Published Date: 2024-10-26 17:27:17
URL: https://www.marketbeat.com/instant-alerts/otcmkts-ccoey-options-data-report-2024-10-26/?ref=biztoc.com
Preview: Capcom Co., Ltd. (OTCMKTS:CCOEY - Get Free Report) was the target of a large decline in short interest in October. As of October 15th, there was short interest totalling 10,300 shares, a decline of 63.1% from the September 30th total of 27,900 shares. Based on an average trading volume of 31,400â€¦

Title: CBB Bancorp, Inc. (OTCMKTS:CBBI) Short Interest Update
Source: marketbeat.com
Published Date: 2024-10-26 17:27:16
URL: https://www.marketbeat.com/instant-alerts/otcmkts-cbbi-options-data-report-2024-10-26/?ref=biztoc.com
Preview: CBB Bancorp, Inc. (OTCMKTS:CBBI - Get Free Report) was the target of a large drop in short interest in October. As of October 15th, there was short interest totalling 100 shares, a drop of 50.0% from the September 30th total of 200 shares. Based on an average daily volume of 6,200 shares, theâ€¦

Title: China Resources Building Materials Technology Holdings Limited (OTCMKTS:CARCY) Short Interest Up 125.0% in October
Source: marketbeat.com
Published Date: 2024-10-26 17:27:15
URL: https://www.marketbeat.com/instant-alerts/otcmkts-carcy-options-data-report-2024-10-26/?ref=biztoc.com
Preview: China Resources Building Materials Technology Holdings Limited (OTCMKTS:CARCY - Get Free Report) saw a significant growth in short interest in the month of October. As of October 15th, there was short interest totalling 900 shares, a growth of 125.0% from the September 30th total of 400 shares.â€¦
",1,1
"I need a list of available news sources, including each source's title and website URL, excluding social media and ticker sources. Additionally, I want detailed information on movies or TV shows that match a specific search term, including details such as images, genre, release date, streaming sources, and average rating. Provide as many as three items for each request.","API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

import requests

def get_news_sources():
    url = ""https://biztoc.p.rapidapi.com/sources""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""biztoc.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()[:3]  # Retrieve the first 3 sources

def search_movies_or_tv(query=""breaking bad""):
    url = ""https://movies-api14.p.rapidapi.com/search""
    querystring = {""query"": query}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""movies-api14.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json().get(""contents"", [])[:3]  # Retrieve the first 3 search results

if __name__ == '__main__':
    # Step 1: Retrieve the list of news sources
    news_sources = get_news_sources()
    print(""Available News Sources:"")
    for source in news_sources:
        print(f""\nSource Title: {source['title']}"")
        print(f""Website: {source['web']}"")

    # Step 2: Search for a specific movie or TV show (e.g., ""Breaking Bad"")
    search_results = search_movies_or_tv(""breaking bad"")
    print(""\nMovie or TV Show Search Results:"")
    for item in search_results:
        print(f""\nTitle: {item['title']}"")
        print(f""Original Title: {item['original_title']}"")
        print(f""Release Date: {item['release_date']}"")
        print(f""Genres: {', '.join(item['genres'])}"")
        print(f""Overview: {item['overview']}"")
        print(f""Rating: {item['vote_average']} / 10"")
        print(f""Number of Votes: {item['vote_count']}"")
        print(f""Poster: {item['poster_path']}"")
        print(f""Backdrop: {item['backdrop_path']}"")
        print(f""YouTube Trailer: {item['youtube_trailer']}"")

        print(""Available on:"")
        for source in item.get(""sources"", []):
            display_name = source.get(""display_name"", ""Unknown Platform"")
            access_type = source.get(""type"", ""Unknown Access Type"")
            link = source.get(""link"", ""No link available"")
            print(f""  - {display_name} ({access_type}): {link}"")
",Fail,-1,-1
"I need a summary of the latest news articles across different topics, including their titles, sources, publication dates, and links. Also, please provide a list of trending news topics, each with the most popular articles related to the topic. Additionally, I want a list of movies or TV shows based on a specific keyword or title, along with details like release date, rating, genre, trailer link, and streaming platforms.","API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

import requests

def fetch_latest_news():
    url = ""https://biztoc.p.rapidapi.com/news/latest""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""biztoc.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def fetch_trending_news_topics():
    url = ""https://biztoc.p.rapidapi.com/news/topics""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""biztoc.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def search_movie_or_tv_show(query):
    url = ""https://movies-api14.p.rapidapi.com/search""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""movies-api14.p.rapidapi.com""
    }
    querystring = {""query"": query}
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

if __name__ == '__main__':
    # Fetch the latest news articles
    latest_news = fetch_latest_news()
    print(""Latest News Articles:"")
    for news in latest_news[:3]:  # Limiting to the first three articles for brevity
        print(f""Title: {news['title']}\nSource: {news['domain']}\nPublished: {news['published']}\nURL: {news['url']}\n"")

    # Fetch trending news topics and related articles
    trending_topics = fetch_trending_news_topics()
    print(""\nTrending News Topics:"")
    for topic in trending_topics[:3]:  # Limiting to first three topics for brevity
        print(f""Topic: {', '.join(topic['topic'])}"")
        for story in topic['stories'][:3]:  # Limiting to first three stories per topic
            print(f""  Title: {story['title']}\n  Source: {story['domain']}\n  Published: {story['published']}\n  URL: {story['url']}\n"")

    # Search for movies or TV shows based on a keyword
    query = ""breaking bad""  # Replace with your desired search term
    movies_or_shows = search_movie_or_tv_show(query)
    print(""\nMovies or TV Shows Search Results:"")
    for content in movies_or_shows['contents'][:3]:  # Limiting to the first three results for brevity
        print(f""Title: {content['title']}\nRelease Date: {content['release_date']}\nRating: {content['vote_average']}\nGenres: {', '.join(content['genres'])}\nTrailer: {content.get('youtube_trailer', 'N/A')}\n"")
",Fail,-1,-1
"I need to gather the latest news articles across various sections, such as general news, technology, culture, and sports, from global and BBC-exclusive sources, as well as social media in Chinese. The summary should include titles, brief descriptions, image links, and URLs where available. Additionally, I want a selection of trending news in Chinese from popular social media platforms, with details on publication times and direct links","API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

import requests

def fetch_news(lang=""en""):
    url = ""https://bbc-api2.p.rapidapi.com/news""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""bbc-api2.p.rapidapi.com""
    }
    params = {""lang"": lang}
    response = requests.get(url, headers=headers, params=params)
    return response.json()

def fetch_latest_news(lang=""en""):
    url = ""https://bbc-api2.p.rapidapi.com/latest""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""bbc-api2.p.rapidapi.com""
    }
    params = {""lang"": lang}
    response = requests.get(url, headers=headers, params=params)
    return response.json()

def fetch_social_media_news_chinese():
    url = ""https://onelike1.p.rapidapi.com/servicech/news""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""onelike1.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

if __name__ == '__main__':
    # Fetch a brief of categorized news articles
    categorized_news = fetch_news(lang=""en"")
    for section in [""Latest"", ""Only from the BBC"", ""Technology""]:
        articles = categorized_news.get(section, [])
        for article in articles[:1]:  # Limit to the first article
            print(f""{section} Article:\nTitle: {article['title']}\nURL: {article['news_link']}\n"")

    # Fetch a single latest news article
    latest_news = fetch_latest_news(lang=""en"")
    if 'Latest' in latest_news:
        article = latest_news['Latest'][0]
        print(f""Latest News:\nTitle: {article['title']}\nURL: {article['news_link']}\n"")

    # Fetch a single trending news from Chinese social media
    social_media_news = fetch_social_media_news_chinese()
    if 'socialmedias' in social_media_news:
        first_news = social_media_news['socialmedias'][0]['news'][0]
        print(f""Chinese Social Media News:\nTitle: {first_news['title']}\nURL: {first_news['url']}\n"")
","Chinese Social Media News:
Title: ä¼Šæœ—å·¡é€»é˜Ÿåœ¨è¾¹å¢ƒé­æ­¦è£…è¢­å‡» åå£«å…µä¸§å‘½ - 8world News, Lifestyle and Entertainment
URL: https://news.google.com/rss/articles/CBMigwFBVV95cUxNbkhzaGFCMENEUW5kU3FoZGFZX0dPZHl5Q3haOUdBQkwtdFJPUi11TEotTEhtQ3VleFhkN0pOVS1rMElJOS1IQTBxV3hUZUR1UVd4a0RkbmVCSE1lZzZKMmFkSF84WHRuTExnN1pWLVVydGhhdmNiMUQ2V1E3SGJ1TTJrVQ?oc=5

",1,1
"I need a brief overview of a specific joke by its unique identifier, including the setup and punchline, as well as essential details about the planets in our Solar System, with a summary of each planetâ€™s attributes like volume, mass, a brief description, and a link to an image.","API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

import requests

def fetch_joke_by_id(joke_id=10):
    url = f""https://manatee-jokes.p.rapidapi.com/manatees/{joke_id}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""manatee-jokes.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    joke_data = response.json()
    print(""Joke Details:"")
    print(f""Setup: {joke_data['setup']}"")
    print(f""Punchline: {joke_data['punchline']}\n"")
    return joke_data

def fetch_all_planets():
    url = ""https://planets-info-by-newbapi.p.rapidapi.com/api/v1/planets/""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""planets-info-by-newbapi.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    planets_data = response.json()
    print(""Planet Information:"")
    for planet in planets_data[:3]:  # Limiting to the first three planets
        print(f""Name: {planet['name']}"")
        print(f""Description: {planet['description']}"")
        print(f""Volume: {planet['basicDetails']['volume']}"")
        print(f""Mass: {planet['basicDetails']['mass']}"")
        print(f""Image: {planet['imgSrc']['img']}\n"")
    return planets_data

if __name__ == '__main__':
    # Fetch and display a specific joke
    fetch_joke_by_id(joke_id=10)

    # Fetch and display information about the planets
    fetch_all_planets()
","Joke Details:
Setup: What does Miss Pacific Ocean wear on her head?
Punchline: A manatiara

Planet Information:
Name: Mercury
Description: Mercury is the smallest planet in the Solar System and the closest to the Sun. Its orbit around the Sun takes 87.97 Earth days, the shortest of all the Sun's planets.
Volume: 6.083 x 10^10 km^3
Mass: 3.3011 x 10^23 kg
Image: https://upload.wikimedia.org/wikipedia/commons/4/4a/Mercury_in_true_color.jpg

Name: Venus
Description: Venus is the second planet from the Sun and is named after the Roman goddess of love and beauty. As the brightest natural object in Earth's night sky after the Moon, Venus can cast shadows and can be visible to the naked eye in broad daylight.
Volume: 9.2843 x 10^11 km^3
Mass: 4.8675 x 10^24 kg
Image: https://upload.wikimedia.org/wikipedia/commons/0/08/Venus_from_Mariner_10.jpg

Name: Mars
Description: Mars is the fourth planet from the Sun and the second-smallest planet in the Solar System, being larger than only Mercury. In English, Mars carries the name of the Roman god of war and is often called the \""Red Planet\"".""
Volume: 1.63118 x 10^11 km^3
Mass: 6.4171 x 10^23 kg
Image: https://upload.wikimedia.org/wikipedia/commons/0/02/OSIRIS_Mars_true_color.jpg
",1,1
"I need information on a specific plant, including its scientific name, habitat, growth details, and visual characteristics. Additionally, I want details about three movies or TV shows that match a specific keyword, providing information like title, genre, rating, release date, and streaming sources.","API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

import requests

def fetch_plant_details():
    url = ""https://daily-knowledge.p.rapidapi.com/plants-de.json""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""daily-knowledge.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    plant_data = response.json()
    print(""Plant Information:"")
    print(f""Name: {plant_data['name']}"")
    print(f""Scientific Name: {plant_data['basicData']['scName']}"")
    print(f""Habitat: {plant_data['basicData']['home']}"")
    print(f""Climate: {plant_data['basicData']['climate']}"")
    print(f""Appearance: {plant_data['descrData']['infoAppear']}\n"")
    return plant_data

def fetch_movie_details(query=""inception""):
    url = ""https://movies-api14.p.rapidapi.com/search""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""movies-api14.p.rapidapi.com""
    }
    querystring = {""query"": query}
    response = requests.get(url, headers=headers, params=querystring)
    movies_data = response.json()
    print(""Movies Information:"")
    for movie in movies_data['contents'][:3]:  # Limiting to the first three results
        print(f""Title: {movie['title']}"")
        print(f""Genre: {', '.join(movie['genres'])}"")
        print(f""Rating: {movie['vote_average']}"")
        print(f""Release Date: {movie['release_date']}"")
        print(f""Streaming Sources: {[source['display_name'] for source in movie.get('sources', [])]}\n"")
    return movies_data

if __name__ == '__main__':
    # Fetch and display details about a specific plant
    fetch_plant_details()

    # Fetch and display details about movies matching a specific keyword
    fetch_movie_details(query=""inception"")
","Plant Information:
Name: Alpen-Fettkraut
Scientific Name: PinguÃ­cula alpÃ­na
Habitat: 17
Climate: 4
Appearance: Die Wurzeln der Pflanze sind lediglich 1 bis 2 cm lang. Die fleischigen LaubblÃ¤tter bilden am Boden eine Rosette. Die BlattoberflÃ¤che ist klebrig, wodurch Insekten gefangen werden. Die Verdauung der Insekten erfolgt mittels eines Enzyms, das aus DrÃ¼sen an der BlattoberflÃ¤che ausgeschieden wird. Die BlÃ¤tter kÃ¶nnen sich bis zur Mitte einrollen.

Movies Information:
Title: Inception
Genre: Action & Adventure, Sci-Fi & Fantasy, Action & Adventure
Rating: 8.362
Release Date: 2010-07-15
Streaming Sources: ['HBO Max', 'Netflix', 'Vudu', 'Apple TV']

Title: The Crack: Inception
Genre: Drama, Thriller
Rating: 6.897
Release Date: 2019-10-04
Streaming Sources: []
",0,0
"I need to calculate the power in watts for both a single-phase and a three-phase system. For the single-phase calculation, use a current of 13 amperes, with the default voltage (230 volts) and power factor (0.95). For the three-phase calculation, also use a current of 13 amperes, with the default three-phase voltage (400 volts) and power factor (0.95).","API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

import requests

def convert_single_phase_current_to_power(current=13, voltage=230, powerfactor=0.95):
    url = ""https://electrical-units.p.rapidapi.com/current_to_power/single_phase""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""electrical-units.p.rapidapi.com""
    }
    querystring = {""current"": str(current), ""voltage"": str(voltage), ""powerfactor"": str(powerfactor)}
    response = requests.get(url, headers=headers, params=querystring)
    single_phase_power = response.json()
    print(""Single-Phase Power Calculation:"")
    print(f""Power: {single_phase_power['power']} watts\n"")
    return single_phase_power

def convert_three_phase_current_to_power(current=13, voltage=400, powerfactor=0.95):
    url = ""https://electrical-units.p.rapidapi.com/current_to_power/three_phase""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""electrical-units.p.rapidapi.com""
    }
    querystring = {""current"": str(current), ""voltage"": str(voltage), ""powerfactor"": str(powerfactor)}
    response = requests.get(url, headers=headers, params=querystring)
    three_phase_power = response.json()
    print(""Three-Phase Power Calculation:"")
    print(f""Power: {three_phase_power['power']} watts\n"")
    return three_phase_power

if __name__ == '__main__':
    # Calculate and display power for single-phase system
    convert_single_phase_current_to_power()

    # Calculate and display power for three-phase system
    convert_three_phase_current_to_power()
","Single-Phase Power Calculation:
Power: 2840.5 watts

Three-Phase Power Calculation:
Power: 8556.33 watts
",1,1
"I need detailed information about a chemical element, including its atomic number, atomic mass, electron configuration, and year of discovery. Additionally, I want to understand the objectives and targets related to the ""Life on Land"" sustainable development goal, with actionable tips for promoting sustainable land use and biodiversity conservation.","API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

import requests

def get_chemical_element_details():
    url = ""https://daily-knowledge.p.rapidapi.com/chemical-elements-en.json""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""daily-knowledge.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    element_data = response.json()

    print(""Chemical Element Details:"")
    print(f""Name: {element_data['name']} ({element_data['symbol']})"")
    print(f""Atomic Number: {element_data['atomicNumber']}"")
    print(f""Atomic Mass: {element_data['atomicMass']}"")
    print(f""Electrons per Shell: {element_data['electronsPerShell']}"")
    print(f""Discovery Date: {element_data['discoveryDate']}\n"")
    return element_data

def get_sustainable_development_goal():
    url = ""https://daily-knowledge.p.rapidapi.com/sustainable-development-goals-en.json""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""daily-knowledge.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    goal_data = response.json()

    print(""Sustainable Development Goal - Life on Land:"")
    print(f""Title: {goal_data['title']}"")
    print(f""Introduction: {goal_data['introduction']}\n"")
    print(""Key Targets:"")
    for target in goal_data['targets'][:3]:  # Limiting to the first three targets
        print(f"" - {target['title']}: {target['body']}"")

    print(""\nActionable Tips:"")
    for tip in goal_data['tips'][:3]:  # Limiting to the first three tips
        print(f"" - {tip}"")
    return goal_data

if __name__ == '__main__':
    # Retrieve and display chemical element details
    get_chemical_element_details()

    # Retrieve and display sustainable development goal information
    get_sustainable_development_goal()
",Fail,-1,-1
"I need the current weather and a 3-day forecast for a specified location. Please provide detailed information on the current conditions, including temperature, wind speed, and humidity, along with a 3-day forecast of daily temperature ranges, precipitation chances, and wind speeds. Include hourly details for each day, and ensure the data includes sunrise, sunset, and moon phase information.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_realtime_weather(location):
    url = ""https://weatherapi-com.p.rapidapi.com/current.json""
    querystring = {""q"": location}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_weather_forecast(location):
    url = ""https://weatherapi-com.p.rapidapi.com/forecast.json""
    querystring = {""q"": location, ""days"": ""3""}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

if __name__ == '__main__':
    # Specify the location, for example, New York
    location = ""New York""

    # Fetch current weather data
    current_weather = get_realtime_weather(location)
    print(""Current Weather Data:"", current_weather)

    # Fetch 3-day weather forecast
    weather_forecast = get_weather_forecast(location)
    print(""3-Day Weather Forecast:"", weather_forecast)
","Current Weather Data: {'location': {'name': 'New York', 'region': 'New York', 'country': 'United States of America', 'lat': 40.7142, 'lon': -74.0064, 'tz_id': 'America/New_York', 'localtime_epoch': 1729965641, 'localtime': '2024-10-26 14:00'}, 'current': {'last_updated_epoch': 1729965600, 'last_updated': '2024-10-26 14:00', 'temp_c': 17.2, 'temp_f': 63.0, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 15.4, 'wind_kph': 24.8, 'wind_degree': 316, 'wind_dir': 'NW', 'pressure_mb': 1020.0, 'pressure_in': 30.12, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 29, 'cloud': 0, 'feelslike_c': 17.2, 'feelslike_f': 63.0, 'windchill_c': 18.3, 'windchill_f': 64.9, 'heatindex_c': 18.3, 'heatindex_f': 64.9, 'dewpoint_c': 9.7, 'dewpoint_f': 49.4, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 5.0, 'gust_mph': 18.9, 'gust_kph': 30.3}}
3-Day Weather Forecast: {'location': {'name': 'New York', 'region': 'New York', 'country': 'United States of America', 'lat': 40.7142, 'lon': -74.0064, 'tz_id': 'America/New_York', 'localtime_epoch': 1729965643, 'localtime': '2024-10-26 14:00'}, 'current': {'last_updated_epoch': 1729965600, 'last_updated': '2024-10-26 14:00', 'temp_c': 17.2, 'temp_f': 63.0, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 15.4, 'wind_kph': 24.8, 'wind_degree': 316, 'wind_dir': 'NW', 'pressure_mb': 1020.0, 'pressure_in': 30.12, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 29, 'cloud': 0, 'feelslike_c': 17.2, 'feelslike_f': 63.0, 'windchill_c': 18.3, 'windchill_f': 64.9, 'heatindex_c': 18.3, 'heatindex_f': 64.9, 'dewpoint_c': 9.7, 'dewpoint_f': 49.4, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 5.0, 'gust_mph': 18.9, 'gust_kph': 30.3}, 'forecast': {'forecastday': [{'date': '2024-10-26', 'date_epoch': 1729900800, 'day': {'maxtemp_c': 19.0, 'maxtemp_f': 66.2, 'mintemp_c': 11.8, 'mintemp_f': 53.3, 'avgtemp_c': 15.1, 'avgtemp_f': 59.1, 'maxwind_mph': 17.7, 'maxwind_kph': 28.4, 'totalprecip_mm': 1.32, 'totalprecip_in': 0.05, 'totalsnow_cm': 0.0, 'avgvis_km': 10.0, 'avgvis_miles': 6.0, 'avghumidity': 57, 'daily_will_it_rain': 1, 'daily_chance_of_rain': 77, 'daily_will_it_snow': 0, 'daily_chance_of_snow': 0, 'condition': {'text': 'Patchy rain nearby', 'icon': '//cdn.weatherapi.com/weather/64x64/day/176.png', 'code': 1063}, 'uv': 4.0}, 'astro': {'sunrise': '07:20 AM', 'sunset': '05:59 PM', 'moonrise': '01:31 AM', 'moonset': '03:41 PM', 'moon_phase': 'Waning Crescent', 'moon_illumination': 34, 'is_moon_up': 0, 'is_sun_up': 0}, 'hour': [{'time_epoch': 1729915200, 'time': '2024-10-26 00:00', 'temp_c': 13.5, 'temp_f': 56.3, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 8.5, 'wind_kph': 13.7, 'wind_degree': 208, 'wind_dir': 'SSW', 'pressure_mb': 1021.0, 'pressure_in': 30.15, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 46, 'cloud': 24, 'feelslike_c': 12.3, 'feelslike_f': 54.1, 'windchill_c': 12.3, 'windchill_f': 54.1, 'heatindex_c': 13.5, 'heatindex_f': 56.3, 'dewpoint_c': 2.9, 'dewpoint_f': 37.1, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 11.7, 'gust_kph': 18.8, 'uv': 0}, {'time_epoch': 1729918800, 'time': '2024-10-26 01:00', 'temp_c': 13.7, 'temp_f': 56.6, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 8.3, 'wind_kph': 13.3, 'wind_degree': 200, 'wind_dir': 'SSW', 'pressure_mb': 1020.0, 'pressure_in': 30.14, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 48, 'cloud': 24, 'feelslike_c': 12.5, 'feelslike_f': 54.5, 'windchill_c': 12.5, 'windchill_f': 54.5, 'heatindex_c': 13.7, 'heatindex_f': 56.6, 'dewpoint_c': 3.3, 'dewpoint_f': 37.9, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 11.7, 'gust_kph': 18.9, 'uv': 0}, {'time_epoch': 1729922400, 'time': '2024-10-26 02:00', 'temp_c': 13.7, 'temp_f': 56.7, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 8.5, 'wind_kph': 13.7, 'wind_degree': 198, 'wind_dir': 'SSW', 'pressure_mb': 1020.0, 'pressure_in': 30.12, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 54, 'cloud': 19, 'feelslike_c': 12.6, 'feelslike_f': 54.7, 'windchill_c': 12.6, 'windchill_f': 54.7, 'heatindex_c': 13.7, 'heatindex_f': 56.7, 'dewpoint_c': 4.9, 'dewpoint_f': 40.7, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 12.3, 'gust_kph': 19.7, 'uv': 0}, {'time_epoch': 1729926000, 'time': '2024-10-26 03:00', 'temp_c': 13.7, 'temp_f': 56.6, 'is_day': 0, 'condition': {'text': 'Patchy rain nearby', 'icon': '//cdn.weatherapi.com/weather/64x64/night/176.png', 'code': 1063}, 'wind_mph': 8.5, 'wind_kph': 13.7, 'wind_degree': 199, 'wind_dir': 'SSW', 'pressure_mb': 1019.0, 'pressure_in': 30.1, 'precip_mm': 0.04, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 60, 'cloud': 87, 'feelslike_c': 12.6, 'feelslike_f': 54.6, 'windchill_c': 12.6, 'windchill_f': 54.6, 'heatindex_c': 13.7, 'heatindex_f': 56.6, 'dewpoint_c': 6.0, 'dewpoint_f': 42.9, 'will_it_rain': 1, 'chance_of_rain': 74, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 12.4, 'gust_kph': 19.9, 'uv': 0}, {'time_epoch': 1729929600, 'time': '2024-10-26 04:00', 'temp_c': 13.6, 'temp_f': 56.6, 'is_day': 0, 'condition': {'text': 'Partly Cloudy ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/116.png', 'code': 1003}, 'wind_mph': 8.5, 'wind_kph': 13.7, 'wind_degree': 199, 'wind_dir': 'SSW', 'pressure_mb': 1018.0, 'pressure_in': 30.07, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 63, 'cloud': 43, 'feelslike_c': 12.6, 'feelslike_f': 54.6, 'windchill_c': 12.6, 'windchill_f': 54.6, 'heatindex_c': 13.6, 'heatindex_f': 56.6, 'dewpoint_c': 6.8, 'dewpoint_f': 44.2, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 12.3, 'gust_kph': 19.9, 'uv': 0}, {'time_epoch': 1729933200, 'time': '2024-10-26 05:00', 'temp_c': 13.6, 'temp_f': 56.5, 'is_day': 0, 'condition': {'text': 'Patchy rain nearby', 'icon': '//cdn.weatherapi.com/weather/64x64/night/176.png', 'code': 1063}, 'wind_mph': 8.1, 'wind_kph': 13.0, 'wind_degree': 204, 'wind_dir': 'SSW', 'pressure_mb': 1018.0, 'pressure_in': 30.06, 'precip_mm': 0.03, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 66, 'cloud': 72, 'feelslike_c': 12.6, 'feelslike_f': 54.7, 'windchill_c': 12.6, 'windchill_f': 54.7, 'heatindex_c': 13.6, 'heatindex_f': 56.5, 'dewpoint_c': 7.5, 'dewpoint_f': 45.4, 'will_it_rain': 1, 'chance_of_rain': 77, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 11.7, 'gust_kph': 18.8, 'uv': 0}, {'time_epoch': 1729936800, 'time': '2024-10-26 06:00', 'temp_c': 13.6, 'temp_f': 56.4, 'is_day': 0, 'condition': {'text': 'Patchy rain nearby', 'icon': '//cdn.weatherapi.com/weather/64x64/night/176.png', 'code': 1063}, 'wind_mph': 7.6, 'wind_kph': 12.2, 'wind_degree': 210, 'wind_dir': 'SSW', 'pressure_mb': 1018.0, 'pressure_in': 30.05, 'precip_mm': 0.13, 'precip_in': 0.01, 'snow_cm': 0.0, 'humidity': 71, 'cloud': 79, 'feelslike_c': 12.5, 'feelslike_f': 54.6, 'windchill_c': 12.5, 'windchill_f': 54.6, 'heatindex_c': 13.6, 'heatindex_f': 56.4, 'dewpoint_c': 8.4, 'dewpoint_f': 47.1, 'will_it_rain': 1, 'chance_of_rain': 100, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 10.8, 'gust_kph': 17.4, 'uv': 0}, {'time_epoch': 1729940400, 'time': '2024-10-26 07:00', 'temp_c': 13.6, 'temp_f': 56.4, 'is_day': 0, 'condition': {'text': 'Light rain shower', 'icon': '//cdn.weatherapi.com/weather/64x64/night/353.png', 'code': 1240}, 'wind_mph': 8.1, 'wind_kph': 13.0, 'wind_degree': 213, 'wind_dir': 'SSW', 'pressure_mb': 1017.0, 'pressure_in': 30.04, 'precip_mm': 0.41, 'precip_in': 0.02, 'snow_cm': 0.0, 'humidity': 76, 'cloud': 53, 'feelslike_c': 12.6, 'feelslike_f': 54.7, 'windchill_c': 12.6, 'windchill_f': 54.7, 'heatindex_c': 13.6, 'heatindex_f': 56.5, 'dewpoint_c': 9.4, 'dewpoint_f': 48.9, 'will_it_rain': 1, 'chance_of_rain': 100, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 11.2, 'gust_kph': 18.0, 'uv': 0}, {'time_epoch': 1729944000, 'time': '2024-10-26 08:00', 'temp_c': 13.7, 'temp_f': 56.6, 'is_day': 1, 'condition': {'text': 'Light rain shower', 'icon': '//cdn.weatherapi.com/weather/64x64/day/353.png', 'code': 1240}, 'wind_mph': 7.2, 'wind_kph': 11.5, 'wind_degree': 235, 'wind_dir': 'SW', 'pressure_mb': 1017.0, 'pressure_in': 30.04, 'precip_mm': 0.24, 'precip_in': 0.01, 'snow_cm': 0.0, 'humidity': 76, 'cloud': 90, 'feelslike_c': 12.8, 'feelslike_f': 55.0, 'windchill_c': 12.8, 'windchill_f': 55.0, 'heatindex_c': 13.7, 'heatindex_f': 56.6, 'dewpoint_c': 9.5, 'dewpoint_f': 49.1, 'will_it_rain': 1, 'chance_of_rain': 100, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 9.6, 'gust_kph': 15.5, 'uv': 3.0}, {'time_epoch': 1729947600, 'time': '2024-10-26 09:00', 'temp_c': 14.2, 'temp_f': 57.5, 'is_day': 1, 'condition': {'text': 'Light rain shower', 'icon': '//cdn.weatherapi.com/weather/64x64/day/353.png', 'code': 1240}, 'wind_mph': 7.6, 'wind_kph': 12.2, 'wind_degree': 245, 'wind_dir': 'WSW', 'pressure_mb': 1018.0, 'pressure_in': 30.05, 'precip_mm': 0.16, 'precip_in': 0.01, 'snow_cm': 0.0, 'humidity': 75, 'cloud': 78, 'feelslike_c': 13.3, 'feelslike_f': 56.0, 'windchill_c': 13.3, 'windchill_f': 56.0, 'heatindex_c': 14.2, 'heatindex_f': 57.5, 'dewpoint_c': 9.5, 'dewpoint_f': 49.1, 'will_it_rain': 1, 'chance_of_rain': 100, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 10.0, 'gust_kph': 16.1, 'uv': 3.0}, {'time_epoch': 1729951200, 'time': '2024-10-26 10:00', 'temp_c': 15.1, 'temp_f': 59.2, 'is_day': 1, 'condition': {'text': 'Light rain shower', 'icon': '//cdn.weatherapi.com/weather/64x64/day/353.png', 'code': 1240}, 'wind_mph': 8.1, 'wind_kph': 13.0, 'wind_degree': 256, 'wind_dir': 'WSW', 'pressure_mb': 1017.0, 'pressure_in': 30.05, 'precip_mm': 0.25, 'precip_in': 0.01, 'snow_cm': 0.0, 'humidity': 73, 'cloud': 63, 'feelslike_c': 14.7, 'feelslike_f': 58.4, 'windchill_c': 14.7, 'windchill_f': 58.4, 'heatindex_c': 15.1, 'heatindex_f': 59.2, 'dewpoint_c': 10.0, 'dewpoint_f': 50.0, 'will_it_rain': 1, 'chance_of_rain': 100, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 10.1, 'gust_kph': 16.3, 'uv': 3.0}, {'time_epoch': 1729954800, 'time': '2024-10-26 11:00', 'temp_c': 16.5, 'temp_f': 61.7, 'is_day': 1, 'condition': {'text': 'Patchy rain nearby', 'icon': '//cdn.weatherapi.com/weather/64x64/day/176.png', 'code': 1063}, 'wind_mph': 8.9, 'wind_kph': 14.4, 'wind_degree': 279, 'wind_dir': 'W', 'pressure_mb': 1017.0, 'pressure_in': 30.05, 'precip_mm': 0.04, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 71, 'cloud': 100, 'feelslike_c': 16.3, 'feelslike_f': 61.3, 'windchill_c': 16.3, 'windchill_f': 61.3, 'heatindex_c': 16.5, 'heatindex_f': 61.7, 'dewpoint_c': 10.8, 'dewpoint_f': 51.5, 'will_it_rain': 0, 'chance_of_rain': 62, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 11.1, 'gust_kph': 17.9, 'uv': 4.0}, {'time_epoch': 1729958400, 'time': '2024-10-26 12:00', 'temp_c': 17.5, 'temp_f': 63.4, 'is_day': 1, 'condition': {'text': 'Overcast ', 'icon': '//cdn.weatherapi.com/weather/64x64/day/122.png', 'code': 1009}, 'wind_mph': 11.0, 'wind_kph': 17.6, 'wind_degree': 307, 'wind_dir': 'NW', 'pressure_mb': 1017.0, 'pressure_in': 30.04, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 66, 'cloud': 100, 'feelslike_c': 17.4, 'feelslike_f': 63.3, 'windchill_c': 17.4, 'windchill_f': 63.3, 'heatindex_c': 17.5, 'heatindex_f': 63.5, 'dewpoint_c': 11.4, 'dewpoint_f': 52.6, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 13.6, 'gust_kph': 21.9, 'uv': 4.0}, {'time_epoch': 1729962000, 'time': '2024-10-26 13:00', 'temp_c': 18.0, 'temp_f': 64.3, 'is_day': 1, 'condition': {'text': 'Overcast ', 'icon': '//cdn.weatherapi.com/weather/64x64/day/122.png', 'code': 1009}, 'wind_mph': 13.2, 'wind_kph': 21.2, 'wind_degree': 310, 'wind_dir': 'NW', 'pressure_mb': 1017.0, 'pressure_in': 30.02, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 62, 'cloud': 100, 'feelslike_c': 17.9, 'feelslike_f': 64.3, 'windchill_c': 17.9, 'windchill_f': 64.3, 'heatindex_c': 18.0, 'heatindex_f': 64.4, 'dewpoint_c': 11.1, 'dewpoint_f': 51.9, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 16.4, 'gust_kph': 26.4, 'uv': 4.0}, {'time_epoch': 1729965600, 'time': '2024-10-26 14:00', 'temp_c': 17.2, 'temp_f': 63.0, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 15.4, 'wind_kph': 24.8, 'wind_degree': 316, 'wind_dir': 'NW', 'pressure_mb': 1020.0, 'pressure_in': 30.12, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 29, 'cloud': 0, 'feelslike_c': 18.3, 'feelslike_f': 64.9, 'windchill_c': 18.3, 'windchill_f': 64.9, 'heatindex_c': 18.3, 'heatindex_f': 64.9, 'dewpoint_c': 9.7, 'dewpoint_f': 49.4, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 16.0, 'vis_miles': 9.0, 'gust_mph': 18.9, 'gust_kph': 30.3, 'uv': 5.0}, {'time_epoch': 1729969200, 'time': '2024-10-26 15:00', 'temp_c': 18.7, 'temp_f': 65.6, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 15.0, 'wind_kph': 24.1, 'wind_degree': 318, 'wind_dir': 'NW', 'pressure_mb': 1017.0, 'pressure_in': 30.04, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 49, 'cloud': 9, 'feelslike_c': 18.6, 'feelslike_f': 65.6, 'windchill_c': 18.6, 'windchill_f': 65.6, 'heatindex_c': 18.7, 'heatindex_f': 65.6, 'dewpoint_c': 7.6, 'dewpoint_f': 45.7, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 18.1, 'gust_kph': 29.1, 'uv': 5.0}, {'time_epoch': 1729972800, 'time': '2024-10-26 16:00', 'temp_c': 18.4, 'temp_f': 65.2, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 16.3, 'wind_kph': 26.3, 'wind_degree': 318, 'wind_dir': 'NW', 'pressure_mb': 1018.0, 'pressure_in': 30.06, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 40, 'cloud': 0, 'feelslike_c': 18.4, 'feelslike_f': 65.2, 'windchill_c': 18.4, 'windchill_f': 65.2, 'heatindex_c': 18.4, 'heatindex_f': 65.2, 'dewpoint_c': 5.0, 'dewpoint_f': 41.0, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 20.4, 'gust_kph': 32.8, 'uv': 5.0}, {'time_epoch': 1729976400, 'time': '2024-10-26 17:00', 'temp_c': 17.3, 'temp_f': 63.2, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 15.4, 'wind_kph': 24.8, 'wind_degree': 322, 'wind_dir': 'NW', 'pressure_mb': 1019.0, 'pressure_in': 30.09, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 40, 'cloud': 0, 'feelslike_c': 17.3, 'feelslike_f': 63.2, 'windchill_c': 17.3, 'windchill_f': 63.2, 'heatindex_c': 17.3, 'heatindex_f': 63.2, 'dewpoint_c': 4.3, 'dewpoint_f': 39.8, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 19.8, 'gust_kph': 31.9, 'uv': 5.0}, {'time_epoch': 1729980000, 'time': '2024-10-26 18:00', 'temp_c': 15.9, 'temp_f': 60.7, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 17.7, 'wind_kph': 28.4, 'wind_degree': 330, 'wind_dir': 'NNW', 'pressure_mb': 1020.0, 'pressure_in': 30.13, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 45, 'cloud': 0, 'feelslike_c': 15.0, 'feelslike_f': 59.0, 'windchill_c': 15.0, 'windchill_f': 59.0, 'heatindex_c': 16.0, 'heatindex_f': 60.7, 'dewpoint_c': 4.2, 'dewpoint_f': 39.5, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 22.9, 'gust_kph': 36.8, 'uv': 0}, {'time_epoch': 1729983600, 'time': '2024-10-26 19:00', 'temp_c': 14.9, 'temp_f': 58.8, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 17.0, 'wind_kph': 27.4, 'wind_degree': 332, 'wind_dir': 'NNW', 'pressure_mb': 1022.0, 'pressure_in': 30.19, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 42, 'cloud': 14, 'feelslike_c': 13.4, 'feelslike_f': 56.2, 'windchill_c': 13.4, 'windchill_f': 56.2, 'heatindex_c': 14.9, 'heatindex_f': 58.8, 'dewpoint_c': 1.9, 'dewpoint_f': 35.4, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 21.9, 'gust_kph': 35.3, 'uv': 0}, {'time_epoch': 1729987200, 'time': '2024-10-26 20:00', 'temp_c': 14.1, 'temp_f': 57.3, 'is_day': 0, 'condition': {'text': 'Cloudy ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/119.png', 'code': 1006}, 'wind_mph': 15.9, 'wind_kph': 25.6, 'wind_degree': 329, 'wind_dir': 'NNW', 'pressure_mb': 1023.0, 'pressure_in': 30.22, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 44, 'cloud': 71, 'feelslike_c': 12.3, 'feelslike_f': 54.1, 'windchill_c': 12.3, 'windchill_f': 54.1, 'heatindex_c': 14.1, 'heatindex_f': 57.3, 'dewpoint_c': 1.8, 'dewpoint_f': 35.3, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 20.6, 'gust_kph': 33.2, 'uv': 0}, {'time_epoch': 1729990800, 'time': '2024-10-26 21:00', 'temp_c': 13.3, 'temp_f': 55.9, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 15.2, 'wind_kph': 24.5, 'wind_degree': 328, 'wind_dir': 'NNW', 'pressure_mb': 1024.0, 'pressure_in': 30.24, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 47, 'cloud': 20, 'feelslike_c': 11.3, 'feelslike_f': 52.3, 'windchill_c': 11.3, 'windchill_f': 52.3, 'heatindex_c': 13.3, 'heatindex_f': 55.9, 'dewpoint_c': 2.3, 'dewpoint_f': 36.2, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 19.7, 'gust_kph': 31.8, 'uv': 0}, {'time_epoch': 1729994400, 'time': '2024-10-26 22:00', 'temp_c': 12.6, 'temp_f': 54.6, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 14.5, 'wind_kph': 23.4, 'wind_degree': 331, 'wind_dir': 'NNW', 'pressure_mb': 1025.0, 'pressure_in': 30.26, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 45, 'cloud': 0, 'feelslike_c': 10.4, 'feelslike_f': 50.8, 'windchill_c': 10.4, 'windchill_f': 50.8, 'heatindex_c': 12.6, 'heatindex_f': 54.6, 'dewpoint_c': 1.0, 'dewpoint_f': 33.7, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 18.8, 'gust_kph': 30.3, 'uv': 0}, {'time_epoch': 1729998000, 'time': '2024-10-26 23:00', 'temp_c': 12.0, 'temp_f': 53.6, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 13.0, 'wind_kph': 20.9, 'wind_degree': 333, 'wind_dir': 'NNW', 'pressure_mb': 1025.0, 'pressure_in': 30.27, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 43, 'cloud': 0, 'feelslike_c': 9.9, 'feelslike_f': 49.8, 'windchill_c': 9.9, 'windchill_f': 49.8, 'heatindex_c': 12.0, 'heatindex_f': 53.6, 'dewpoint_c': -0.2, 'dewpoint_f': 31.7, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 17.0, 'gust_kph': 27.4, 'uv': 0}]}, {'date': '2024-10-27', 'date_epoch': 1729987200, 'day': {'maxtemp_c': 13.5, 'maxtemp_f': 56.3, 'mintemp_c': 7.9, 'mintemp_f': 46.1, 'avgtemp_c': 10.9, 'avgtemp_f': 51.7, 'maxwind_mph': 12.5, 'maxwind_kph': 20.2, 'totalprecip_mm': 0.0, 'totalprecip_in': 0.0, 'totalsnow_cm': 0.0, 'avgvis_km': 10.0, 'avgvis_miles': 6.0, 'avghumidity': 37, 'daily_will_it_rain': 0, 'daily_chance_of_rain': 0, 'daily_will_it_snow': 0, 'daily_chance_of_snow': 0, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'uv': 4.0}, 'astro': {'sunrise': '07:21 AM', 'sunset': '05:58 PM', 'moonrise': '02:33 AM', 'moonset': '04:01 PM', 'moon_phase': 'Waning Crescent', 'moon_illumination': 25, 'is_moon_up': 0, 'is_sun_up': 0}, 'hour': [{'time_epoch': 1730001600, 'time': '2024-10-27 00:00', 'temp_c': 11.6, 'temp_f': 52.9, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 11.4, 'wind_kph': 18.4, 'wind_degree': 333, 'wind_dir': 'NNW', 'pressure_mb': 1025.0, 'pressure_in': 30.28, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 43, 'cloud': 0, 'feelslike_c': 9.6, 'feelslike_f': 49.2, 'windchill_c': 9.6, 'windchill_f': 49.2, 'heatindex_c': 11.6, 'heatindex_f': 52.9, 'dewpoint_c': -0.6, 'dewpoint_f': 31.0, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 15.3, 'gust_kph': 24.7, 'uv': 0}, {'time_epoch': 1730005200, 'time': '2024-10-27 01:00', 'temp_c': 11.2, 'temp_f': 52.2, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 10.5, 'wind_kph': 16.9, 'wind_degree': 327, 'wind_dir': 'NNW', 'pressure_mb': 1025.0, 'pressure_in': 30.28, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 44, 'cloud': 2, 'feelslike_c': 9.1, 'feelslike_f': 48.4, 'windchill_c': 9.1, 'windchill_f': 48.4, 'heatindex_c': 11.2, 'heatindex_f': 52.2, 'dewpoint_c': -0.7, 'dewpoint_f': 30.8, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 14.3, 'gust_kph': 23.0, 'uv': 0}, {'time_epoch': 1730008800, 'time': '2024-10-27 02:00', 'temp_c': 10.8, 'temp_f': 51.5, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 10.7, 'wind_kph': 17.3, 'wind_degree': 319, 'wind_dir': 'NW', 'pressure_mb': 1026.0, 'pressure_in': 30.29, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 47, 'cloud': 6, 'feelslike_c': 8.6, 'feelslike_f': 47.5, 'windchill_c': 8.6, 'windchill_f': 47.5, 'heatindex_c': 10.8, 'heatindex_f': 51.5, 'dewpoint_c': -0.1, 'dewpoint_f': 31.8, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 14.7, 'gust_kph': 23.6, 'uv': 0}, {'time_epoch': 1730012400, 'time': '2024-10-27 03:00', 'temp_c': 10.4, 'temp_f': 50.7, 'is_day': 0, 'condition': {'text': 'Overcast ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/122.png', 'code': 1009}, 'wind_mph': 11.2, 'wind_kph': 18.0, 'wind_degree': 321, 'wind_dir': 'NW', 'pressure_mb': 1026.0, 'pressure_in': 30.31, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 49, 'cloud': 37, 'feelslike_c': 8.1, 'feelslike_f': 46.5, 'windchill_c': 8.1, 'windchill_f': 46.5, 'heatindex_c': 10.4, 'heatindex_f': 50.7, 'dewpoint_c': 0.1, 'dewpoint_f': 32.2, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 15.0, 'gust_kph': 24.1, 'uv': 0}, {'time_epoch': 1730016000, 'time': '2024-10-27 04:00', 'temp_c': 10.0, 'temp_f': 50.0, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 11.4, 'wind_kph': 18.4, 'wind_degree': 323, 'wind_dir': 'NW', 'pressure_mb': 1027.0, 'pressure_in': 30.32, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 51, 'cloud': 68, 'feelslike_c': 7.5, 'feelslike_f': 45.5, 'windchill_c': 7.5, 'windchill_f': 45.5, 'heatindex_c': 10.0, 'heatindex_f': 50.0, 'dewpoint_c': 0.4, 'dewpoint_f': 32.7, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 15.2, 'gust_kph': 24.5, 'uv': 0}, {'time_epoch': 1730019600, 'time': '2024-10-27 05:00', 'temp_c': 9.5, 'temp_f': 49.1, 'is_day': 0, 'condition': {'text': 'Overcast ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/122.png', 'code': 1009}, 'wind_mph': 11.9, 'wind_kph': 19.1, 'wind_degree': 325, 'wind_dir': 'NW', 'pressure_mb': 1027.0, 'pressure_in': 30.34, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 53, 'cloud': 100, 'feelslike_c': 6.8, 'feelslike_f': 44.2, 'windchill_c': 6.8, 'windchill_f': 44.2, 'heatindex_c': 9.5, 'heatindex_f': 49.1, 'dewpoint_c': 0.6, 'dewpoint_f': 33.1, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 15.6, 'gust_kph': 25.1, 'uv': 0}, {'time_epoch': 1730023200, 'time': '2024-10-27 06:00', 'temp_c': 8.9, 'temp_f': 48.1, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 12.1, 'wind_kph': 19.4, 'wind_degree': 327, 'wind_dir': 'NNW', 'pressure_mb': 1028.0, 'pressure_in': 30.37, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 52, 'cloud': 69, 'feelslike_c': 6.1, 'feelslike_f': 43.0, 'windchill_c': 6.1, 'windchill_f': 43.0, 'heatindex_c': 8.9, 'heatindex_f': 48.1, 'dewpoint_c': -0.3, 'dewpoint_f': 31.5, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 15.7, 'gust_kph': 25.2, 'uv': 0}, {'time_epoch': 1730026800, 'time': '2024-10-27 07:00', 'temp_c': 8.4, 'temp_f': 47.1, 'is_day': 0, 'condition': {'text': 'Overcast ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/122.png', 'code': 1009}, 'wind_mph': 12.3, 'wind_kph': 19.8, 'wind_degree': 329, 'wind_dir': 'NNW', 'pressure_mb': 1029.0, 'pressure_in': 30.39, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 50, 'cloud': 38, 'feelslike_c': 5.4, 'feelslike_f': 41.7, 'windchill_c': 5.4, 'windchill_f': 41.7, 'heatindex_c': 8.4, 'heatindex_f': 47.1, 'dewpoint_c': -1.3, 'dewpoint_f': 29.7, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 15.9, 'gust_kph': 25.6, 'uv': 0}, {'time_epoch': 1730030400, 'time': '2024-10-27 08:00', 'temp_c': 8.5, 'temp_f': 47.3, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 12.5, 'wind_kph': 20.2, 'wind_degree': 331, 'wind_dir': 'NNW', 'pressure_mb': 1030.0, 'pressure_in': 30.41, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 49, 'cloud': 8, 'feelslike_c': 5.5, 'feelslike_f': 41.9, 'windchill_c': 5.5, 'windchill_f': 41.9, 'heatindex_c': 8.4, 'heatindex_f': 47.2, 'dewpoint_c': -2.2, 'dewpoint_f': 28.1, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 16.0, 'gust_kph': 25.7, 'uv': 3.0}, {'time_epoch': 1730034000, 'time': '2024-10-27 09:00', 'temp_c': 8.8, 'temp_f': 47.9, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 11.9, 'wind_kph': 19.1, 'wind_degree': 332, 'wind_dir': 'NNW', 'pressure_mb': 1030.0, 'pressure_in': 30.42, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 42, 'cloud': 5, 'feelslike_c': 6.0, 'feelslike_f': 42.9, 'windchill_c': 6.0, 'windchill_f': 42.9, 'heatindex_c': 8.8, 'heatindex_f': 47.9, 'dewpoint_c': -3.9, 'dewpoint_f': 25.0, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 14.8, 'gust_kph': 23.8, 'uv': 3.0}, {'time_epoch': 1730037600, 'time': '2024-10-27 10:00', 'temp_c': 9.4, 'temp_f': 48.9, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 11.2, 'wind_kph': 18.0, 'wind_degree': 334, 'wind_dir': 'NNW', 'pressure_mb': 1031.0, 'pressure_in': 30.43, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 36, 'cloud': 3, 'feelslike_c': 6.8, 'feelslike_f': 44.3, 'windchill_c': 6.8, 'windchill_f': 44.3, 'heatindex_c': 9.4, 'heatindex_f': 48.9, 'dewpoint_c': -5.6, 'dewpoint_f': 21.9, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 13.4, 'gust_kph': 21.6, 'uv': 3.0}, {'time_epoch': 1730041200, 'time': '2024-10-27 11:00', 'temp_c': 10.1, 'temp_f': 50.3, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 10.5, 'wind_kph': 16.9, 'wind_degree': 335, 'wind_dir': 'NNW', 'pressure_mb': 1031.0, 'pressure_in': 30.44, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 29, 'cloud': 0, 'feelslike_c': 7.9, 'feelslike_f': 46.2, 'windchill_c': 7.9, 'windchill_f': 46.2, 'heatindex_c': 10.1, 'heatindex_f': 50.3, 'dewpoint_c': -7.3, 'dewpoint_f': 18.9, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 12.1, 'gust_kph': 19.5, 'uv': 3.0}, {'time_epoch': 1730044800, 'time': '2024-10-27 12:00', 'temp_c': 11.0, 'temp_f': 51.8, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 10.3, 'wind_kph': 16.6, 'wind_degree': 335, 'wind_dir': 'NNW', 'pressure_mb': 1030.0, 'pressure_in': 30.42, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 26, 'cloud': 0, 'feelslike_c': 9.0, 'feelslike_f': 48.3, 'windchill_c': 9.0, 'windchill_f': 48.3, 'heatindex_c': 11.0, 'heatindex_f': 51.8, 'dewpoint_c': -8.0, 'dewpoint_f': 17.6, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 11.9, 'gust_kph': 19.1, 'uv': 4.0}, {'time_epoch': 1730048400, 'time': '2024-10-27 13:00', 'temp_c': 12.0, 'temp_f': 53.6, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 9.8, 'wind_kph': 15.8, 'wind_degree': 335, 'wind_dir': 'NNW', 'pressure_mb': 1029.0, 'pressure_in': 30.4, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 23, 'cloud': 0, 'feelslike_c': 10.3, 'feelslike_f': 50.5, 'windchill_c': 10.3, 'windchill_f': 50.5, 'heatindex_c': 12.0, 'heatindex_f': 53.6, 'dewpoint_c': -8.7, 'dewpoint_f': 16.3, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 11.4, 'gust_kph': 18.4, 'uv': 4.0}, {'time_epoch': 1730052000, 'time': '2024-10-27 14:00', 'temp_c': 12.5, 'temp_f': 54.6, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 9.6, 'wind_kph': 15.5, 'wind_degree': 334, 'wind_dir': 'NNW', 'pressure_mb': 1029.0, 'pressure_in': 30.38, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 20, 'cloud': 0, 'feelslike_c': 10.9, 'feelslike_f': 51.7, 'windchill_c': 10.9, 'windchill_f': 51.7, 'heatindex_c': 12.5, 'heatindex_f': 54.6, 'dewpoint_c': -9.4, 'dewpoint_f': 15.1, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 11.1, 'gust_kph': 17.8, 'uv': 4.0}, {'time_epoch': 1730055600, 'time': '2024-10-27 15:00', 'temp_c': 12.9, 'temp_f': 55.3, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 10.5, 'wind_kph': 16.9, 'wind_degree': 335, 'wind_dir': 'NNW', 'pressure_mb': 1029.0, 'pressure_in': 30.38, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 21, 'cloud': 0, 'feelslike_c': 11.3, 'feelslike_f': 52.4, 'windchill_c': 11.3, 'windchill_f': 52.4, 'heatindex_c': 12.9, 'heatindex_f': 55.3, 'dewpoint_c': -8.8, 'dewpoint_f': 16.2, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 12.5, 'gust_kph': 20.2, 'uv': 4.0}, {'time_epoch': 1730059200, 'time': '2024-10-27 16:00', 'temp_c': 13.2, 'temp_f': 55.8, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 11.4, 'wind_kph': 18.4, 'wind_degree': 336, 'wind_dir': 'NNW', 'pressure_mb': 1029.0, 'pressure_in': 30.37, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 21, 'cloud': 0, 'feelslike_c': 11.5, 'feelslike_f': 52.8, 'windchill_c': 11.5, 'windchill_f': 52.8, 'heatindex_c': 13.2, 'heatindex_f': 55.8, 'dewpoint_c': -8.3, 'dewpoint_f': 17.1, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 13.9, 'gust_kph': 22.3, 'uv': 4.0}, {'time_epoch': 1730062800, 'time': '2024-10-27 17:00', 'temp_c': 13.1, 'temp_f': 55.5, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 12.3, 'wind_kph': 19.8, 'wind_degree': 337, 'wind_dir': 'NNW', 'pressure_mb': 1029.0, 'pressure_in': 30.37, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 22, 'cloud': 0, 'feelslike_c': 11.4, 'feelslike_f': 52.5, 'windchill_c': 11.4, 'windchill_f': 52.5, 'heatindex_c': 13.1, 'heatindex_f': 55.5, 'dewpoint_c': -7.7, 'dewpoint_f': 18.2, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 15.3, 'gust_kph': 24.6, 'uv': 4.0}, {'time_epoch': 1730066400, 'time': '2024-10-27 18:00', 'temp_c': 12.7, 'temp_f': 54.8, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 11.0, 'wind_kph': 17.6, 'wind_degree': 343, 'wind_dir': 'NNW', 'pressure_mb': 1029.0, 'pressure_in': 30.4, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 25, 'cloud': 0, 'feelslike_c': 11.0, 'feelslike_f': 51.9, 'windchill_c': 11.0, 'windchill_f': 51.9, 'heatindex_c': 12.7, 'heatindex_f': 54.8, 'dewpoint_c': -6.9, 'dewpoint_f': 19.6, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 14.1, 'gust_kph': 22.7, 'uv': 0}, {'time_epoch': 1730070000, 'time': '2024-10-27 19:00', 'temp_c': 12.2, 'temp_f': 54.0, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 9.8, 'wind_kph': 15.8, 'wind_degree': 348, 'wind_dir': 'NNW', 'pressure_mb': 1030.0, 'pressure_in': 30.42, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 27, 'cloud': 0, 'feelslike_c': 10.6, 'feelslike_f': 51.1, 'windchill_c': 10.6, 'windchill_f': 51.1, 'heatindex_c': 12.2, 'heatindex_f': 54.0, 'dewpoint_c': -6.1, 'dewpoint_f': 21.0, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 13.0, 'gust_kph': 20.9, 'uv': 0}, {'time_epoch': 1730073600, 'time': '2024-10-27 20:00', 'temp_c': 11.7, 'temp_f': 53.1, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 8.5, 'wind_kph': 13.7, 'wind_degree': 353, 'wind_dir': 'N', 'pressure_mb': 1031.0, 'pressure_in': 30.44, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 30, 'cloud': 0, 'feelslike_c': 10.2, 'feelslike_f': 50.4, 'windchill_c': 10.2, 'windchill_f': 50.4, 'heatindex_c': 11.7, 'heatindex_f': 53.1, 'dewpoint_c': -5.3, 'dewpoint_f': 22.5, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 11.8, 'gust_kph': 19.0, 'uv': 0}, {'time_epoch': 1730077200, 'time': '2024-10-27 21:00', 'temp_c': 11.3, 'temp_f': 52.3, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 7.8, 'wind_kph': 12.6, 'wind_degree': 351, 'wind_dir': 'N', 'pressure_mb': 1031.0, 'pressure_in': 30.45, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 32, 'cloud': 1, 'feelslike_c': 9.8, 'feelslike_f': 49.6, 'windchill_c': 9.8, 'windchill_f': 49.6, 'heatindex_c': 11.3, 'heatindex_f': 52.3, 'dewpoint_c': -4.9, 'dewpoint_f': 23.2, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 11.0, 'gust_kph': 17.6, 'uv': 0}, {'time_epoch': 1730080800, 'time': '2024-10-27 22:00', 'temp_c': 10.8, 'temp_f': 51.4, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 7.2, 'wind_kph': 11.5, 'wind_degree': 349, 'wind_dir': 'N', 'pressure_mb': 1032.0, 'pressure_in': 30.47, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 34, 'cloud': 2, 'feelslike_c': 9.4, 'feelslike_f': 48.9, 'windchill_c': 9.4, 'windchill_f': 48.9, 'heatindex_c': 10.8, 'heatindex_f': 51.4, 'dewpoint_c': -4.5, 'dewpoint_f': 23.9, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 10.1, 'gust_kph': 16.2, 'uv': 0}, {'time_epoch': 1730084400, 'time': '2024-10-27 23:00', 'temp_c': 10.3, 'temp_f': 50.6, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 6.5, 'wind_kph': 10.4, 'wind_degree': 347, 'wind_dir': 'NNW', 'pressure_mb': 1032.0, 'pressure_in': 30.48, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 36, 'cloud': 3, 'feelslike_c': 9.0, 'feelslike_f': 48.2, 'windchill_c': 9.0, 'windchill_f': 48.2, 'heatindex_c': 10.3, 'heatindex_f': 50.6, 'dewpoint_c': -4.1, 'dewpoint_f': 24.5, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 9.0, 'gust_kph': 14.5, 'uv': 0}]}, {'date': '2024-10-28', 'date_epoch': 1730073600, 'day': {'maxtemp_c': 12.8, 'maxtemp_f': 55.1, 'mintemp_c': 8.1, 'mintemp_f': 46.6, 'avgtemp_c': 10.8, 'avgtemp_f': 51.5, 'maxwind_mph': 9.4, 'maxwind_kph': 15.1, 'totalprecip_mm': 0.0, 'totalprecip_in': 0.0, 'totalsnow_cm': 0.0, 'avgvis_km': 10.0, 'avgvis_miles': 6.0, 'avghumidity': 42, 'daily_will_it_rain': 0, 'daily_chance_of_rain': 0, 'daily_will_it_snow': 0, 'daily_chance_of_snow': 0, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'uv': 4.0}, 'astro': {'sunrise': '07:23 AM', 'sunset': '05:57 PM', 'moonrise': '03:34 AM', 'moonset': '04:20 PM', 'moon_phase': 'Waning Crescent', 'moon_illumination': 17, 'is_moon_up': 0, 'is_sun_up': 0}, 'hour': [{'time_epoch': 1730088000, 'time': '2024-10-28 00:00', 'temp_c': 10.0, 'temp_f': 50.0, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 6.3, 'wind_kph': 10.1, 'wind_degree': 238, 'wind_dir': 'WSW', 'pressure_mb': 1032.0, 'pressure_in': 30.48, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 37, 'cloud': 2, 'feelslike_c': 8.6, 'feelslike_f': 47.5, 'windchill_c': 8.6, 'windchill_f': 47.5, 'heatindex_c': 10.0, 'heatindex_f': 50.0, 'dewpoint_c': -4.0, 'dewpoint_f': 24.8, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 8.7, 'gust_kph': 14.0, 'uv': 0}, {'time_epoch': 1730091600, 'time': '2024-10-28 01:00', 'temp_c': 9.6, 'temp_f': 49.2, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 6.0, 'wind_kph': 9.7, 'wind_degree': 129, 'wind_dir': 'SE', 'pressure_mb': 1033.0, 'pressure_in': 30.49, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 38, 'cloud': 2, 'feelslike_c': 8.2, 'feelslike_f': 46.7, 'windchill_c': 8.2, 'windchill_f': 46.7, 'heatindex_c': 9.6, 'heatindex_f': 49.2, 'dewpoint_c': -3.9, 'dewpoint_f': 25.0, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 8.5, 'gust_kph': 13.7, 'uv': 0}, {'time_epoch': 1730095200, 'time': '2024-10-28 02:00', 'temp_c': 9.2, 'temp_f': 48.6, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 5.8, 'wind_kph': 9.4, 'wind_degree': 20, 'wind_dir': 'NNE', 'pressure_mb': 1033.0, 'pressure_in': 30.5, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 40, 'cloud': 1, 'feelslike_c': 8.0, 'feelslike_f': 46.4, 'windchill_c': 8.0, 'windchill_f': 46.4, 'heatindex_c': 9.2, 'heatindex_f': 48.6, 'dewpoint_c': -3.8, 'dewpoint_f': 25.1, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 8.1, 'gust_kph': 13.1, 'uv': 0}, {'time_epoch': 1730098800, 'time': '2024-10-28 03:00', 'temp_c': 9.0, 'temp_f': 48.1, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 5.1, 'wind_kph': 8.3, 'wind_degree': 20, 'wind_dir': 'NNE', 'pressure_mb': 1033.0, 'pressure_in': 30.5, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 40, 'cloud': 2, 'feelslike_c': 7.9, 'feelslike_f': 46.2, 'windchill_c': 7.9, 'windchill_f': 46.2, 'heatindex_c': 9.0, 'heatindex_f': 48.1, 'dewpoint_c': -3.8, 'dewpoint_f': 25.2, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 7.2, 'gust_kph': 11.5, 'uv': 0}, {'time_epoch': 1730102400, 'time': '2024-10-28 04:00', 'temp_c': 8.7, 'temp_f': 47.7, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 4.3, 'wind_kph': 6.8, 'wind_degree': 21, 'wind_dir': 'NNE', 'pressure_mb': 1033.0, 'pressure_in': 30.51, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 41, 'cloud': 2, 'feelslike_c': 7.8, 'feelslike_f': 46.1, 'windchill_c': 7.8, 'windchill_f': 46.1, 'heatindex_c': 8.7, 'heatindex_f': 47.7, 'dewpoint_c': -3.8, 'dewpoint_f': 25.2, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 6.3, 'gust_kph': 10.1, 'uv': 0}, {'time_epoch': 1730106000, 'time': '2024-10-28 05:00', 'temp_c': 8.5, 'temp_f': 47.3, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 3.6, 'wind_kph': 5.8, 'wind_degree': 21, 'wind_dir': 'NNE', 'pressure_mb': 1033.0, 'pressure_in': 30.52, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 42, 'cloud': 3, 'feelslike_c': 7.7, 'feelslike_f': 45.9, 'windchill_c': 7.7, 'windchill_f': 45.9, 'heatindex_c': 8.5, 'heatindex_f': 47.3, 'dewpoint_c': -3.8, 'dewpoint_f': 25.1, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 5.2, 'gust_kph': 8.3, 'uv': 0}, {'time_epoch': 1730109600, 'time': '2024-10-28 06:00', 'temp_c': 8.4, 'temp_f': 47.0, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 3.6, 'wind_kph': 5.8, 'wind_degree': 22, 'wind_dir': 'NNE', 'pressure_mb': 1034.0, 'pressure_in': 30.53, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 42, 'cloud': 4, 'feelslike_c': 7.6, 'feelslike_f': 45.6, 'windchill_c': 7.6, 'windchill_f': 45.6, 'heatindex_c': 8.4, 'heatindex_f': 47.0, 'dewpoint_c': -3.9, 'dewpoint_f': 25.0, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 5.1, 'gust_kph': 8.3, 'uv': 0}, {'time_epoch': 1730113200, 'time': '2024-10-28 07:00', 'temp_c': 8.2, 'temp_f': 46.8, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 3.8, 'wind_kph': 6.1, 'wind_degree': 22, 'wind_dir': 'NNE', 'pressure_mb': 1034.0, 'pressure_in': 30.55, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 42, 'cloud': 5, 'feelslike_c': 7.4, 'feelslike_f': 45.3, 'windchill_c': 7.4, 'windchill_f': 45.3, 'heatindex_c': 8.2, 'heatindex_f': 46.8, 'dewpoint_c': -4.0, 'dewpoint_f': 24.8, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 5.4, 'gust_kph': 8.6, 'uv': 0}, {'time_epoch': 1730116800, 'time': '2024-10-28 08:00', 'temp_c': 8.7, 'temp_f': 47.6, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 3.8, 'wind_kph': 6.1, 'wind_degree': 22, 'wind_dir': 'NNE', 'pressure_mb': 1035.0, 'pressure_in': 30.56, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 42, 'cloud': 6, 'feelslike_c': 8.0, 'feelslike_f': 46.3, 'windchill_c': 8.0, 'windchill_f': 46.3, 'heatindex_c': 8.7, 'heatindex_f': 47.6, 'dewpoint_c': -4.1, 'dewpoint_f': 24.6, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 5.4, 'gust_kph': 8.7, 'uv': 3.0}, {'time_epoch': 1730120400, 'time': '2024-10-28 09:00', 'temp_c': 9.4, 'temp_f': 49.0, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 3.6, 'wind_kph': 5.8, 'wind_degree': 54, 'wind_dir': 'NE', 'pressure_mb': 1035.0, 'pressure_in': 30.57, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 39, 'cloud': 5, 'feelslike_c': 8.9, 'feelslike_f': 48.0, 'windchill_c': 8.9, 'windchill_f': 48.0, 'heatindex_c': 9.4, 'heatindex_f': 49.0, 'dewpoint_c': -4.2, 'dewpoint_f': 24.4, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 4.7, 'gust_kph': 7.6, 'uv': 3.0}, {'time_epoch': 1730124000, 'time': '2024-10-28 10:00', 'temp_c': 10.3, 'temp_f': 50.6, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 3.4, 'wind_kph': 5.4, 'wind_degree': 86, 'wind_dir': 'E', 'pressure_mb': 1036.0, 'pressure_in': 30.58, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 36, 'cloud': 5, 'feelslike_c': 10.0, 'feelslike_f': 50.0, 'windchill_c': 10.0, 'windchill_f': 50.0, 'heatindex_c': 10.3, 'heatindex_f': 50.6, 'dewpoint_c': -4.2, 'dewpoint_f': 24.4, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 4.3, 'gust_kph': 6.8, 'uv': 4.0}, {'time_epoch': 1730127600, 'time': '2024-10-28 11:00', 'temp_c': 11.1, 'temp_f': 51.9, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 3.1, 'wind_kph': 5.0, 'wind_degree': 118, 'wind_dir': 'ESE', 'pressure_mb': 1036.0, 'pressure_in': 30.59, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 33, 'cloud': 4, 'feelslike_c': 10.7, 'feelslike_f': 51.3, 'windchill_c': 10.7, 'windchill_f': 51.3, 'heatindex_c': 11.1, 'heatindex_f': 51.9, 'dewpoint_c': -4.3, 'dewpoint_f': 24.2, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 3.6, 'gust_kph': 5.8, 'uv': 4.0}, {'time_epoch': 1730131200, 'time': '2024-10-28 12:00', 'temp_c': 11.7, 'temp_f': 53.0, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 4.5, 'wind_kph': 7.2, 'wind_degree': 136, 'wind_dir': 'SE', 'pressure_mb': 1035.0, 'pressure_in': 30.57, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 33, 'cloud': 3, 'feelslike_c': 11.2, 'feelslike_f': 52.1, 'windchill_c': 11.2, 'windchill_f': 52.1, 'heatindex_c': 11.7, 'heatindex_f': 53.0, 'dewpoint_c': -3.8, 'dewpoint_f': 25.2, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 5.1, 'gust_kph': 8.3, 'uv': 4.0}, {'time_epoch': 1730134800, 'time': '2024-10-28 13:00', 'temp_c': 12.3, 'temp_f': 54.1, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 5.6, 'wind_kph': 9.0, 'wind_degree': 153, 'wind_dir': 'SSE', 'pressure_mb': 1035.0, 'pressure_in': 30.55, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 33, 'cloud': 1, 'feelslike_c': 11.5, 'feelslike_f': 52.7, 'windchill_c': 11.5, 'windchill_f': 52.7, 'heatindex_c': 12.3, 'heatindex_f': 54.1, 'dewpoint_c': -3.3, 'dewpoint_f': 26.1, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 6.5, 'gust_kph': 10.4, 'uv': 4.0}, {'time_epoch': 1730138400, 'time': '2024-10-28 14:00', 'temp_c': 12.5, 'temp_f': 54.6, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 6.9, 'wind_kph': 11.2, 'wind_degree': 171, 'wind_dir': 'S', 'pressure_mb': 1034.0, 'pressure_in': 30.53, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 33, 'cloud': 0, 'feelslike_c': 11.6, 'feelslike_f': 52.9, 'windchill_c': 11.6, 'windchill_f': 52.9, 'heatindex_c': 12.5, 'heatindex_f': 54.6, 'dewpoint_c': -2.9, 'dewpoint_f': 26.9, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 8.0, 'gust_kph': 12.9, 'uv': 4.0}, {'time_epoch': 1730142000, 'time': '2024-10-28 15:00', 'temp_c': 12.7, 'temp_f': 54.8, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 7.6, 'wind_kph': 12.2, 'wind_degree': 167, 'wind_dir': 'SSE', 'pressure_mb': 1034.0, 'pressure_in': 30.52, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 36, 'cloud': 1, 'feelslike_c': 11.6, 'feelslike_f': 52.8, 'windchill_c': 11.6, 'windchill_f': 52.8, 'heatindex_c': 12.7, 'heatindex_f': 54.8, 'dewpoint_c': -1.9, 'dewpoint_f': 28.6, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 8.9, 'gust_kph': 14.4, 'uv': 4.0}, {'time_epoch': 1730145600, 'time': '2024-10-28 16:00', 'temp_c': 12.7, 'temp_f': 54.9, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 8.3, 'wind_kph': 13.3, 'wind_degree': 163, 'wind_dir': 'SSE', 'pressure_mb': 1033.0, 'pressure_in': 30.51, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 39, 'cloud': 1, 'feelslike_c': 11.5, 'feelslike_f': 52.6, 'windchill_c': 11.5, 'windchill_f': 52.6, 'heatindex_c': 12.7, 'heatindex_f': 54.9, 'dewpoint_c': -0.9, 'dewpoint_f': 30.4, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 9.8, 'gust_kph': 15.8, 'uv': 4.0}, {'time_epoch': 1730149200, 'time': '2024-10-28 17:00', 'temp_c': 12.7, 'temp_f': 54.8, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 8.9, 'wind_kph': 14.4, 'wind_degree': 160, 'wind_dir': 'SSE', 'pressure_mb': 1033.0, 'pressure_in': 30.51, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 41, 'cloud': 2, 'feelslike_c': 11.3, 'feelslike_f': 52.3, 'windchill_c': 11.3, 'windchill_f': 52.3, 'heatindex_c': 12.7, 'heatindex_f': 54.8, 'dewpoint_c': 0.0, 'dewpoint_f': 32.0, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 10.9, 'gust_kph': 17.6, 'uv': 4.0}, {'time_epoch': 1730152800, 'time': '2024-10-28 18:00', 'temp_c': 12.5, 'temp_f': 54.5, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 9.2, 'wind_kph': 14.8, 'wind_degree': 163, 'wind_dir': 'SSE', 'pressure_mb': 1033.0, 'pressure_in': 30.51, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 44, 'cloud': 1, 'feelslike_c': 11.1, 'feelslike_f': 52.0, 'windchill_c': 11.1, 'windchill_f': 52.0, 'heatindex_c': 12.5, 'heatindex_f': 54.5, 'dewpoint_c': 0.8, 'dewpoint_f': 33.4, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 11.6, 'gust_kph': 18.7, 'uv': 0}, {'time_epoch': 1730156400, 'time': '2024-10-28 19:00', 'temp_c': 12.4, 'temp_f': 54.3, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 9.2, 'wind_kph': 14.8, 'wind_degree': 166, 'wind_dir': 'SSE', 'pressure_mb': 1033.0, 'pressure_in': 30.52, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 47, 'cloud': 1, 'feelslike_c': 10.9, 'feelslike_f': 51.5, 'windchill_c': 10.9, 'windchill_f': 51.5, 'heatindex_c': 12.4, 'heatindex_f': 54.3, 'dewpoint_c': 1.5, 'dewpoint_f': 34.7, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 12.3, 'gust_kph': 19.8, 'uv': 0}, {'time_epoch': 1730160000, 'time': '2024-10-28 20:00', 'temp_c': 12.2, 'temp_f': 53.9, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 9.4, 'wind_kph': 15.1, 'wind_degree': 170, 'wind_dir': 'S', 'pressure_mb': 1034.0, 'pressure_in': 30.52, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 50, 'cloud': 0, 'feelslike_c': 10.6, 'feelslike_f': 51.1, 'windchill_c': 10.6, 'windchill_f': 51.1, 'heatindex_c': 12.2, 'heatindex_f': 53.9, 'dewpoint_c': 2.3, 'dewpoint_f': 36.1, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 13.1, 'gust_kph': 21.1, 'uv': 0}, {'time_epoch': 1730163600, 'time': '2024-10-28 21:00', 'temp_c': 12.0, 'temp_f': 53.7, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 9.4, 'wind_kph': 15.1, 'wind_degree': 180, 'wind_dir': 'S', 'pressure_mb': 1034.0, 'pressure_in': 30.52, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 52, 'cloud': 0, 'feelslike_c': 10.4, 'feelslike_f': 50.7, 'windchill_c': 10.4, 'windchill_f': 50.7, 'heatindex_c': 12.0, 'heatindex_f': 53.7, 'dewpoint_c': 2.5, 'dewpoint_f': 36.5, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 13.2, 'gust_kph': 21.2, 'uv': 0}, {'time_epoch': 1730167200, 'time': '2024-10-28 22:00', 'temp_c': 11.9, 'temp_f': 53.4, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 9.4, 'wind_kph': 15.1, 'wind_degree': 190, 'wind_dir': 'S', 'pressure_mb': 1034.0, 'pressure_in': 30.52, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 53, 'cloud': 1, 'feelslike_c': 10.2, 'feelslike_f': 50.4, 'windchill_c': 10.2, 'windchill_f': 50.4, 'heatindex_c': 11.9, 'heatindex_f': 53.4, 'dewpoint_c': 2.7, 'dewpoint_f': 36.9, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 13.4, 'gust_kph': 21.6, 'uv': 0}, {'time_epoch': 1730170800, 'time': '2024-10-28 23:00', 'temp_c': 11.6, 'temp_f': 53.0, 'is_day': 0, 'condition': {'text': 'Clear ', 'icon': '//cdn.weatherapi.com/weather/64x64/night/113.png', 'code': 1000}, 'wind_mph': 9.4, 'wind_kph': 15.1, 'wind_degree': 200, 'wind_dir': 'SSW', 'pressure_mb': 1033.0, 'pressure_in': 30.52, 'precip_mm': 0.0, 'precip_in': 0.0, 'snow_cm': 0.0, 'humidity': 54, 'cloud': 1, 'feelslike_c': 10.0, 'feelslike_f': 50.0, 'windchill_c': 10.0, 'windchill_f': 50.0, 'heatindex_c': 11.6, 'heatindex_f': 53.0, 'dewpoint_c': 2.9, 'dewpoint_f': 37.1, 'will_it_rain': 0, 'chance_of_rain': 0, 'will_it_snow': 0, 'chance_of_snow': 0, 'vis_km': 10.0, 'vis_miles': 6.0, 'gust_mph': 13.6, 'gust_kph': 21.9, 'uv': 0}]}]}}
",1,1
"I need geolocation and timezone details based on a specific IP address. This includes detailed information about the location, such as the continent, country, city, latitude, and longitude, as well as the timezone, local time, and other related data. Additionally, I need the time zone information verified and detailed through the timezone API.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_ip_geolocation(ip_address):
    url = ""https://weatherapi-com.p.rapidapi.com/ip.json""
    querystring = {""q"": ip_address}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_timezone(location):
    url = ""https://weatherapi-com.p.rapidapi.com/timezone.json""
    querystring = {""q"": location}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

if __name__ == '__main__':
    # Specify the IP address
    ip_address = ""8.8.8.8""

    # Fetch geolocation data based on IP address
    geolocation_data = get_ip_geolocation(ip_address)
    print(""Geolocation Data:"", geolocation_data)

    # Fetch timezone information based on location details from geolocation response
    location = f""{geolocation_data['lat']},{geolocation_data['lon']}""
    timezone_data = get_timezone(location)
    print(""Timezone Information:"", timezone_data)
","Geolocation Data: {'ip': '8.8.8.8', 'type': 'ipv4', 'continent_code': 'NA', 'continent_name': 'North America', 'country_code': 'US', 'country_name': 'United States', 'is_eu': 'false', 'geoname_id': 5375480, 'city': 'Mountain View', 'region': 'California', 'lat': 37.4223, 'lon': -122.085, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1729965738, 'localtime': '2024-10-26 11:02'}
Timezone Information: {'location': {'name': 'Mountain View', 'region': 'California', 'country': 'United States of America', 'lat': 37.386, 'lon': -122.083, 'tz_id': 'America/Los_Angeles', 'localtime_epoch': 1729965742, 'localtime': '2024-10-26 11:02'}}
",1,1
"I need the current weather and astronomical data for a specified location, including temperature, wind speed, humidity, sunrise, sunset, moonrise, moonset, and moon phase. Ensure the weather and astronomical details are specific to this location and include as much information as possible for both conditions in a single retrieval.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_realtime_weather(location):
    url = ""https://weatherapi-com.p.rapidapi.com/current.json""
    querystring = {""q"": location}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_astronomy_data(location, date=None):
    url = ""https://weatherapi-com.p.rapidapi.com/astronomy.json""
    querystring = {""q"": location}
    if date:
        querystring[""dt""] = date
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

if __name__ == '__main__':
    # Specify the location (e.g., ""New York"")
    location = ""New York""
    date = ""2024-10-27""  # Optional: specify a date for astronomy data if needed

    # Fetch current weather data
    current_weather = get_realtime_weather(location)
    print(""Current Weather Data:"", current_weather)

    # Fetch astronomical data for the specified date
    astronomy_data = get_astronomy_data(location, date)
    print(""Astronomy Data:"", astronomy_data)
","Current Weather Data: {'location': {'name': 'New York', 'region': 'New York', 'country': 'United States of America', 'lat': 40.7142, 'lon': -74.0064, 'tz_id': 'America/New_York', 'localtime_epoch': 1729965837, 'localtime': '2024-10-26 14:03'}, 'current': {'last_updated_epoch': 1729965600, 'last_updated': '2024-10-26 14:00', 'temp_c': 17.2, 'temp_f': 63.0, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 15.4, 'wind_kph': 24.8, 'wind_degree': 316, 'wind_dir': 'NW', 'pressure_mb': 1020.0, 'pressure_in': 30.12, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 29, 'cloud': 0, 'feelslike_c': 17.2, 'feelslike_f': 63.0, 'windchill_c': 18.3, 'windchill_f': 64.9, 'heatindex_c': 18.3, 'heatindex_f': 64.9, 'dewpoint_c': 9.7, 'dewpoint_f': 49.4, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 5.0, 'gust_mph': 18.9, 'gust_kph': 30.3}}
Astronomy Data: {'location': {'name': 'New York', 'region': 'New York', 'country': 'United States of America', 'lat': 40.7142, 'lon': -74.0064, 'tz_id': 'America/New_York', 'localtime_epoch': 1729965835, 'localtime': '2024-10-26 14:03'}, 'astronomy': {'astro': {'sunrise': '07:21 AM', 'sunset': '05:58 PM', 'moonrise': '02:33 AM', 'moonset': '04:01 PM', 'moon_phase': 'Waning Crescent', 'moon_illumination': 25, 'is_moon_up': 0, 'is_sun_up': 0}}}
",1,1
"I need the current weather, timezone, and historical weather data for a specific location. Please include current temperature, wind speed, and humidity, timezone details with local time, and historical weather data for a specific date, including daily high and low temperatures, precipitation, and significant events such as sunrise, sunset, and moon phase.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_realtime_weather(location):
    url = ""https://weatherapi-com.p.rapidapi.com/current.json""
    querystring = {""q"": location}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_timezone_info(location):
    url = ""https://weatherapi-com.p.rapidapi.com/timezone.json""
    querystring = {""q"": location}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_historical_weather(location, date):
    url = ""https://weatherapi-com.p.rapidapi.com/history.json""
    querystring = {""q"": location, ""dt"": date}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

if __name__ == '__main__':
    # Specify the location and date for historical data
    location = ""New York""
    date = ""2023-10-01""  # Example historical date

    # Fetch current weather data
    current_weather = get_realtime_weather(location)
    print(""Current Weather Data:"", current_weather)

    # Fetch timezone information
    timezone_info = get_timezone_info(location)
    print(""Timezone Information:"", timezone_info)

    # Fetch historical weather data for the specified date
    historical_weather = get_historical_weather(location, date)
    print(""Historical Weather Data:"", historical_weather)
","Current Weather Data: {'location': {'name': 'New York', 'region': 'New York', 'country': 'United States of America', 'lat': 40.7142, 'lon': -74.0064, 'tz_id': 'America/New_York', 'localtime_epoch': 1729965837, 'localtime': '2024-10-26 14:03'}, 'current': {'last_updated_epoch': 1729965600, 'last_updated': '2024-10-26 14:00', 'temp_c': 17.2, 'temp_f': 63.0, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 15.4, 'wind_kph': 24.8, 'wind_degree': 316, 'wind_dir': 'NW', 'pressure_mb': 1020.0, 'pressure_in': 30.12, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 29, 'cloud': 0, 'feelslike_c': 17.2, 'feelslike_f': 63.0, 'windchill_c': 18.3, 'windchill_f': 64.9, 'heatindex_c': 18.3, 'heatindex_f': 64.9, 'dewpoint_c': 9.7, 'dewpoint_f': 49.4, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 5.0, 'gust_mph': 18.9, 'gust_kph': 30.3}}
Timezone Information: {'location': {'name': 'New York', 'region': 'New York', 'country': 'United States of America', 'lat': 40.7142, 'lon': -74.0064, 'tz_id': 'America/New_York', 'localtime_epoch': 1729966004, 'localtime': '2024-10-26 14:06'}}
Historical Weather Data: {'error': {'code': 1008, 'message': 'API key is limited to get history data. Please check our pricing page and upgrade to higher plan.'}}
",1,1
"I need to get the forecasted weather, geolocation based on an IP address, and autocomplete suggestions for a location name. Specifically, I want the 3-day weather forecast for a specified location with hourly details, geolocation data for a given IP, and detailed autocomplete information (including geographical coordinates and unique identifiers) for a specified place.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_forecast_weather(location, days=3):
    url = ""https://weatherapi-com.p.rapidapi.com/forecast.json""
    querystring = {""q"": location, ""days"": str(days)}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_ip_geolocation(ip_address):
    url = ""https://weatherapi-com.p.rapidapi.com/ip.json""
    querystring = {""q"": ip_address}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_location_autocomplete(query):
    url = ""https://weatherapi-com.p.rapidapi.com/search.json""
    querystring = {""q"": query}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

if __name__ == '__main__':
    # Specify inputs for each function
    location = ""New York""
    ip_address = ""8.8.8.8""
    autocomplete_query = ""Los Angeles""

    # Fetch the 3-day weather forecast
    forecast_weather = get_forecast_weather(location)
    print(""3-Day Weather Forecast:"", forecast_weather)

    # Fetch geolocation data based on IP address
    ip_geolocation = get_ip_geolocation(ip_address)
    print(""Geolocation Data:"", ip_geolocation)

    # Fetch autocomplete suggestions for location
    autocomplete_data = get_location_autocomplete(autocomplete_query)
    print(""Location Autocomplete Suggestions:"", autocomplete_data)
",Fail,-1,-1
"I need detailed weather data, including both historical hourly and monthly records, for a specified weather station. Additionally, I want to retrieve location details and identifiers for a city using an autocomplete search. Please provide hourly historical data with details such as temperature, humidity, and wind, and monthly data including average, minimum, and maximum temperatures, along with precipitation and sunshine duration. For location details, retrieve the city name, region, country, and coordinates.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_location_autocomplete(query):
    url = ""https://weatherapi-com.p.rapidapi.com/search.json""
    querystring = {""q"": query}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_hourly_station_data(station_id, start_date, end_date):
    url = ""https://meteostat.p.rapidapi.com/stations/hourly""
    querystring = {""station"": station_id, ""start"": start_date, ""end"": end_date, ""tz"": ""UTC""}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""meteostat.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_monthly_station_data(station_id, start_date, end_date):
    url = ""https://meteostat.p.rapidapi.com/stations/monthly""
    querystring = {""station"": station_id, ""start"": start_date, ""end"": end_date}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""meteostat.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

if __name__ == '__main__':
    # Example inputs
    location_query = ""Los Angeles""
    station_id = ""10637""  # Example station ID
    start_date = ""2023-01-01""
    end_date = ""2023-12-31""

    # Fetch location details through autocomplete
    location_data = get_location_autocomplete(location_query)
    print(""Location Autocomplete Data:"", location_data)

    # Fetch hourly historical data for the station
    hourly_data = get_hourly_station_data(station_id, start_date, start_date)  # One day of hourly data
    print(""Hourly Station Data:"", hourly_data)

    # Fetch monthly historical data for the station
    monthly_data = get_monthly_station_data(station_id, start_date, end_date)
    print(""Monthly Station Data:"", monthly_data)
","Location Autocomplete Data: [{'id': 2548773, 'name': 'Los Angeles', 'region': 'California', 'country': 'United States of America', 'lat': 34.05, 'lon': -118.24, 'url': 'los-angeles-california-united-states-of-america'}, {'id': 371191, 'name': 'Los Angeles', 'region': 'Bio-Bio', 'country': 'Chile', 'lat': -37.47, 'lon': -72.35, 'url': 'los-angeles-bio-bio-chile'}, {'id': 1849673, 'name': 'Los Angeles', 'region': 'Butuan', 'country': 'Philippines', 'lat': 9.01, 'lon': 125.61, 'url': 'los-angeles-butuan-philippines'}, {'id': 3265054, 'name': 'Los Angeles', 'region': 'Guanajuato', 'country': 'Mexico', 'lat': 20.55, 'lon': -100.94, 'url': 'los-angeles-guanajuato-mexico'}, {'id': 3217043, 'name': 'Los Angeles', 'region': 'Oaxaca', 'country': 'Mexico', 'lat': 17.16, 'lon': -97.8, 'url': 'los-angeles-oaxaca-mexico'}]
Hourly Station Data: {'meta': {'generated': '2024-10-26 18:10:22'}, 'data': [{'time': '2023-01-01 00:00:00', 'temp': 10.9, 'dwpt': 6.8, 'rhum': 76.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 220.0, 'wspd': 10.1, 'wpgt': 17.0, 'pres': 1017.4, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 01:00:00', 'temp': 12.4, 'dwpt': 7.7, 'rhum': 73.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 230.0, 'wspd': 15.5, 'wpgt': 22.0, 'pres': 1017.2, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 02:00:00', 'temp': 11.9, 'dwpt': 7.2, 'rhum': 73.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 190.0, 'wspd': 10.8, 'wpgt': 19.0, 'pres': 1017.2, 'tsun': 0, 'coco': 1}, {'time': '2023-01-01 03:00:00', 'temp': 11.2, 'dwpt': 7.5, 'rhum': 78.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 200.0, 'wspd': 5.4, 'wpgt': 10.0, 'pres': 1017.8, 'tsun': 0, 'coco': 2}, {'time': '2023-01-01 04:00:00', 'temp': 11.7, 'dwpt': 7.6, 'rhum': 76.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 110.0, 'wspd': 4.0, 'wpgt': 9.0, 'pres': 1017.9, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 05:00:00', 'temp': 8.8, 'dwpt': 6.9, 'rhum': 88.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 200.0, 'wspd': 8.6, 'wpgt': 17.0, 'pres': 1018.1, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 06:00:00', 'temp': 11.1, 'dwpt': 7.6, 'rhum': 79.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 200.0, 'wspd': 14.0, 'wpgt': 33.0, 'pres': 1018.5, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 07:00:00', 'temp': 10.9, 'dwpt': 7.2, 'rhum': 78.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 210.0, 'wspd': 12.6, 'wpgt': 24.0, 'pres': 1019.0, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 08:00:00', 'temp': 11.5, 'dwpt': 7.2, 'rhum': 75.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 220.0, 'wspd': 15.8, 'wpgt': 29.0, 'pres': 1019.7, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 09:00:00', 'temp': 11.1, 'dwpt': 7.4, 'rhum': 78.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 220.0, 'wspd': 15.1, 'wpgt': 32.0, 'pres': 1020.0, 'tsun': 5, 'coco': 4}, {'time': '2023-01-01 10:00:00', 'temp': 13.3, 'dwpt': 7.9, 'rhum': 70.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 220.0, 'wspd': 14.8, 'wpgt': 26.0, 'pres': 1020.3, 'tsun': 1, 'coco': 4}, {'time': '2023-01-01 11:00:00', 'temp': 14.4, 'dwpt': 7.7, 'rhum': 64.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 220.0, 'wspd': 20.5, 'wpgt': 31.0, 'pres': 1020.3, 'tsun': 4, 'coco': 4}, {'time': '2023-01-01 12:00:00', 'temp': 14.8, 'dwpt': 8.3, 'rhum': 65.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 220.0, 'wspd': 22.3, 'wpgt': 35.0, 'pres': 1019.4, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 13:00:00', 'temp': 15.0, 'dwpt': 7.8, 'rhum': 62.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 220.0, 'wspd': 22.0, 'wpgt': 37.0, 'pres': 1019.5, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 14:00:00', 'temp': 14.7, 'dwpt': 7.7, 'rhum': 63.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 220.0, 'wspd': 20.2, 'wpgt': 31.0, 'pres': 1019.6, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 15:00:00', 'temp': 14.2, 'dwpt': 8.4, 'rhum': 68.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 210.0, 'wspd': 15.1, 'wpgt': 26.0, 'pres': 1019.7, 'tsun': 0, 'coco': 7}, {'time': '2023-01-01 16:00:00', 'temp': 14.3, 'dwpt': 7.8, 'rhum': 65.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 210.0, 'wspd': 11.2, 'wpgt': 23.0, 'pres': 1019.4, 'tsun': 0, 'coco': 7}, {'time': '2023-01-01 17:00:00', 'temp': 12.1, 'dwpt': 8.2, 'rhum': 77.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 190.0, 'wspd': 6.1, 'wpgt': 14.0, 'pres': 1019.4, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 18:00:00', 'temp': 11.5, 'dwpt': 8.2, 'rhum': 80.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 160.0, 'wspd': 6.8, 'wpgt': 14.0, 'pres': 1018.8, 'tsun': 0, 'coco': 8}, {'time': '2023-01-01 19:00:00', 'temp': 11.2, 'dwpt': 8.2, 'rhum': 82.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 140.0, 'wspd': 6.5, 'wpgt': 15.0, 'pres': 1018.2, 'tsun': 0, 'coco': 8}, {'time': '2023-01-01 20:00:00', 'temp': 10.7, 'dwpt': 8.1, 'rhum': 84.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 90.0, 'wspd': 2.5, 'wpgt': 11.0, 'pres': 1017.7, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 21:00:00', 'temp': 10.3, 'dwpt': 8.2, 'rhum': 87.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 310.0, 'wspd': 4.3, 'wpgt': 15.0, 'pres': 1017.4, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 22:00:00', 'temp': 10.3, 'dwpt': 7.9, 'rhum': 85.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 210.0, 'wspd': 4.7, 'wpgt': 14.0, 'pres': 1017.7, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 23:00:00', 'temp': 9.2, 'dwpt': 7.5, 'rhum': 89.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 160.0, 'wspd': 2.9, 'wpgt': 5.0, 'pres': 1017.9, 'tsun': 0, 'coco': 4}]}
Monthly Station Data: {'meta': {'generated': '2024-10-26 18:10:23'}, 'data': [{'date': '2023-01-01', 'tavg': 5.0, 'tmin': 2.5, 'tmax': 7.1, 'prcp': 62.9, 'wspd': 16.8, 'pres': 1018.7, 'tsun': 2052}, {'date': '2023-02-01', 'tavg': 4.9, 'tmin': 1.3, 'tmax': 8.4, 'prcp': 9.3, 'wspd': 13.7, 'pres': 1028.9, 'tsun': 5808}, {'date': '2023-03-01', 'tavg': 7.6, 'tmin': 3.1, 'tmax': 12.2, 'prcp': 98.1, 'wspd': 16.1, 'pres': 1012.6, 'tsun': 6792}, {'date': '2023-04-01', 'tavg': 9.6, 'tmin': 4.7, 'tmax': 14.3, 'prcp': 43.9, 'wspd': 12.9, 'pres': 1017.3, 'tsun': 9372}, {'date': '2023-05-01', 'tavg': 15.8, 'tmin': 10.2, 'tmax': 21.2, 'prcp': 53.3, 'wspd': 13.2, 'pres': 1019.9, 'tsun': 16212}, {'date': '2023-06-01', 'tavg': 21.7, 'tmin': 14.5, 'tmax': 28.2, 'prcp': 37.6, 'wspd': 11.5, 'pres': 1017.1, 'tsun': 20304}, {'date': '2023-07-01', 'tavg': 20.8, 'tmin': 14.2, 'tmax': 26.7, 'prcp': 107.5, 'wspd': 14.6, 'pres': 1014.4, 'tsun': 13914}, {'date': '2023-08-01', 'tavg': 19.7, 'tmin': 14.8, 'tmax': 25.2, 'prcp': 137.2, 'wspd': 11.8, 'pres': 1015.0, 'tsun': 11076}, {'date': '2023-09-01', 'tavg': 18.8, 'tmin': 12.7, 'tmax': 25.5, 'prcp': 20.8, 'wspd': 9.5, 'pres': 1018.5, 'tsun': 15804}, {'date': '2023-10-01', 'tavg': 12.9, 'tmin': 8.0, 'tmax': 18.0, 'prcp': 80.8, 'wspd': 11.9, 'pres': 1012.9, 'tsun': 6894}, {'date': '2023-11-01', 'tavg': 7.3, 'tmin': 4.4, 'tmax': 10.1, 'prcp': 91.1, 'wspd': 17.0, 'pres': 1009.1, 'tsun': 1908}, {'date': '2023-12-01', 'tavg': 5.4, 'tmin': 2.6, 'tmax': 7.6, 'prcp': 66.2, 'wspd': 17.2, 'pres': 1016.4, 'tsun': 1548}]}
",0,0
"I need detailed address information based on a query search, including postal codes, street names, and geographical coordinates. Additionally, I want detailed album information, covering artists, tracks, and release details for specified album IDs. Please provide address details with as many results as possible (up to a limit of 10) and detailed album information including tracks and artists.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_address_details(query, limit=10):
    url = ""https://uk-postcode.p.rapidapi.com/search""
    querystring = {""q"": query, ""limit"": str(limit)}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""uk-postcode.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_album_details(album_ids):
    url = ""https://spotify23.p.rapidapi.com/albums/""
    querystring = {""ids"": album_ids}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""spotify23.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

if __name__ == '__main__':
    # Specify inputs for each function
    address_query = ""Downing Street London""
    album_ids = ""3IBcauSj5M2A6lTeffJzdv,7Ey4PD4MYsKc5I2DolUwbH""  # Example album IDs

    # Fetch address details
    address_data = get_address_details(address_query)
    print(""Address Details:"", address_data)

    # Fetch album details
    album_data = get_album_details(album_ids)
    print(""Album Details:"", album_data)
","Address Details: {'success': True, 'results': [{'postCode': 'SW1A 2AA', 'postCodeTrimmed': 'SW1A2AA', 'streetName': 'Downing Street', 'longitude': -0.127695, 'latitude': 51.50354, 'plusCode': '9C3XGV3C+CW', 'region': 'Greater London', 'locality': 'London', 'area': 'England', 'numUPRNs': 2}, {'postCode': 'SW1A 2AB', 'postCodeTrimmed': 'SW1A2AB', 'streetName': 'Downing Street', 'longitude': -0.127874, 'latitude': 51.503399, 'plusCode': '9C3XGV3C+9V', 'region': 'Greater London', 'locality': 'London', 'area': 'England', 'numUPRNs': 1}, {'postCode': 'SW1A 2AD', 'postCodeTrimmed': 'SW1A2AD', 'streetName': 'Downing Street', 'longitude': -0.128105, 'latitude': 51.503384, 'plusCode': '9C3XGV3C+9Q', 'region': 'Greater London', 'locality': 'London', 'area': 'England', 'numUPRNs': 2}, {'postCode': 'SW1A 2AG', 'postCodeTrimmed': 'SW1A2AG', 'streetName': 'Downing Street', 'longitude': -0.126639, 'latitude': 51.503649, 'plusCode': '9C3XGV3F+F8', 'region': 'Greater London', 'locality': 'London', 'area': 'England', 'numUPRNs': 3}, {'postCode': 'SW1A 2AL', 'postCodeTrimmed': 'SW1A2AL', 'streetName': 'Downing Street', 'longitude': -0.126639, 'latitude': 51.503649, 'plusCode': '9C3XGV3F+F8', 'region': 'Greater London', 'locality': 'London', 'area': 'England', 'numUPRNs': 1}]}
Album Details: {'albums': [{'album_type': 'album', 'total_tracks': 10, 'is_playable': True, 'external_urls': {'spotify': 'https://open.spotify.com/album/3IBcauSj5M2A6lTeffJzdv'}, 'id': '3IBcauSj5M2A6lTeffJzdv', 'images': [{'url': 'https://i.scdn.co/image/ab67616d0000b27367c738a703dc979f5c3c52ef', 'height': 640, 'width': 640}, {'url': 'https://i.scdn.co/image/ab67616d00001e0267c738a703dc979f5c3c52ef', 'height': 300, 'width': 300}, {'url': 'https://i.scdn.co/image/ab67616d0000485167c738a703dc979f5c3c52ef', 'height': 64, 'width': 64}], 'name': 'Kan', 'release_date': '2021-03-19', 'release_date_precision': 'day', 'type': 'album', 'uri': 'spotify:album:3IBcauSj5M2A6lTeffJzdv', 'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/51DevdOxIJin6DB1FXJpD1'}, 'id': '51DevdOxIJin6DB1FXJpD1', 'name': 'UZI', 'type': 'artist', 'uri': 'spotify:artist:51DevdOxIJin6DB1FXJpD1'}], 'tracks': {'limit': 50, 'next': None, 'offset': 0, 'previous': None, 'total': 10, 'items': [{'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/51DevdOxIJin6DB1FXJpD1'}, 'id': '51DevdOxIJin6DB1FXJpD1', 'name': 'UZI', 'type': 'artist', 'uri': 'spotify:artist:51DevdOxIJin6DB1FXJpD1'}], 'disc_number': 1, 'duration_ms': 211016, 'explicit': True, 'external_urls': {'spotify': 'https://open.spotify.com/track/5jvhTc0g18kwYQNUJM5C4e'}, 'id': '5jvhTc0g18kwYQNUJM5C4e', 'is_playable': True, 'name': 'Makina', 'preview_url': 'https://p.scdn.co/mp3-preview/27ab6f738e8bbb6be4225f2e10886883ef760381?cid=d8a5ed958d274c2e8ee717e6a4b0971d', 'track_number': 1, 'type': 'track', 'uri': 'spotify:track:5jvhTc0g18kwYQNUJM5C4e', 'is_local': False}, {'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/51DevdOxIJin6DB1FXJpD1'}, 'id': '51DevdOxIJin6DB1FXJpD1', 'name': 'UZI', 'type': 'artist', 'uri': 'spotify:artist:51DevdOxIJin6DB1FXJpD1'}], 'disc_number': 1, 'duration_ms': 185458, 'explicit': True, 'external_urls': {'spotify': 'https://open.spotify.com/track/72t3CRd8YEFrlc3x0OVaob'}, 'id': '72t3CRd8YEFrlc3x0OVaob', 'is_playable': True, 'name': 'Umrumda DeÄŸil', 'preview_url': 'https://p.scdn.co/mp3-preview/ca2df079cdae1386e8b2899951cadf93c335018c?cid=d8a5ed958d274c2e8ee717e6a4b0971d', 'track_number': 2, 'type': 'track', 'uri': 'spotify:track:72t3CRd8YEFrlc3x0OVaob', 'is_local': False}, {'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/51DevdOxIJin6DB1FXJpD1'}, 'id': '51DevdOxIJin6DB1FXJpD1', 'name': 'UZI', 'type': 'artist', 'uri': 'spotify:artist:51DevdOxIJin6DB1FXJpD1'}, {'external_urls': {'spotify': 'https://open.spotify.com/artist/3BVPc9s4JXzM6O1InlLxED'}, 'id': '3BVPc9s4JXzM6O1InlLxED', 'name': 'Mavi', 'type': 'artist', 'uri': 'spotify:artist:3BVPc9s4JXzM6O1InlLxED'}], 'disc_number': 1, 'duration_ms': 200000, 'explicit': False, 'external_urls': {'spotify': 'https://open.spotify.com/track/5TkQatzJqKafPgHQerZ0dL'}, 'id': '5TkQatzJqKafPgHQerZ0dL', 'is_playable': True, 'name': 'Gecenin IÃ§ine Gir', 'preview_url': 'https://p.scdn.co/mp3-preview/dad0bc5179800da1fe3d5a7c8ca5d9c8563b786b?cid=d8a5ed958d274c2e8ee717e6a4b0971d', 'track_number': 3, 'type': 'track', 'uri': 'spotify:track:5TkQatzJqKafPgHQerZ0dL', 'is_local': False}, {'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/51DevdOxIJin6DB1FXJpD1'}, 'id': '51DevdOxIJin6DB1FXJpD1', 'name': 'UZI', 'type': 'artist', 'uri': 'spotify:artist:51DevdOxIJin6DB1FXJpD1'}], 'disc_number': 1, 'duration_ms': 243205, 'explicit': True, 'external_urls': {'spotify': 'https://open.spotify.com/track/6IW5ocUH5DRWagxkLTlbUS'}, 'id': '6IW5ocUH5DRWagxkLTlbUS', 'is_playable': True, 'name': 'Nedenini Sorma', 'preview_url': 'https://p.scdn.co/mp3-preview/3b60dc4dd3c57960b5e5309f069709034d5e6888?cid=d8a5ed958d274c2e8ee717e6a4b0971d', 'track_number': 4, 'type': 'track', 'uri': 'spotify:track:6IW5ocUH5DRWagxkLTlbUS', 'is_local': False}, {'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/51DevdOxIJin6DB1FXJpD1'}, 'id': '51DevdOxIJin6DB1FXJpD1', 'name': 'UZI', 'type': 'artist', 'uri': 'spotify:artist:51DevdOxIJin6DB1FXJpD1'}, {'external_urls': {'spotify': 'https://open.spotify.com/artist/7GaMopkesD4KK9dNbgyO5D'}, 'id': '7GaMopkesD4KK9dNbgyO5D', 'name': 'Eko Fresh', 'type': 'artist', 'uri': 'spotify:artist:7GaMopkesD4KK9dNbgyO5D'}], 'disc_number': 1, 'duration_ms': 152301, 'explicit': True, 'external_urls': {'spotify': 'https://open.spotify.com/track/1ijjjMFlM3Pe8t3ykXBzxk'}, 'id': '1ijjjMFlM3Pe8t3ykXBzxk', 'is_playable': True, 'name': 'Mahalle', 'preview_url': 'https://p.scdn.co/mp3-preview/355ebff1c469cda6bc280f59cf4546c4a8c0e13a?cid=d8a5ed958d274c2e8ee717e6a4b0971d', 'track_number': 5, 'type': 'track', 'uri': 'spotify:track:1ijjjMFlM3Pe8t3ykXBzxk', 'is_local': False}, {'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/51DevdOxIJin6DB1FXJpD1'}, 'id': '51DevdOxIJin6DB1FXJpD1', 'name': 'UZI', 'type': 'artist', 'uri': 'spotify:artist:51DevdOxIJin6DB1FXJpD1'}], 'disc_number': 1, 'duration_ms': 171880, 'explicit': True, 'external_urls': {'spotify': 'https://open.spotify.com/track/4a1WLOoydq7u011UG9jjC9'}, 'id': '4a1WLOoydq7u011UG9jjC9', 'is_playable': True, 'name': 'Krvn', 'preview_url': 'https://p.scdn.co/mp3-preview/c595da41713f76fe3935b4314acda5b4b54a82c2?cid=d8a5ed958d274c2e8ee717e6a4b0971d', 'track_number': 6, 'type': 'track', 'uri': 'spotify:track:4a1WLOoydq7u011UG9jjC9', 'is_local': False}, {'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/51DevdOxIJin6DB1FXJpD1'}, 'id': '51DevdOxIJin6DB1FXJpD1', 'name': 'UZI', 'type': 'artist', 'uri': 'spotify:artist:51DevdOxIJin6DB1FXJpD1'}], 'disc_number': 1, 'duration_ms': 155010, 'explicit': True, 'external_urls': {'spotify': 'https://open.spotify.com/track/4hy4nY2PiYWx8qVXjpky3P'}, 'id': '4hy4nY2PiYWx8qVXjpky3P', 'is_playable': True, 'name': 'Vur', 'preview_url': 'https://p.scdn.co/mp3-preview/5b57e9e80e8dc443ad508fa794686e731a3e92ef?cid=d8a5ed958d274c2e8ee717e6a4b0971d', 'track_number': 7, 'type': 'track', 'uri': 'spotify:track:4hy4nY2PiYWx8qVXjpky3P', 'is_local': False}, {'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/51DevdOxIJin6DB1FXJpD1'}, 'id': '51DevdOxIJin6DB1FXJpD1', 'name': 'UZI', 'type': 'artist', 'uri': 'spotify:artist:51DevdOxIJin6DB1FXJpD1'}, {'external_urls': {'spotify': 'https://open.spotify.com/artist/6dOAGo4z0syiCjbnlh4VSO'}, 'id': '6dOAGo4z0syiCjbnlh4VSO', 'name': 'Critical', 'type': 'artist', 'uri': 'spotify:artist:6dOAGo4z0syiCjbnlh4VSO'}], 'disc_number': 1, 'duration_ms': 223608, 'explicit': True, 'external_urls': {'spotify': 'https://open.spotify.com/track/4PpYi6USHlY7OhOcDASnD3'}, 'id': '4PpYi6USHlY7OhOcDASnD3', 'is_playable': True, 'name': 'Davetiye', 'preview_url': 'https://p.scdn.co/mp3-preview/f162cdd28694f1289d689294a35937d51255e333?cid=d8a5ed958d274c2e8ee717e6a4b0971d', 'track_number': 8, 'type': 'track', 'uri': 'spotify:track:4PpYi6USHlY7OhOcDASnD3', 'is_local': False}, {'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/51DevdOxIJin6DB1FXJpD1'}, 'id': '51DevdOxIJin6DB1FXJpD1', 'name': 'UZI', 'type': 'artist', 'uri': 'spotify:artist:51DevdOxIJin6DB1FXJpD1'}, {'external_urls': {'spotify': 'https://open.spotify.com/artist/3R27mVPp04i87RNmvysZfY'}, 'id': '3R27mVPp04i87RNmvysZfY', 'name': 'Stap', 'type': 'artist', 'uri': 'spotify:artist:3R27mVPp04i87RNmvysZfY'}], 'disc_number': 1, 'duration_ms': 196682, 'explicit': True, 'external_urls': {'spotify': 'https://open.spotify.com/track/5gx3dMxQGJ1JDw5qHarRqp'}, 'id': '5gx3dMxQGJ1JDw5qHarRqp', 'is_playable': True, 'name': 'Elhamdulillah', 'preview_url': 'https://p.scdn.co/mp3-preview/4b926e5459e68e85007155518bd13dc42eff3b46?cid=d8a5ed958d274c2e8ee717e6a4b0971d', 'track_number': 9, 'type': 'track', 'uri': 'spotify:track:5gx3dMxQGJ1JDw5qHarRqp', 'is_local': False}, {'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/51DevdOxIJin6DB1FXJpD1'}, 'id': '51DevdOxIJin6DB1FXJpD1', 'name': 'UZI', 'type': 'artist', 'uri': 'spotify:artist:51DevdOxIJin6DB1FXJpD1'}], 'disc_number': 1, 'duration_ms': 192694, 'explicit': False, 'external_urls': {'spotify': 'https://open.spotify.com/track/4PUniKS3Cywu23xjdtoji5'}, 'id': '4PUniKS3Cywu23xjdtoji5', 'is_playable': True, 'name': 'Outro', 'preview_url': 'https://p.scdn.co/mp3-preview/daa66ec952bed822128228eb026861b970dc4b8f?cid=d8a5ed958d274c2e8ee717e6a4b0971d', 'track_number': 10, 'type': 'track', 'uri': 'spotify:track:4PUniKS3Cywu23xjdtoji5', 'is_local': False}]}, 'copyrights': [{'text': '2021 M.O.B Entertainment Associated Label Of Govinet', 'type': 'C'}, {'text': '2021 M.O.B Entertainment Associated Label Of Govinet', 'type': 'P'}], 'external_ids': {'upc': '3616553578384'}, 'genres': [], 'label': 'M.O.B. Entertainment', 'popularity': 69}, None]}
",0,1
"I need an overview of available content categories, including Music, Podcasts, and Audiobooks, each represented with a title, artwork, and URI details. Additionally, I want to retrieve a specific album's tracklist with metadata such as track names, play counts, and duration for up to 300 tracks, starting from the first track.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_explore_content():
    url = ""https://spotify23.p.rapidapi.com/browse_all/""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""spotify23.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    response.raise_for_status()  # Raises an error for HTTP issues
    return response.json()

def get_album_tracks(album_id, offset=0, limit=300):
    url = ""https://spotify23.p.rapidapi.com/album_tracks/""
    querystring = {""id"": album_id, ""offset"": str(offset), ""limit"": str(limit)}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""spotify23.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    response.raise_for_status()  # Raises an error for HTTP issues
    return response.json()

if __name__ == '__main__':
    # Fetch explore content details
    explore_content_data = get_explore_content()

    # Extract a limited number of Explore Categories
    explore_data = [
        {
            ""title"": item.get('content', {}).get('data', {}).get('data', {}).get('cardRepresentation', {}).get('title', {}).get('transformedLabel', ""No Title""),
            ""artwork"": item.get('content', {}).get('data', {}).get('data', {}).get('cardRepresentation', {}).get('artwork', {}).get('sources', [{}])[0].get('url', ""No Artwork""),
            ""uri"": item.get('uri', ""No URI"")
        }
        for item in explore_content_data.get('data', {}).get('browseStart', {}).get('sections', {}).get('items', [])[0].get('sectionItems', {}).get('items', [])
    ][:3]  # Limiting to first 3 categories for display

    print(""Explore Categories:"")
    for category in explore_data:
        print(f""Title: {category['title']}"")
        print(f""Artwork URL: {category['artwork']}"")
        print(f""URI: {category['uri']}\n"")

    # Example Album ID to fetch tracks (replace 'your_album_id' with an actual album ID)
    album_tracks_data = get_album_tracks(""your_album_id"")

    # Improved check for structure of album_tracks_data
    album_data = album_tracks_data.get('data', {}).get('album', {})
    if album_data and 'tracks' in album_data and 'items' in album_data['tracks']:
        # Extract a limited number of Album Track Details
        album_tracks = [
            {
                ""track_name"": track.get('track', {}).get('name', ""No Track Name""),
                ""play_count"": track.get('track', {}).get('playcount', ""No Play Count""),
                ""duration_ms"": track.get('track', {}).get('duration', {}).get('totalMilliseconds', ""No Duration""),
                ""artists"": [artist.get('profile', {}).get('name', ""Unknown Artist"") for artist in track.get('track', {}).get('artists', {}).get('items', [])]
            }
            for track in album_data['tracks'].get('items', [])
        ][:3]  # Limiting to first 3 tracks for display

        print(""\nAlbum Tracks:"")
        for track in album_tracks:
            print(f""Track Name: {track['track_name']}"")
            print(f""Play Count: {track['play_count']}"")
            print(f""Duration (ms): {track['duration_ms']}"")
            print(f""Artists: {', '.join(track['artists'])}\n"")
    else:
        print(""Error: Album track data is missing or not in the expected format."")
","Explore Categories:
Title: Music
Artwork URL: https://i.scdn.co/image/ab67fb8200005caf474a477debc822a3a45c5acb
URI: spotify:page:0JQ5DAqbMKFSi39LMRT0Cy

Title: Podcasts
Artwork URL: https://i.scdn.co/image/ab6765630000ba8a81f07e1ead0317ee3c285bfa
URI: spotify:page:0JQ5DArNBzkmxXHCqFLx2J

Title: Audiobooks
Artwork URL: https://i.scdn.co/image/ab67fb8200005cafa6152f62518b4c3251858b21
URI: spotify:page:0JQ5DAqbMKFETqK4t8f1n3

Error: Album track data is missing or not in the expected format.
",1,1
Result,,Avg. Pass rate: 0.775,Avg. success Rate: 0.719,Kappa coefficient: 0.82