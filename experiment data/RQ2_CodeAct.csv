requirement,runnable_code,result,success_rate_label1,success_rate_label2,LLM Calls,Token Cost
"I'd like to know the latest news, user reviews, and global reach rates for the first three games so I can gauge their popularity and community activity.","import requests

# Constants
API_KEY = ""e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa""
API_HOST = ""steam2.p.rapidapi.com""
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": API_HOST
}

# Function to get news
def get_app_news(app_id, limit=10, content_length=300):
    url = f""https://{API_HOST}/newsForApp/{app_id}/limit/{limit}/{content_length}""
    response = requests.get(url, headers=HEADERS)
    return response.json() if response.status_code == 200 else {""error"": f""Failed to retrieve news for app ID {app_id}""}

# Function to get reviews
def get_app_reviews(app_id, limit=40, cursor=""*""):
    url = f""https://{API_HOST}/appReviews/{app_id}/limit/{limit}/{cursor}""
    response = requests.get(url, headers=HEADERS)
    return response.json() if response.status_code == 200 else {""error"": f""Failed to retrieve reviews for app ID {app_id}""}

# Function to get global achievement percentages
def get_global_achievement_percentages(app_id):
    url = f""https://{API_HOST}/globalAchievementPercentagesForApp/{app_id}""
    response = requests.get(url, headers=HEADERS)
    return response.json() if response.status_code == 200 else {""error"": f""Failed to retrieve achievements for app ID {app_id}""}

# Function to fetch data for multiple apps
def fetch_game_data(app_ids):
    for app_id in app_ids:
        print(f""\n{'=' * 20} App ID: {app_id} {'=' * 20}\n"")

        # Fetch and display news data
        news_data = get_app_news(app_id)
        news_items = news_data.get('appnews', {}).get('newsitems', [])
        if news_items:
            print(""Latest News:"")
            for news in news_items[:3]:  # Show top 3 news items for brevity
                print(f"" - {news.get('title')}: {news.get('contents')[:200]}..."")
        else:
            print(""No news available or failed to fetch news.\n"")

        # Fetch and display review data
        reviews_data = get_app_reviews(app_id)
        reviews = reviews_data.get('reviews', [])
        if reviews:
            print(""\nUser Reviews:"")
            for review in reviews[:3]:  # Show top 3 reviews for brevity
                print(f"" - Review: {review.get('review')[:200]}... | Rating: {review.get('voted_up')}"")
        else:
            print(""No reviews available or failed to fetch reviews.\n"")

        # Fetch and display achievement data
        achievements_data = get_global_achievement_percentages(app_id)
        achievements = achievements_data.get('achievementpercentages', {}).get('achievements', [])
        if achievements:
            print(""\nGlobal Achievement Percentages:"")
            for achievement in achievements[:3]:  # Show top 3 achievements for brevity
                print(f"" - {achievement.get('name')}: {achievement.get('percent')}%"")
        else:
            print(""No achievements data available or failed to fetch achievements.\n"")

if __name__ == '__main__':
    # List of app IDs for the top three games (example: CS:GO, Team Fortress 2, Dota 2)
    app_ids = [730, 440, 570]
    fetch_game_data(app_ids)
","==================== App ID: 730 ====================

Latest News:
 - Release Notes for 10/24/2024: Fixed a bug where grenades were generating headshot sounds. Fixed a bug where grenades were not bouncing off certain dynamic props. Fixed a bug where Copenhagen 2024 winners were not showing up in the...
 - Release Notes for 10/23/2024: Vertigo Fixed a bad collision interaction between grenades and a prop. Fixed a wall bang exploit. Mills Updated to the latest version from the Community Workshop (Update Notes). Added localization str...
 - Release Notes for 10/15/2024: Fixed mixed resolution rendering from erroneously resetting during map changes. Fixed a known crash when compiling lighting in Counter-Strike 2 Workshop Tools. Fixed detail normal UV scaling. Fixed se...

User Reviews:
 - Review: A year has passed. Armory update is a cash grab with no real gameplay value. VAC nowhere to be found, no new maps, the CSGO community has been wiped off the face of the earth. Disgusting company... | Rating: False
 - Review: Counter Strike was once a great game, still good, but too bad its full of cheaters and scammers.

Anti cheat system is a joke. This game would be amazing, if it was free of cheaters and scammers.... | Rating: True
 - Review: 1 Year since release.  Hardly any anti cheat improvements, removed gamemodes, removed live games, removed OverWatch system, no seasons as promised, and now we get the barebones BattlePass with no miss... | Rating: False

Global Achievement Percentages:
 - PLAY_CS2: 26.899999618530273%

==================== App ID: 440 ====================

Latest News:
 - Sacred Scouts 6v6 Cup: {STEAM_CLAN_IMAGE}/554111/cc86e5120ef0bf75bd90c4074018fc55dc7144f3.png Sacred Scouts is back for its second season of 6v6 Scout-only chaos! Sign-ups will open on October 31st and close on November 21s...
 - Sacred Scouts 6v6 Cup: <a href=""https://www.cltf2.com/"" target=""_blank""> </a> (Image credit: <a href=""https://steamcommunity.com/id/metalcloud/"" target=""_blank"">MetalCloud</a>) Sacred Scouts is back for its second season of...
 - Team Fortress 2 Update Released: An update to Team Fortress 2 has been released. The update will be applied automatically when you restart Team Fortress 2. The major changes include: Reverted BLU Scout pants fix because it conflicts ...

User Reviews:
 - Review: Team Fortress 2 is one of my favorite games and hat simulators of all time. It's not really much of a secret, given how many hours I've gotten in it plus the fact I've been playing it for over 10 year... | Rating: True
 - Review: ---{ Graphics }---
ðŸ”² Awful
ðŸ”² Bad
ðŸ”² Will do
âœ… Decent (Nothing special)
ðŸ”² Stunning
ðŸ”² Visual Masterpiece

---{ Gameplay }---
ðŸ”² Bad
ðŸ”² Okay
âœ… Nothing special
ðŸ”² Good
ðŸ”² Very good
ðŸ”² Try not to get addicted

-... | Rating: True
 - Review: They call this ""Team"" Fortress 2, but don't let this deceive you. There is no team. You're teammates are to busy playing with each others dicks to actual play the fucking video game while you get infi... | Rating: True

Global Achievement Percentages:
 - TF_SCOUT_LONG_DISTANCE_RUNNER: 51.099998474121094%
 - TF_HEAVY_DAMAGE_TAKEN: 41.400001525878906%
 - TF_GET_CONSECUTIVEKILLS_NODEATHS: 37.70000076293945%

==================== App ID: 570 ====================

Latest News:
 - 7.37d Gameplay Patch: Patch 7.37d is out now. You can check out the patchnotes here....
 - The International Grand Champions: {STEAM_CLAN_IMAGE}/3703047/5d614dc7454bfde1701834ffc951eb194a3a55af.jpgCongratulations to Team Liquid, Grand Champions at The International 2024. Dropping only three games throughout their championshi...
 - The International is Here: {STEAM_CLAN_IMAGE}/3703047/19deca390e7d27a5d4f6237fea74d1034cd0a29c.jpg For five straight days, sixteen teams have slugged their way through the Playoffs, and eight teams have earned a coveted spot at...
No reviews available or failed to fetch reviews.
",1,1,10,31800
I want to filter out free games for a specific platform based on multiple tags and sort by release date in order to find the three most recently released games.,"import requests

# Constants
API_KEY = ""e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa""
API_HOST = ""free-to-play-games-database.p.rapidapi.com""
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": API_HOST
}

# Function to filter games by platform and tags, sorted by release date
def get_filtered_sorted_games(platform, tags, top_n=3):
    """"""
    Retrieve games filtered by platform and tags, and sort by release date.
    Only the top_n most recent games are returned.
    """"""
    url = f""https://{API_HOST}/api/games""
    querystring = {""platform"": platform, ""sort-by"": ""release-date""}

    response = requests.get(url, headers=HEADERS, params=querystring)

    # Error handling for API response
    if response.status_code != 200:
        print(f""Error fetching games: {response.status_code}"")
        print(""Response:"", response.json())
        return []

    # Check API response data structure
    games = response.json()
    print(""Total games retrieved:"", len(games))  # Debug: Check the number of games returned

    # Filter games by tags (case insensitive)
    filtered_games = [
        game for game in games
        if all(tag.lower() in game.get('genre', '').lower() for tag in tags)
    ]

    # Check filtered results
    if not filtered_games:
        print(f""No games found for platform '{platform}' with tags {tags}. Please check tags or platform."")
        return []

    # Sort and retrieve the top_n most recent games
    sorted_games = sorted(filtered_games, key=lambda x: x.get(""release_date"", """"), reverse=True)[:top_n]

    return sorted_games

# Main execution
if __name__ == '__main__':
    # Define the platform and tags to filter
    platform = ""pc""  # Example platform
    tags = [""3d"", ""fantasy""]  # Example tags; adjust based on requirements

    # Retrieve and display the top 3 most recent games
    recent_games = get_filtered_sorted_games(platform, tags, top_n=3)

    if recent_games:
        print(f""\nTop {len(recent_games)} recent games for platform '{platform}' with tags {tags}:"")
        for game in recent_games:
            print(f"" - {game.get('title')}: Released on {game.get('release_date')}"")
    else:
        print(""No games found that match the criteria."")
","Top 0 recent games for platform 'pc' with tags ['3d', 'fantasy']:",1,1,11,30470
"I would like to learn about the first to third evolutionary paths and conditions of Pokemon, including the resources required to strengthen and the number of steps taken. With these three APIs, you have full access to all the information of Pokemon from evolution to enhancement.","import requests

# Constants
API_KEY = ""e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa""
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""pokemon-go1.p.rapidapi.com""
}


# Function to fetch Pokemon evolution data
def pokemon_evolutions():
    url = ""https://pokemon-go1.p.rapidapi.com/pokemon_evolutions.json""
    response = requests.get(url, headers=HEADERS)
    return response.json() if response.status_code == 200 else None


# Function to fetch Pokemon buddy distances
def pokemon_buddy_distances():
    url = ""https://pokemon-go1.p.rapidapi.com/pokemon_buddy_distances.json""
    response = requests.get(url, headers=HEADERS)
    return response.json() if response.status_code == 200 else None


# Function to fetch Pokemon power-up requirements
def pokemon_power_up_requirements():
    url = ""https://pokemon-go1.p.rapidapi.com/pokemon_powerup_requirements.json""
    response = requests.get(url, headers=HEADERS)
    return response.json() if response.status_code == 200 else None


# Main function to display evolution, buddy distance, and power-up requirements
def display_pokemon_evolution_and_requirements(pokemon_name):
    # Fetch all data from APIs
    evolutions_data = pokemon_evolutions()
    buddy_distances_data = pokemon_buddy_distances()
    power_up_data = pokemon_power_up_requirements()

    if not evolutions_data or not buddy_distances_data or not power_up_data:
        print(""Error fetching data from one or more APIs."")
        return

    # Step 1: Find evolution path
    print(f""\nEvolution Path for {pokemon_name.capitalize()}:"")
    for pokemon in evolutions_data:
        if pokemon['pokemon_name'].lower() == pokemon_name.lower():
            evolutions = pokemon['evolutions'][:3]  # Limit to first 3 evolutions
            base_name = pokemon['pokemon_name']
            print(f""{base_name} (Base Form)"")
            for i, evo in enumerate(evolutions, 1):
                evo_name = evo['pokemon_name']
                candies_required = evo.get('candy_required', 'N/A')
                print(f""  -> Stage {i}: {evo_name} (Requires {candies_required} Candies)"")
            break
    else:
        print(""No evolution data found for this Pokemon."")
        return

    # Step 2: Find buddy distance for each form, limiting to the first 3
    print(""\nBuddy Distance Requirements:"")
    count = 0
    for buddy_id, buddy_info_list in buddy_distances_data.items():
        for buddy_info in buddy_info_list:
            if buddy_info['pokemon_name'].lower() == pokemon_name.lower():
                distance = buddy_info.get('distance', 'N/A')
                form = buddy_info.get('form', 'N/A')
                print(f""{buddy_info['pokemon_name']} (Form: {form}) - Distance: {distance} km"")
                count += 1
                if count >= 3:  # Limit output to the first 3 entries
                    break
        if count >= 3:
            break

    # Step 3: Display power-up requirements with data structure check
    print(""\nPower-Up Requirements:"")
    if isinstance(power_up_data, list):
        for requirement in power_up_data:
            if isinstance(requirement, dict) and requirement.get('pokemon_name', '').lower() == pokemon_name.lower():
                current_level = requirement.get('current_level')
                level_after = requirement.get('level_after_powering')
                candy = requirement.get('candy_to_upgrade', 'N/A')
                stardust = requirement.get('stardust_to_upgrade', 'N/A')
                xl_candy = requirement.get('xl_candy_to_upgrade', 'N/A')
                print(
                    f""Level {current_level} -> {level_after}: Candy = {candy}, Stardust = {stardust}, XL Candy = {xl_candy}"")
    else:
        print(""Power-up data is not in the expected format."")


# Main execution
if __name__ == '__main__':
    # Example Pokemon name
    pokemon_name = ""Pikachu""

    # Display evolution path, buddy distance, and power-up requirements
    display_pokemon_evolution_and_requirements(pokemon_name)
","Evolution Path for Pikachu:
Pikachu (Base Form)
  -> Stage 1: Raichu (Requires 50 Candies)

Buddy Distance Requirements:
Pikachu (Form: Adventure_hat_2020) - Distance: 1 km
Pikachu (Form: Copy_2019) - Distance: 1 km
Pikachu (Form: Costume_2020) - Distance: 1 km

Power-Up Requirements:
Power-up data is not in the expected format.",1,1,8,27360
I'd like to filter out three games by platform and specific tags and generate a list of games that match the criteria.,"import requests

# Constants for API
API_KEY = ""e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa""
API_HOST = ""free-to-play-games-database.p.rapidapi.com""
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": API_HOST
}


# Function to filter games by a single tag and platform
def filter_games_by_single_tag_and_platform(tag, platform=None, limit=3):
    """"""
    Retrieve a list of games filtered by a single tag and platform,
    limiting to a specified number of results.
    """"""
    url = f""https://{API_HOST}/api/filter""
    querystring = {""tag"": tag}
    if platform:
        querystring[""platform""] = platform

    response = requests.get(url, headers=HEADERS, params=querystring)

    # Check if request was successful
    if response.status_code != 200:
        print(f""Error fetching games with tag '{tag}' and platform '{platform}': {response.status_code}"")
        print(""Response Content:"", response.text)  # Debugging output for more details
        return []

    # Return only the top 'limit' number of games
    games = response.json()
    return games[:limit]  # Limit the number of results


# Test different single tags
if __name__ == '__main__':
    print(""\nTesting single-tag search with different tags and platform...\n"")
    test_tags = [""3d"", ""mmorpg"", ""fantasy"", ""pvp""]  # Example single tags
    platform = ""pc""  # Example platform

    for tag in test_tags:
        print(f""\nTesting with tag '{tag}' on platform '{platform}':"")
        games = filter_games_by_single_tag_and_platform(tag, platform)

        # Display results if any games are found
        if games:
            print(f""Top {len(games)} games with tag '{tag}':"")
            for i, game in enumerate(games, 1):
                print(f""{i}. {game.get('title')} - Genre: {game.get('genre')} - Platform: {game.get('platform')}"")
        else:
            print(f""No games found with tag '{tag}' on platform '{platform}'."")
","Testing single-tag search with different tags and platform...


Testing with tag '3d' on platform 'pc':
Top 3 games with tag '3d':
1. Dauntless - Genre: MMORPG - Platform: PC (Windows)
2. World of Tanks - Genre: Shooter - Platform: PC (Windows)
3. Warframe - Genre: Shooter - Platform: PC (Windows)

Testing with tag 'mmorpg' on platform 'pc':
Top 3 games with tag 'mmorpg':
1. Dauntless - Genre: MMORPG - Platform: PC (Windows)
2. Blade and Soul - Genre: MMORPG - Platform: PC (Windows)
3. Trove - Genre: MMORPG - Platform: PC (Windows)

Testing with tag 'fantasy' on platform 'pc':
Top 3 games with tag 'fantasy':
1. Dauntless - Genre: MMORPG - Platform: PC (Windows)
2. Blade and Soul - Genre: MMORPG - Platform: PC (Windows)
3. Trove - Genre: MMORPG - Platform: PC (Windows)

Testing with tag 'pvp' on platform 'pc':
Top 3 games with tag 'pvp':
1. World of Tanks - Genre: Shooter - Platform: PC (Windows)
2. Warframe - Genre: Shooter - Platform: PC (Windows)
3. CRSED: F.O.A.D. - Genre: Shooter - Platform: PC (Windows)",0,0,20,66600
Three chess puzzles are presented for users to solve and learn.,"import requests

# API Constants
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""chess-puzzles.p.rapidapi.com""
}

# Function to fetch chess puzzles based on themes and difficulty
def fetch_chess_puzzles(themes=""middlegame"", rating=1500, playerMoves=4, count=3):
    """"""
    Fetch a specified number of chess puzzles based on theme, rating, and moves.
    """"""
    url = ""https://chess-puzzles.p.rapidapi.com/""
    querystring = {
        ""themes"": f'[""{themes}""]',
        ""rating"": rating,
        ""playerMoves"": playerMoves,
        ""count"": count
    }
    response = requests.get(url, headers=HEADERS, params=querystring)

    # Check if request was successful
    if response.status_code != 200:
        print(f""Error fetching puzzles: {response.status_code}"")
        return []

    return response.json().get(""puzzles"", [])

# Function to display fetched chess puzzles
def display_chess_puzzles():
    """"""
    Display a list of chess puzzles with details for learning and practice.
    """"""
    puzzles = fetch_chess_puzzles(themes=""middlegame"", rating=1500, playerMoves=4, count=3)

    if not puzzles:
        print(""No puzzles found. Please check your request or API parameters."")
        return

    print(""\nChess Puzzles for Learning:"")
    for i, puzzle in enumerate(puzzles, 1):
        puzzle_id = puzzle.get(""puzzleid"")
        theme = puzzle.get(""themes"", [])
        rating = puzzle.get(""rating"", ""N/A"")
        moves = puzzle.get(""moves"", [])

        print(f""\nPuzzle {i}:"")
        print(f""  - ID: {puzzle_id}"")
        print(f""  - Theme: {', '.join(theme)}"")
        print(f""  - Rating: {rating}"")
        print(f""  - Moves: {moves}"")
        print(""  - Try solving the puzzle on your chess board or an online chess platform!"")

# Main execution
if __name__ == '__main__':
    # Display selected chess puzzles
    display_chess_puzzles()
",Fail,-1,-1,31,105152
"I want to get health assessment data, including BMI (body mass index) based on height and weight, BMR (basal metabolic rate) based on information such as age, weight, and height, combined with activity level to get a total daily calorie burn (TDEE) for my diet and fitness program.","import requests

# API Constants
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""gym-fit.p.rapidapi.com""
}

# Function to calculate BMI based on weight and height
def get_bmi(weight, height):
    url = ""https://gym-fit.p.rapidapi.com/v1/calculator/bmi""
    querystring = {""weight"": str(weight), ""height"": str(height)}
    response = requests.get(url, headers=HEADERS, params=querystring)

    # Check for successful response
    if response.status_code == 200:
        bmi_result = response.json().get('bmi')
        return bmi_result
    else:
        print(""Error fetching BMI:"", response.status_code)
        return None

# Function to calculate BMR based on weight, height, gender, and age
def get_bmr(weight, height, gender, age):
    url = ""https://gym-fit.p.rapidapi.com/v1/calculator/bmr""
    querystring = {""weight"": str(weight), ""height"": str(height), ""gender"": gender, ""age"": str(age)}
    response = requests.get(url, headers=HEADERS, params=querystring)

    # Check for successful response
    if response.status_code == 200:
        bmr_result = response.json().get('bmr')
        return bmr_result
    else:
        print(""Error fetching BMR:"", response.status_code)
        return None

# Function to calculate TDEE based on BMR and activity level
def get_tdee(bmr, activity_level):
    activity_multiplier = {
        ""sedentary"": 1.2,
        ""lightly_active"": 1.375,
        ""moderately_active"": 1.55,
        ""active"": 1.725,
        ""very_active"": 1.9
    }
    multiplier = activity_multiplier.get(activity_level.lower())

    if bmr and multiplier:
        tdee = bmr * multiplier
        return round(tdee, 2)
    else:
        print(""Invalid BMR or activity level."")
        return None

# Main function to display health assessment results
def display_health_assessment(weight, height, gender, age, activity_level):
    print(f""Health Assessment for {gender.capitalize()}, Age {age}, Weight {weight}kg, Height {height}cm"")

    # Step 1: Calculate BMI
    bmi = get_bmi(weight, height)
    if bmi:
        print(f""\nBMI: {bmi} - {'Underweight' if bmi < 18.5 else 'Normal' if 18.5 <= bmi < 24.9 else 'Overweight' if 25 <= bmi < 29.9 else 'Obese'}"")

    # Step 2: Calculate BMR
    bmr = get_bmr(weight, height, gender, age)
    if bmr:
        print(f""BMR (Basal Metabolic Rate): {bmr} calories/day"")

    # Step 3: Calculate TDEE based on BMR and activity level
    tdee = get_tdee(bmr, activity_level)
    if tdee:
        print(f""TDEE (Total Daily Energy Expenditure): {tdee} calories/day"")

# Main execution
if __name__ == '__main__':
    # Input data for health assessment
    weight = 78  # kg
    height = 180  # cm
    gender = ""male""
    age = 25
    activity_level = ""active""  # Options: sedentary, lightly_active, moderately_active, active, very_active

    # Display health assessment
    display_health_assessment(weight, height, gender, age, activity_level)
","Health Assessment for Male, Age 25, Weight 78kg, Height 180cm
Invalid BMR or activity level.",1,1,13,33943
"I want to get appropriate exercise recommendations based on the city's real-time air quality data, including air quality index (AQI) and concentrations of specific pollutants like PM2.5, O3, etc. When the air quality is poor, indoor exercise is recommended.","import requests

# API Key and Headers
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
HEADERS_AIR = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""airvisual1.p.rapidapi.com""
}
HEADERS_GYM = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""gym-fit.p.rapidapi.com""
}

# Function to get air quality data based on city ID
def get_air_quality(city_id):
    """"""
    Retrieves air quality data for the specified city ID.
    """"""
    url = ""https://airvisual1.p.rapidapi.com/cities/v2/get-measurements""
    querystring = {
        ""id"": city_id,
        ""x-user-lang"": ""en-US"",
        ""x-user-timezone"": ""Asia/Singapore"",
        ""x-aqi-index"": ""us""
    }
    response = requests.get(url, headers=HEADERS_AIR, params=querystring)

    # Check if the response is successful
    if response.status_code == 200:
        data = response.json().get(""data"", {}).get(""hourlyMeasurements"", [{}])[0].get(""measurements"", [])
        if not data:
            print(""Warning: No measurement data found."")
        return data
    else:
        print(""Error fetching air quality data:"", response.status_code)
        return []

# Function to get indoor exercise recommendations
def get_indoor_exercises():
    """"""
    Retrieves a list of recommended indoor exercises.
    """"""
    url = ""https://gym-fit.p.rapidapi.com/v1/exercises/search""
    querystring = {""number"": ""50"", ""offset"": ""0""}
    response = requests.get(url, headers=HEADERS_GYM, params=querystring)

    # Filter for indoor exercises
    exercises = response.json().get(""results"", [])
    indoor_exercises = [exercise for exercise in exercises if ""indoor"" in exercise.get(""name"", """").lower()]
    return indoor_exercises

# Function to provide exercise recommendations based on air quality
def exercise_recommendation(city_id):
    """"""
    Provides exercise recommendations based on the AQI and pollution levels in the specified city.
    """"""
    air_quality = get_air_quality(city_id)

    # Print air quality data for debugging
    print(""Air Quality Data:"", air_quality)

    # Extract AQI, PM2.5, and O3 values safely
    aqi = next((item['value'] for item in air_quality if item.get('parameter') == ""AQI""), None)
    pm25 = next((item['value'] for item in air_quality if item.get('parameter') == ""PM2.5""), None)
    o3 = next((item['value'] for item in air_quality if item.get('parameter') == ""O3""), None)

    # Display air quality information
    print(f""\nAir Quality Index (AQI): {aqi}"")
    print(f""PM2.5 Concentration: {pm25} Âµg/mÂ³"")
    print(f""Ozone (O3) Concentration: {o3} Âµg/mÂ³"")

    # Set thresholds for outdoor and indoor exercise recommendations
    if aqi is not None and pm25 is not None:
        if aqi > 100 or pm25 > 35:
            print(""\nAir quality is poor. Recommended indoor exercises:"")
            indoor_exercises = get_indoor_exercises()
            for i, exercise in enumerate(indoor_exercises[:5], 1):  # Display up to 5 indoor exercises
                print(f""{i}. {exercise['name']}"")
        else:
            print(""\nAir quality is good. Outdoor exercises are suitable."")
    else:
        print(""\nAir quality data is incomplete. Unable to provide a recommendation."")

# Main execution
if __name__ == '__main__':
    # Example city ID for air quality data
    city_id = 'hW7vArorRd8cT9h6v'  # Example city ID

    # Provide exercise recommendations based on air quality
    exercise_recommendation(city_id)
",Fail,-1,-1,31,104036
"I want to get a training plan that matches my fitness level, including a difficulty level based on the user's current level, appropriate body part training movements, and detailed workout instructions to help increase the intensity of the workout safely and effectively. Now help me optimize and modify the following code to meet the new requirements above
","import requests

# API Key and Headers
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""exercise-db-fitness-workout-gym.p.rapidapi.com""
}

# Static list of valid muscle groups as a backup
VALID_MUSCLE_GROUPS = [
    ""abductors"", ""adductors"", ""biceps"", ""calves"", ""chest"", ""core"", ""glutes"",
    ""hamstrings"", ""lats"", ""lower_back"", ""middle_back"", ""neck"", ""quadriceps"",
    ""shoulders"", ""traps"", ""triceps""
]

# Function to get exercises based on user's difficulty level
def get_exercises_by_level(level):
    url = f""https://exercise-db-fitness-workout-gym.p.rapidapi.com/exercises/level/{level}""
    response = requests.get(url, headers=HEADERS)

    if response.status_code == 200:
        return response.json()
    else:
        print(""Error fetching exercises by level:"", response.status_code)
        return []

# Function to get a list of valid target muscles (with static backup)
def get_valid_muscle_groups():
    """"""
    Attempt to retrieve a list of valid muscle groups from the API.
    If this fails, use a predefined static list as a backup.
    """"""
    url = ""https://exercisedb.p.rapidapi.com/exercises/targetList""
    response = requests.get(url, headers=HEADERS)

    if response.status_code == 200:
        return response.json()
    else:
        print(""Error fetching muscle groups:"", response.status_code)
        print(""Using static backup muscle group list."")
        return VALID_MUSCLE_GROUPS

# Function to get exercises targeting specific muscle group
def get_exercises_for_target_muscle(target_muscle, limit=5):
    url = f""https://exercisedb.p.rapidapi.com/exercises/target/{target_muscle}""
    querystring = {""limit"": str(limit)}
    response = requests.get(url, headers=HEADERS, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(""Error fetching exercises for target muscle:"", response.status_code)
        return []

# Function to display recommended workout plan based on user level and muscle target
def display_workout_plan(level, target_muscle):
    # Step 1: Get exercises based on user's level
    level_exercises = get_exercises_by_level(level)
    print(f""\nRecommended Exercises for Level '{level.capitalize()}':"")
    for i, exercise in enumerate(level_exercises[:5], 1):
        body_part = exercise.get('bodyPart', 'Not specified')
        equipment = exercise.get('equipment', 'Not specified')
        print(f""{i}. {exercise.get('name')} - Body Part: {body_part} - Equipment: {equipment}"")

    # Step 2: Check if target muscle is valid using static or fetched list
    valid_muscles = get_valid_muscle_groups()
    if target_muscle.lower() not in [muscle.lower() for muscle in valid_muscles]:
        print(f""\nError: '{target_muscle}' is not a valid muscle group."")
        print(""Please choose from the following available muscle groups:"")
        print("", "".join(valid_muscles))
        return

    # Step 3: Get exercises for a specific target muscle group
    target_exercises = get_exercises_for_target_muscle(target_muscle)
    print(f""\nExercises Targeting '{target_muscle.capitalize()}':"")
    for i, exercise in enumerate(target_exercises, 1):
        print(f""{i}. {exercise.get('name')}"")
        print(f""   - Body Part: {exercise.get('bodyPart', 'Not specified')}"")
        print(f""   - Equipment: {exercise.get('equipment', 'Not specified')}"")
        print(f""   - Instructions: {exercise.get('instructions', 'No instructions available')}"")

# Main execution
if __name__ == '__main__':
    # Define user level and target muscle for workout plan
    user_level = 'intermediate'  # Example levels: 'beginner', 'intermediate', 'advanced'
    target_muscle = 'abductors'   # Example target muscle

    # Display workout plan based on user's fitness level and target muscle
    display_workout_plan(user_level, target_muscle)
","Recommended Exercises for Level 'Intermediate':
1. Ab Crunch Machine - Body Part: Not specified - Equipment: machine
2. Ab Roller - Body Part: Not specified - Equipment: other
3. Adductor - Body Part: Not specified - Equipment: foam roll
4. Adductor/Groin - Body Part: Not specified - Equipment: None
5. Advanced Kettlebell Windmill - Body Part: Not specified - Equipment: kettlebells
Error fetching muscle groups: 404
Using static backup muscle group list.
Error fetching exercises for target muscle: 404

Exercises Targeting 'Abductors':",1,1,17,51323
I need to assess their current weight status by BMI (body mass index) and develop a targeted fitness plan based on specific information about the target muscle groups and recommended training movements to better focus the training on the target area.,"import requests

# API Key and Headers
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""gym-fit.p.rapidapi.com""
}

# Function to calculate BMI and assess weight status
def get_bmi(weight, height):
    url = ""https://gym-fit.p.rapidapi.com/v1/calculator/bmi""
    params = {""weight"": weight, ""height"": height}
    response = requests.get(url, headers=HEADERS, params=params)

    # Extract BMI result and calculate weight status
    bmi_value = response.json().get('result')
    if bmi_value:
        if bmi_value < 18.5:
            status = ""Underweight""
        elif 18.5 <= bmi_value < 24.9:
            status = ""Normal weight""
        elif 25 <= bmi_value < 29.9:
            status = ""Overweight""
        else:
            status = ""Obese""
        return bmi_value, status
    else:
        return None, ""Error calculating BMI""

# Function to get exercise details by exercise ID
def get_exercise_by_id(exercise_id):
    url = f""https://gym-fit.p.rapidapi.com/v1/exercises/{exercise_id}""
    response = requests.get(url, headers=HEADERS)
    return response.json()

# Function to get muscle information by muscle ID
def get_muscle_by_id(muscle_id):
    url = f""https://gym-fit.p.rapidapi.com/v1/muscles/{muscle_id}""
    response = requests.get(url, headers=HEADERS)
    return response.json()

# Function to recommend exercises based on BMI and target muscle group
def recommend_exercises(weight, height, target_muscle_id):
    # Step 1: Calculate BMI and determine weight status
    bmi_value, status = get_bmi(weight, height)
    print(f""Your BMI is {bmi_value}, indicating '{status}' status."")

    # Step 2: Get muscle details for the target area
    muscle_details = get_muscle_by_id(target_muscle_id)
    muscle_name = muscle_details.get('name', 'Unknown Muscle')
    print(f""\nTargeting Muscle: {muscle_name}"")

    # Step 3: Get exercises targeting the specified muscle
    exercises = muscle_details.get('exercises', [])
    if not exercises:
        print(f""No exercises found for the muscle '{muscle_name}'."")
        return

    print(f""\nRecommended Exercises for '{muscle_name}':"")
    for i, exercise in enumerate(exercises[:5], 1):  # Limit to top 5 exercises
        exercise_details = get_exercise_by_id(exercise['id'])
        print(f""{i}. {exercise_details.get('name')}"")
        print(f""   - Description: {exercise_details.get('description', 'No description available')}"")
        print(f""   - Equipment: {exercise_details.get('equipment', 'Body weight')}"")
        print(f""   - Instructions: {exercise_details.get('instructions', 'No instructions provided')}"")

# Main execution
if __name__ == '__main__':
    # Example user data
    weight = 78  # User's weight in kilograms
    height = 180  # User's height in centimeters
    target_muscle_id = 'biceps'  # Example target muscle ID

    # Recommend exercises based on user's BMI and target muscle group
    recommend_exercises(weight, height, target_muscle_id)
","Your BMI is 24.1, indicating 'Normal weight' status.

Targeting Muscle: Unknown Muscle
No exercises found for the muscle 'Unknown Muscle'.
",1,1,11,34045
"I want to get training movements for specific muscle groups to strengthen muscles or shape. The system needs to provide information about the target muscle groups, recommend suitable training movements, and give detailed instructions and steps for each movement to help users train effectively.

","import requests

# API Key and Headers
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""exercisedb.p.rapidapi.com""
}

# Function to get the list of available target muscle groups
def get_muscle_groups():
    url = ""https://exercisedb.p.rapidapi.com/exercises/targetList""
    response = requests.get(url, headers=HEADERS)

    if response.status_code == 200:
        return response.json()
    else:
        print(""Error fetching muscle groups:"", response.status_code)
        return []

# Function to get exercises targeting a specific muscle
def get_exercises_for_muscle(target, limit=10, offset=0):
    url = f""https://exercisedb.p.rapidapi.com/exercises/target/{target}""
    querystring = {""limit"": str(limit), ""offset"": str(offset)}
    response = requests.get(url, headers=HEADERS, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(""Error fetching exercises for target muscle:"", response.status_code)
        return []

# Function to search detailed information about specific muscles
def get_muscle_info(muscle_name, number=1, offset=0):
    """"""
    Attempts to fetch detailed information about a specific muscle.
    If the API returns a string list instead of a dictionary, the function will handle it gracefully.
    """"""
    url = ""https://gym-fit.p.rapidapi.com/v1/muscles/search""
    querystring = {
        ""query"": muscle_name,
        ""number"": str(number),
        ""offset"": str(offset)
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""gym-fit.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        data = response.json()
        # Ensure data is a list of dictionaries, not a list of strings
        if isinstance(data, list) and all(isinstance(item, dict) for item in data):
            return data
        else:
            print(""Unexpected data format received for muscle information."")
            return []
    else:
        print(""Error fetching muscle information:"", response.status_code)
        return []

# Main function to provide a workout plan focused on a specific muscle group
def display_muscle_workout_plan(target_muscle):
    # Step 1: Get muscle group info
    muscle_info = get_muscle_info(target_muscle)
    if muscle_info:
        print(f""\nMuscle Information for '{target_muscle.capitalize()}':"")
        for muscle in muscle_info:
            print(f"" - Name: {muscle.get('name', 'N/A')}"")
            print(f"" - Description: {muscle.get('description', 'No description available')}"")
    else:
        print(f""No detailed information available for '{target_muscle}'."")

    # Step 2: Get exercises for the selected muscle group
    exercises = get_exercises_for_muscle(target_muscle)
    if exercises:
        print(f""\nRecommended Exercises for '{target_muscle.capitalize()}':"")
        for i, exercise in enumerate(exercises, 1):
            print(f""\n{i}. {exercise.get('name')}"")
            print(f""   - Body Part: {exercise.get('bodyPart', 'Not specified')}"")
            print(f""   - Equipment: {exercise.get('equipment', 'Body weight')}"")
            print(f""   - Instructions: {exercise.get('instructions', 'No instructions provided')}"")
            print(f""   - Steps: {exercise.get('steps', 'No step-by-step guide available')}"")
    else:
        print(f""No exercises found for muscle '{target_muscle}'."")

# Main execution
if __name__ == '__main__':
    # Step 1: List available muscle groups
    muscle_groups = get_muscle_groups()
    print(""Available Muscle Groups:"", "", "".join(muscle_groups))

    # Example: Choose a target muscle group for focused training (e.g., 'biceps')
    target_muscle = ""biceps""  # This can be dynamic based on user choice

    # Step 2: Display workout plan based on the chosen muscle group
    display_muscle_workout_plan(target_muscle)
",Fail,-1,-1,30,107220
"By entering the postcode, I want to get additional postcode and location data within a 5km radius of the location, which can be used to explore nearby areas or find adjacent locations.","import requests

# API Key and Headers
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
ZIP_API_HOST = ""zip-code-distance-radius.p.rapidapi.com""
GEO_API_HOST = ""navitime-geocoding.p.rapidapi.com""

# Function to get zip codes within a radius
def get_zipcodes_within_radius(zip_code, radius):
    url = f""https://{ZIP_API_HOST}/api/zipCodesWithinRadius""
    querystring = {""zipCode"": zip_code, ""radius"": str(radius)}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ZIP_API_HOST
    }
    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching zip codes within radius: {response.status_code}"")
        return []

# Function to get detailed location data based on postal code
def get_location_data(postal_code):
    """"""
    Retrieves location data for a given postal code. Returns None if an error occurs.
    """"""
    url = f""https://{GEO_API_HOST}/address/postal_code""
    querystring = {""datum"": ""wgs84"", ""coord_unit"": ""degree"", ""limit"": ""1"", ""postal_code"": postal_code}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": GEO_API_HOST
    }
    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching location data for postal code {postal_code}: {response.status_code}"")
        return None

# Main function to display nearby zip codes with location data
def display_nearby_zipcode_data(zip_code, radius_km):
    # Step 1: Get nearby zip codes within the specified radius
    zip_codes_data = get_zipcodes_within_radius(zip_code, radius_km)

    if not zip_codes_data:
        print(""No nearby zip codes found."")
        return

    # Step 2: Retrieve and display location data for each nearby zip code
    print(f""Nearby Locations within {radius_km} km of {zip_code}:"")
    for item in zip_codes_data:
        # Try to fetch location details; if unavailable, mark as N/A
        zip_code_detail = get_location_data(item[""zipCode""])

        location_name = zip_code_detail.get(""location"", {}).get(""name"", ""N/A"") if zip_code_detail else ""N/A""
        coordinates = zip_code_detail.get(""location"", {}).get(""coordinates"", ""N/A"") if zip_code_detail else ""N/A""

        print(f""\nZip Code: {item['zipCode']}"")
        print(f""  - Distance: {item['distance']} km"")
        print(f""  - Location: {location_name}"")
        print(f""  - Coordinates: {coordinates}"")

# Main execution
if __name__ == '__main__':
    # Input parameters
    zip_code = ""48195""  # Example zip code
    radius_km = 5       # Radius in km

    # Display nearby zip code data with location details
    display_nearby_zipcode_data(zip_code, radius_km)
","Nearby Locations within 5 km of 48195:
Error fetching location data for postal code 48195: 400

Zip Code: 48195
  - Distance: 0.0 km
  - Location: N/A
  - Coordinates: N/A
Error fetching location data for postal code 48193: 400

Zip Code: 48193
  - Distance: 2.1363509674204266 km
  - Location: N/A
  - Coordinates: N/A
Error fetching location data for postal code 48192: 400

Zip Code: 48192
  - Distance: 2.5145184538923644 km
  - Location: N/A
  - Coordinates: N/A
Error fetching location data for postal code 48146: 400

Zip Code: 48146
  - Distance: 2.9437606650577206 km
  - Location: N/A
  - Coordinates: N/A
Error fetching location data for postal code 48180: 400

Zip Code: 48180
  - Distance: 3.476270389701451 km
  - Location: N/A
  - Coordinates: N/A
Error fetching location data for postal code 48101: 400

Zip Code: 48101
  - Distance: 3.790888925723768 km
  - Location: N/A
  - Coordinates: N/A
Error fetching location data for postal code 48229: 400

Zip Code: 48229
  - Distance: 4.555562331497093 km
  - Location: N/A
  - Coordinates: N/A
",1,1,13,32656
"I would like to query a country and a subset of cities in a specific state to get geographic information about each city for display on a map or for regional analysis.

","import requests

# API Key and Headers
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
BASE_URL = ""https://rwanda.p.rapidapi.com""
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""rwanda.p.rapidapi.com""
}

# Function to get provinces
def get_provinces():
    url = f""{BASE_URL}/provinces""
    response = requests.get(url, headers=HEADERS)

    if response.status_code == 200:
        return response.json().get(""data"", [])
    else:
        print(f""Error fetching provinces: {response.status_code}"")
        return []

# Function to get districts in a given province
def get_districts(province):
    url = f""{BASE_URL}/districts""
    params = {""p"": province}
    response = requests.get(url, headers=HEADERS, params=params)

    if response.status_code == 200:
        return response.json().get(""data"", [])
    else:
        print(f""Error fetching districts for province {province}: {response.status_code}"")
        return []

# Function to get villages in a given province and district
def get_villages(province, district):
    url = f""{BASE_URL}/villages""
    params = {""p"": province, ""d"": district}
    response = requests.get(url, headers=HEADERS, params=params)

    if response.status_code == 200:
        return response.json().get(""data"", [])
    else:
        print(f""Error fetching villages for district {district} in province {province}: {response.status_code}"")
        return []

# Main function to collect administrative data
def collect_administrative_data():
    all_data = {}

    # Step 1: Get all provinces
    provinces = get_provinces()
    for province in provinces:
        province_name = province.get('name', 'Unknown Province')
        province_data = {}

        # Step 2: Get districts for each province
        districts = get_districts(province_name)
        for district in districts:
            district_name = district.get('name', 'Unknown District')

            # Step 3: Get villages for each district
            villages = get_villages(province_name, district_name)
            province_data[district_name] = villages

        all_data[province_name] = province_data

    return all_data

if __name__ == '__main__':
    # Collect and print administrative data
    administrative_data = collect_administrative_data()
    print(""Administrative Data Structure:"")
    for province, districts in administrative_data.items():
        print(f""\nProvince: {province}"")
        for district, villages in districts.items():
            print(f""  District: {district}"")
            for village in villages:
                print(f""    Village: {village}"")
","Top 10 Cities in Florida, United States:
 - City: Aberdeen, Latitude: 26.55063000, Longitude: -80.14866000
 - City: Alachua, Latitude: 29.75163000, Longitude: -82.42483000
 - City: Alachua County, Latitude: 29.67476000, Longitude: -82.35770000
 - City: Alafaya, Latitude: 28.56410000, Longitude: -81.21140000
 - City: Allapattah, Latitude: 25.81454000, Longitude: -80.22394000
 - City: Altamonte Springs, Latitude: 28.66111000, Longitude: -81.36562000
 - City: Alturas, Latitude: 27.87169000, Longitude: -81.71508000
 - City: Alva, Latitude: 26.71562000, Longitude: -81.61008000
 - City: Andover, Latitude: 25.96843000, Longitude: -80.21283000
 - City: Anna Maria, Latitude: 27.53115000, Longitude: -82.73343000",1,1,14,42686
"I need to obtain the information of some of the ten administrative units (such as provinces, regions, townships, and villages) in a specified country/region for administrative data collation or geographic classification display.

","import requests

# API Key and Headers
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
BASE_URL = ""https://rwanda.p.rapidapi.com""
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""rwanda.p.rapidapi.com""
}

# Function to get provinces (limit to first 10)
def get_provinces():
    print(""Fetching provinces..."")
    url = f""{BASE_URL}/provinces""
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        if response.status_code == 200:
            return response.json().get(""data"", [])[:10]  # Limit to 10 provinces
        else:
            print(f""Error fetching provinces: {response.status_code}"")
    except requests.exceptions.Timeout:
        print(""Request timed out while fetching provinces."")
    return []

# Function to get districts in a given province (limit to first 10)
def get_districts(province):
    print(f""Fetching districts for province: {province}"")
    url = f""{BASE_URL}/districts""
    params = {""p"": province}
    try:
        response = requests.get(url, headers=HEADERS, params=params, timeout=10)
        if response.status_code == 200:
            return response.json().get(""data"", [])[:10]  # Limit to 10 districts
        else:
            print(f""Error fetching districts for province {province}: {response.status_code}"")
    except requests.exceptions.Timeout:
        print(f""Request timed out while fetching districts for province: {province}"")
    return []

# Function to get villages in a given province and district (limit to first 10)
def get_villages(province, district):
    print(f""Fetching villages for district: {district} in province: {province}"")
    url = f""{BASE_URL}/villages""
    params = {""p"": province, ""d"": district}
    try:
        response = requests.get(url, headers=HEADERS, params=params, timeout=10)
        if response.status_code == 200:
            return response.json().get(""data"", [])[:10]  # Limit to 10 villages
        else:
            print(f""Error fetching villages for district {district} in province {province}: {response.status_code}"")
    except requests.exceptions.Timeout:
        print(f""Request timed out while fetching villages for district: {district} in province: {province}"")
    return []

# Main function to collect administrative data
def collect_administrative_data():
    all_data = {}

    # Step 1: Get all provinces (limited to 10)
    provinces = get_provinces()
    for province in provinces:
        if isinstance(province, dict):
            province_name = province.get('name', 'Unknown Province')
        elif isinstance(province, str):
            province_name = province
        else:
            print(f""Unexpected data format for province: {province}"")
            continue

        province_data = {}

        # Step 2: Get districts for each province (limited to 10)
        districts = get_districts(province_name)
        for district in districts:
            if isinstance(district, dict):
                district_name = district.get('name', 'Unknown District')
            elif isinstance(district, str):
                district_name = district
            else:
                print(f""Unexpected data format for district: {district}"")
                continue

            # Step 3: Get villages for each district (limited to 10)
            villages = get_villages(province_name, district_name)
            province_data[district_name] = villages

        all_data[province_name] = province_data

    return all_data

if __name__ == '__main__':
    # Collect and print administrative data
    print(""Starting data collection..."")
    administrative_data = collect_administrative_data()
    print(""Data collection completed."")
    print(""Administrative Data Structure:"")
    for province, districts in administrative_data.items():
        print(f""\nProvince: {province}"")
        for district, villages in districts.items():
            print(f""  District: {district}"")
            for village in villages:
                print(f""    Village: {village}"")
",Fail,-1,-1,29,86681
Enter an address and the system will automatically suggest completions to quickly find the correct address and get relevant geographic information.,"import requests

# API Key and Headers
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""navitime-geocoding.p.rapidapi.com""
}

def address_autocomplete(word):
    url = ""https://navitime-geocoding.p.rapidapi.com/address/autocomplete""
    querystring = {""datum"": ""wgs84"", ""word"": word, ""coord_unit"": ""degree""}
    response = requests.get(url, headers=HEADERS, params=querystring)

    if response.status_code == 200:
        data = response.json()
        return data.get(""items"", [])
    else:
        print(f""Error fetching autocomplete suggestions: {response.status_code}"")
        return []

def get_address_details(code):
    url = ""https://navitime-geocoding.p.rapidapi.com/address""
    querystring = {""coord_unit"": ""degree"", ""datum"": ""wgs84"", ""code"": code}
    response = requests.get(url, headers=HEADERS, params=querystring)

    if response.status_code == 200:
        data = response.json()
        return data.get(""items"", [])
    else:
        print(f""Error fetching address details: {response.status_code}"")
        return []

if __name__ == '__main__':
    # ä½¿ç”¨é»˜è®¤åœ°å€å…³é”®å­— 'ä»£ã€…æœ¨' è¿›è¡Œè¡¥å…¨
    user_input = ""ä»£ã€…æœ¨""
    autocomplete_results = address_autocomplete(user_input)

    if autocomplete_results:
        print(""Autocomplete suggestions:"")
        for idx, suggestion in enumerate(autocomplete_results):
            print(f""{idx + 1}. {suggestion['name']} - {suggestion.get('address', 'N/A')}"")

        # å‡è®¾ç”¨æˆ·é€‰æ‹©ç¬¬ä¸€ä¸ªç»“æžœ
        selected_index = 0
        selected_code = autocomplete_results[selected_index][""code""]

        # èŽ·å–è¯¦ç»†çš„åœ°ç†ä¿¡æ¯
        detailed_info = get_address_details(selected_code)
        if detailed_info:
            print(""\nDetailed geographic information:"")
            for item in detailed_info:
                print(f""Name: {item.get('name', 'N/A')}"")
                print(f""Address: {item.get('address', 'N/A')}"")
                print(f""Latitude: {item.get('latitude', 'N/A')}"")
                print(f""Longitude: {item.get('longitude', 'N/A')}"")
                print(""\n---\n"")
        else:
            print(""No detailed information found for the selected address."")
    else:
        print(""No autocomplete suggestions available."")
",Fail,-1,-1,30,85860
"I enter the starting point and destination, and I need to provide the best driving route, show the driving path, and estimate the arrival time.

","import requests
from datetime import datetime, timedelta

# API Key and Headers
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def FindDrivingRoute(start, destination):
    url = ""https://trueway-directions2.p.rapidapi.com/FindDrivingRoute""
    querystring = {
        ""stops"": f""{start[0]},{start[1]};{destination[0]},{destination[1]}"",
        ""optimize"": ""true"",
        ""geometry_format"": ""latlng""
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""trueway-directions2.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching driving route: {response.status_code}"")
        return None

def GetDistance(start, destination):
    url = ""https://geocodeapi.p.rapidapi.com/GetDistance""
    querystring = {
        ""lat1"": start[0],
        ""lon1"": start[1],
        ""lat2"": destination[0],
        ""lon2"": destination[1]
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""geocodeapi.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching distance: {response.status_code}"")
        return None

def GetTimezone(lat, lon):
    url = ""https://geocodeapi.p.rapidapi.com/GetTimezone""
    querystring = {""latitude"": lat, ""longitude"": lon}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""geocodeapi.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching timezone: {response.status_code}"")
        return None

if __name__ == '__main__':
    # é»˜è®¤çš„èµ·ç‚¹å’Œç›®çš„åœ°åæ ‡ï¼ˆçº½çº¦å¸‚ Times Square å’Œ Brooklyn Bridgeï¼‰
    start = (40.758896, -73.985130)  # Times Square
    destination = (40.706086, -73.996864)  # Brooklyn Bridge

    # èŽ·å–è¡Œè½¦è·¯çº¿è¯¦æƒ…
    route_data = FindDrivingRoute(start, destination)
    if route_data and 'route' in route_data:
        route_duration_seconds = route_data['route']['duration']
        route_distance = route_data['route']['distance']
        route_path = route_data['route']['geometry']['coordinates']

        # èŽ·å–è·ç¦»å’Œæ–¹ä½ä¿¡æ¯
        distance_data = GetDistance(start, destination)

        # èŽ·å–ç›®çš„åœ°çš„æ—¶åŒºï¼Œä»¥ä¾¿è®¡ç®—æœ¬åœ°åˆ°è¾¾æ—¶é—´
        timezone_data = GetTimezone(destination[0], destination[1])
        if timezone_data and 'LocalTime_Now' in timezone_data:
            local_time_now = datetime.strptime(timezone_data['LocalTime_Now'], '%I:%M:%S %p')
            arrival_time = local_time_now + timedelta(seconds=route_duration_seconds)

            # æ˜¾ç¤ºç»“æžœ
            print(""Driving route details:"")
            print(""Distance:"", route_distance, ""meters"")
            print(""Estimated Duration:"", route_duration_seconds // 60, ""minutes"")
            print(""Arrival Time (local at destination):"", arrival_time.strftime('%I:%M:%S %p'))
            print(""Route Path:"", route_path)
        else:
            print(""Failed to retrieve local time at destination."")
    else:
        print(""Failed to retrieve driving route details."")
","Driving route details:
Distance: 9777 meters
Estimated Duration: 21 minutes
Arrival Time (local at destination): 10:19:25 AM
Route Path: [[40.758773, -73.984914], [40.758553, -73.985072], [40.758314, -73.98525], [40.758274, -73.985279], [40.758245, -73.985309], [40.757937, -73.985536], [40.757769, -73.985655], [40.757542, -73.985824], [40.757532, -73.985824], [40.757424, -73.985903], [40.757325, -73.985972], [40.75702, -73.986201], [40.756696, -73.98643], [40.75652, -73.98657], [40.756451, -73.98662], [40.756049, -73.98692], [40.755685, -73.987201], [40.755409, -73.987412], [40.755162, -73.987573], [40.755013, -73.987704], [40.754775, -73.987866], [40.754616, -73.987977], [40.754457, -73.988098], [40.754407, -73.988139], [40.754307, -73.988209], [40.754297, -73.98822], [40.754257, -73.98824], [40.754177, -73.988301], [40.753544, -73.988787], [40.752916, -73.989233], [40.752763, -73.989345], [40.752438, -73.989578], [40.752285, -73.989689], [40.752132, -73.989801], [40.751817, -73.990033], [40.751766, -73.990064], [40.751654, -73.990145], [40.751369, -73.990367], [40.751135, -73.990538], [40.750983, -73.990639], [40.750588, -73.989679], [40.750487, -73.989436], [40.750416, -73.989254], [40.750235, -73.988858], [40.750054, -73.988402], [40.749933, -73.988139], [40.749853, -73.987957], [40.749813, -73.987855], [40.749753, -73.987734], [40.749663, -73.987502], [40.749523, -73.98714], [40.749404, -73.98689], [40.749394, -73.98685], [40.749215, -73.98644], [40.749175, -73.98634], [40.749105, -73.986181], [40.748858, -73.985576], [40.748848, -73.985546], [40.74864, -73.985062], [40.748433, -73.984559], [40.748128, -73.983819], [40.747981, -73.983503], [40.747843, -73.983186], [40.747765, -73.982978], [40.747539, -73.98245], [40.747431, -73.98218], [40.747343, -73.98198], [40.747294, -73.98187], [40.747225, -73.981709], [40.747206, -73.981659], [40.747127, -73.981467], [40.747088, -73.981367], [40.747058, -73.981276], [40.747009, -73.981165], [40.746931, -73.980963], [40.746891, -73.980861], [40.746576, -73.980091], [40.746557, -73.98003], [40.746429, -73.979726], [40.74634, -73.979513], [40.74629, -73.979392], [40.746251, -73.979301], [40.745984, -73.978644], [40.745904, -73.978462], [40.745765, -73.9781], [40.745556, -73.9776], [40.745437, -73.97732], [40.745257, -73.976933], [40.745157, -73.976695], [40.745097, -73.976536], [40.744806, -73.975884], [40.744735, -73.975707], [40.744725, -73.975697], [40.744614, -73.975421], [40.744533, -73.975224], [40.744483, -73.975096], [40.744301, -73.974701], [40.744169, -73.974394], [40.744017, -73.974038], [40.743885, -73.973729], [40.743824, -73.97353], [40.743723, -73.97337], [40.743601, -73.97308], [40.74357, -73.973], [40.743458, -73.972739], [40.743306, -73.972377], [40.743285, -73.972326], [40.743234, -73.972195], [40.743214, -73.972155], [40.743194, -73.972185], [40.743163, -73.972225], [40.743143, -73.972256], [40.743132, -73.972286], [40.743112, -73.972316], [40.743031, -73.972336], [40.742959, -73.972367], [40.742786, -73.972447], [40.742766, -73.972467], [40.742755, -73.972467], [40.742725, -73.972477], [40.742674, -73.972508], [40.742613, -73.972528], [40.742542, -73.972558], [40.74246, -73.972588], [40.742369, -73.972628], [40.742297, -73.972649], [40.742226, -73.972679], [40.742145, -73.972709], [40.742054, -73.972729], [40.741973, -73.972749], [40.741881, -73.972779], [40.74179, -73.972799], [40.74175, -73.972799], [40.741679, -73.972819], [40.741547, -73.97285], [40.741366, -73.97288], [40.741335, -73.97289], [40.741094, -73.97292], [40.740943, -73.97294], [40.740923, -73.97295], [40.740863, -73.97288], [40.740703, -73.97292], [40.740643, -73.97293], [40.740573, -73.97294], [40.740394, -73.97298], [40.740334, -73.97299], [40.740145, -73.97303], [40.740036, -73.97305], [40.739947, -73.97307], [40.739917, -73.97307], [40.739868, -73.97308], [40.739828, -73.97309], [40.73967, -73.97313], [40.739621, -73.97315], [40.739512, -73.97318], [40.739414, -73.97321], [40.739305, -73.97326], [40.739207, -73.9733], [40.73906, -73.97337], [40.738971, -73.97341], [40.738951, -73.97342], [40.738893, -73.97346], [40.738804, -73.97351], [40.738716, -73.97356], [40.738618, -73.97363], [40.738529, -73.97369], [40.738451, -73.973749], [40.738353, -73.973819], [40.738255, -73.973889], [40.738166, -73.973958], [40.738068, -73.974028], [40.73797, -73.974097], [40.737842, -73.974196], [40.737636, -73.974345], [40.737616, -73.974365], [40.737547, -73.974414], [40.737448, -73.974493], [40.737438, -73.974493], [40.73735, -73.974563], [40.737211, -73.974661], [40.737172, -73.974691], [40.737063, -73.97476], [40.737043, -73.97477], [40.736974, -73.97481], [40.736924, -73.974839], [40.736904, -73.974849], [40.736875, -73.974869], [40.736825, -73.974889], [40.736775, -73.974908], [40.736686, -73.974948], [40.736676, -73.974948], [40.736586, -73.974977], [40.736477, -73.975007], [40.736447, -73.975017], [40.736367, -73.975037], [40.736277, -73.975056], [40.736147, -73.975066], [40.736097, -73.975066], [40.736057, -73.975066], [40.736047, -73.975066], [40.735996, -73.975066], [40.735956, -73.975066], [40.735916, -73.975066], [40.735876, -73.975066], [40.735826, -73.975066], [40.735775, -73.975056], [40.735715, -73.975046], [40.735705, -73.975046], [40.735655, -73.975037], [40.735574, -73.975017], [40.735473, -73.974987], [40.735362, -73.974968], [40.735352, -73.974958], [40.73525, -73.974938], [40.735169, -73.974918], [40.735159, -73.974918], [40.735027, -73.974879], [40.734763, -73.974819], [40.73454, -73.97476], [40.734489, -73.97475], [40.734316, -73.974711], [40.734163, -73.974671], [40.73398, -73.974622], [40.733623, -73.974533], [40.73345, -73.974493], [40.733338, -73.974464], [40.733003, -73.974385], [40.732922, -73.974365], [40.732739, -73.974315], [40.732568, -73.974276], [40.732446, -73.974246], [40.732325, -73.974206], [40.732154, -73.974167], [40.732014, -73.974137], [40.731903, -73.974107], [40.731793, -73.974077], [40.731743, -73.974067], [40.731673, -73.974038], [40.731583, -73.974008], [40.731563, -73.973998], [40.731414, -73.973938], [40.731374, -73.973908], [40.731264, -73.973859], [40.731205, -73.973819], [40.731155, -73.973789], [40.731046, -73.97371], [40.730947, -73.97363], [40.730917, -73.9736], [40.730897, -73.97358], [40.730828, -73.97351], [40.730739, -73.97342], [40.73068, -73.97335], [40.73064, -73.97331], [40.730571, -73.97322], [40.730355, -73.97296], [40.730325, -73.97291], [40.730315, -73.9729], [40.730217, -73.972779], [40.729961, -73.972467], [40.729843, -73.972356], [40.729696, -73.972225], [40.729598, -73.972155], [40.72949, -73.972074], [40.729333, -73.971983], [40.729323, -73.971983], [40.729196, -73.971922], [40.729098, -73.971892], [40.729019, -73.971861], [40.728901, -73.971821], [40.728891, -73.971821], [40.728813, -73.971811], [40.728704, -73.971791], [40.728616, -73.97178], [40.728498, -73.97178], [40.72834, -73.971791], [40.727964, -73.971841], [40.727706, -73.971872], [40.727317, -73.971932], [40.727087, -73.971963], [40.726886, -73.971993], [40.726574, -73.972074], [40.726402, -73.972134], [40.72625, -73.972195], [40.726048, -73.972286], [40.725906, -73.972356], [40.725885, -73.972377], [40.725845, -73.972397], [40.725652, -73.972498], [40.72557, -73.972548], [40.72556, -73.972548], [40.72553, -73.972568], [40.725336, -73.972679], [40.725234, -73.972739], [40.725183, -73.972769], [40.725132, -73.972809], [40.725071, -73.97284], [40.72499, -73.9729], [40.724888, -73.97295], [40.724725, -73.97305], [40.724633, -73.97311], [40.724603, -73.97313], [40.724491, -73.9732], [40.72443, -73.97325], [40.724399, -73.97326], [40.724277, -73.97334], [40.724135, -73.97343], [40.724104, -73.97344], [40.723658, -73.973729], [40.723456, -73.973849], [40.723295, -73.973948], [40.723164, -73.974028], [40.722863, -73.974216], [40.722663, -73.974325], [40.722563, -73.974375], [40.722463, -73.974424], [40.722364, -73.974474], [40.722254, -73.974513], [40.722145, -73.974543], [40.722105, -73.974563], [40.722046, -73.974582], [40.721927, -73.974602], [40.721828, -73.974632], [40.721719, -73.974652], [40.721621, -73.974661], [40.721621, -73.974671], [40.721296, -73.974701], [40.720853, -73.97476], [40.720696, -73.97477], [40.720657, -73.97478], [40.720333, -73.974819], [40.720166, -73.974839], [40.720157, -73.974839], [40.720009, -73.974859], [40.719616, -73.974908], [40.719241, -73.974958], [40.719103, -73.974977], [40.719043, -73.974987], [40.718895, -73.974997], [40.718686, -73.975027], [40.718556, -73.975046], [40.718427, -73.975066], [40.718377, -73.975076], [40.718207, -73.975116], [40.717996, -73.975165], [40.717946, -73.975175], [40.717896, -73.975185], [40.717866, -73.975194], [40.717584, -73.975283], [40.717372, -73.975342], [40.717291, -73.975372], [40.716835, -73.975549], [40.716631, -73.975628], [40.716204, -73.975845], [40.716112, -73.975894], [40.715908, -73.975993], [40.715755, -73.976072], [40.715633, -73.976131], [40.715531, -73.976181], [40.715409, -73.97625], [40.715044, -73.976428], [40.714891, -73.976507], [40.71477, -73.976566], [40.714709, -73.976596], [40.714568, -73.976665], [40.714537, -73.976685], [40.714315, -73.976794], [40.714235, -73.976833], [40.714205, -73.976853], [40.713983, -73.976972], [40.713344, -73.97729], [40.713245, -73.97734], [40.713195, -73.97737], [40.712967, -73.97749], [40.712957, -73.97749], [40.712818, -73.97757], [40.712571, -73.97769], [40.712325, -73.97784], [40.712276, -73.97787], [40.712227, -73.9779], [40.712138, -73.97799], [40.711932, -73.97816], [40.711755, -73.978372], [40.711608, -73.978573], [40.711461, -73.978815], [40.711441, -73.978845], [40.711274, -73.979179], [40.711157, -73.979463], [40.711137, -73.979534], [40.711068, -73.979757], [40.711039, -73.979899], [40.711009, -73.98003], [40.710999, -73.980101], [40.71096, -73.980294], [40.71096, -73.980304], [40.710931, -73.980588], [40.710921, -73.980699], [40.710881, -73.981306], [40.710862, -73.981618], [40.710832, -73.98202], [40.710832, -73.98205], [40.710793, -73.98256], [40.710783, -73.982719], [40.710763, -73.982958], [40.710714, -73.98376], [40.710665, -73.98449], [40.710645, -73.984677], [40.710626, -73.984875], [40.710527, -73.985764], [40.710507, -73.985873], [40.710468, -73.986231], [40.710458, -73.98635], [40.710438, -73.98646], [40.710419, -73.98669], [40.710281, -73.987845], [40.710281, -73.987876], [40.710251, -73.988108], [40.710231, -73.988351], [40.710122, -73.989254], [40.710093, -73.989497], [40.710063, -73.98975], [40.709994, -73.990215], [40.709964, -73.990457], [40.709944, -73.990618], [40.709924, -73.990689], [40.709895, -73.99092], [40.709865, -73.99116], [40.709825, -73.99139], [40.709815, -73.99147], [40.709775, -73.99171], [40.709746, -73.991918], [40.709746, -73.991938], [40.709706, -73.992167], [40.709686, -73.992355], [40.709666, -73.992454], [40.709646, -73.992592], [40.709606, -73.992819], [40.709596, -73.992918], [40.709556, -73.993145], [40.709537, -73.993283], [40.709527, -73.993372], [40.709497, -73.993559], [40.709497, -73.993608], [40.709467, -73.993835], [40.709437, -73.994072], [40.709427, -73.994181], [40.709387, -73.994507], [40.709337, -73.994824], [40.709297, -73.995161], [40.709247, -73.99549], [40.709207, -73.99578], [40.709187, -73.99594], [40.709177, -73.99601], [40.709147, -73.996201], [40.709127, -73.996382], [40.709067, -73.996815], [40.709057, -73.996855], [40.709027, -73.997088], [40.709027, -73.997128], [40.708996, -73.997321], [40.708976, -73.997442], [40.708956, -73.997544], [40.708956, -73.997564], [40.708916, -73.997726], [40.708916, -73.997736], [40.708856, -73.997959], [40.708856, -73.99797], [40.708806, -73.998152], [40.708765, -73.998264], [40.708735, -73.998365], [40.708725, -73.998406], [40.708695, -73.998487], [40.708644, -73.998598], [40.708604, -73.99871], [40.708564, -73.998821], [40.708554, -73.998851], [40.708533, -73.998952], [40.708523, -73.999023], [40.708513, -73.999074], [40.708513, -73.999175], [40.708513, -73.999276], [40.708523, -73.999356], [40.708533, -73.999437], [40.708544, -73.999498], [40.708564, -73.999588], [40.708634, -73.99999], [40.708665, -74.0001], [40.708665, -74.00012], [40.708675, -74.00015], [40.708705, -74.00023], [40.708715, -74.00027], [40.708735, -74.00032], [40.708745, -74.00037], [40.708755, -74.0004], [40.708795, -74.00048], [40.708816, -74.00051], [40.708826, -74.00053], [40.708886, -74.00063], [40.708926, -74.00069], [40.708946, -74.000739], [40.709007, -74.000869], [40.709107, -74.000998], [40.709207, -74.001127], [40.709287, -74.001236], [40.709377, -74.001355], [40.709437, -74.001434], [40.709566, -74.001602], [40.709606, -74.001661], [40.709676, -74.00174], [40.709934, -74.002076], [40.710112, -74.002293], [40.710419, -74.002687], [40.710468, -74.002756], [40.710517, -74.002815], [40.710557, -74.002875], [40.710645, -74.002983], [40.710675, -74.003023], [40.710714, -74.003082], [40.710744, -74.003131], [40.710773, -74.003171], [40.710862, -74.003309], [40.710881, -74.003378], [40.710901, -74.003477], [40.710911, -74.003536], [40.710921, -74.003586], [40.710931, -74.003635], [40.710931, -74.003685], [40.71094, -74.003764], [40.71095, -74.003784], [40.71096, -74.003853], [40.71097, -74.003933], [40.71097, -74.003992], [40.71098, -74.004022], [40.71099, -74.004111], [40.710999, -74.004171], [40.711019, -74.004221], [40.711039, -74.004241], [40.711039, -74.004251], [40.711068, -74.00429], [40.711098, -74.00432], [40.711127, -74.00434], [40.711166, -74.00435], [40.711196, -74.00436], [40.711225, -74.00436], [40.711265, -74.00436], [40.711294, -74.00435], [40.711323, -74.00434], [40.711353, -74.00432], [40.711392, -74.00429], [40.711402, -74.004271], [40.711431, -74.004241], [40.711451, -74.004181], [40.711461, -74.004151], [40.711471, -74.004131], [40.711471, -74.004101], [40.71148, -74.004072], [40.71148, -74.004042], [40.71148, -74.004012], [40.71148, -74.003982], [40.711471, -74.003952], [40.711461, -74.003903], [40.711441, -74.003863], [40.711431, -74.003824], [40.711392, -74.003764], [40.711353, -74.003705], [40.711304, -74.003635], [40.711265, -74.003586], [40.711235, -74.003546], [40.711206, -74.003497], [40.711176, -74.003457], [40.711127, -74.003388], [40.711088, -74.003319], [40.710881, -74.003023], [40.710793, -74.002894], [40.710675, -74.002717], [40.710419, -74.002392], [40.71029, -74.002234], [40.709904, -74.00175], [40.709736, -74.001543], [40.709566, -74.001335], [40.709377, -74.001097], [40.709347, -74.001057], [40.709117, -74.000769], [40.709017, -74.00064], [40.708906, -74.00051], [40.708806, -74.00038], [40.708685, -74.00023], [40.708655, -74.00019], [40.708614, -74.00015], [40.708554, -74.00007], [40.708473, -73.99998], [40.708362, -73.99984], [40.70825, -73.999689], [40.708119, -73.999518], [40.707967, -73.999326], [40.707896, -73.999245], [40.707824, -73.999155], [40.707723, -73.999023], [40.707631, -73.998922], [40.707275, -73.998466], [40.707132, -73.998304], [40.70643, -73.997422], [40.706297, -73.99725], [40.706023, -73.996906]]
",1,1,17,46733
"I enter GPS coordinates to determine if the location is on a body of water (such as a lake, river, or ocean) and provide information about the nearest city so I know the geographic location.

","import requests

# API Key and Headers
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""geocodeapi.p.rapidapi.com""
}

def is_on_water(lat, lon):
    url = ""https://geocodeapi.p.rapidapi.com/isonwater""
    querystring = {""lat"": str(lat), ""lon"": str(lon)}
    response = requests.get(url, headers=HEADERS, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching water information: {response.status_code}"")
        return None

def get_nearest_cities(latitude, longitude, range_meters):
    url = ""https://geocodeapi.p.rapidapi.com/GetNearestCities""
    querystring = {""latitude"": str(latitude), ""longitude"": str(longitude), ""range"": str(range_meters)}
    response = requests.get(url, headers=HEADERS, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching nearest cities information: {response.status_code}"")
        return None

def get_timezone(latitude, longitude):
    url = ""https://geocodeapi.p.rapidapi.com/GetTimezone""
    querystring = {""latitude"": str(latitude), ""longitude"": str(longitude)}
    response = requests.get(url, headers=HEADERS, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching timezone information: {response.status_code}"")
        return None

if __name__ == '__main__':
    # é»˜è®¤åæ ‡ï¼ˆçº½çº¦å¸‚ä¸­å¿ƒï¼‰
    latitude = 40.7128
    longitude = -74.0060

    # æ£€æŸ¥è¯¥ä½ç½®æ˜¯å¦åœ¨æ°´ä¸Š
    water_check = is_on_water(latitude, longitude)
    if water_check and 'isOnWater' in water_check:
        if water_check['isOnWater']:
            water_type = ""sea"" if water_check.get('sea') else ""lake"" if water_check.get('lake') else ""other body of water""
            print(f""The location is on a {water_type}."")
        else:
            print(""The location is on land."")
    else:
        print(""Unable to determine if the location is on water."")

    # èŽ·å–æœ€è¿‘åŸŽå¸‚ä¿¡æ¯
    nearest_cities_info = get_nearest_cities(latitude, longitude, range_meters=50000)  # 50 å…¬é‡ŒåŠå¾„
    if nearest_cities_info:
        print(""\nNearest cities within 50 km:"")
        for city in nearest_cities_info:
            print(f""- {city.get('name', 'Unknown City')} ({city.get('country', 'N/A')}), Distance: {city.get('distance', 'N/A')} meters"")
    else:
        print(""No nearest cities information available."")

    # èŽ·å–è¯¥ä½ç½®çš„æ—¶åŒºä¿¡æ¯
    timezone_info = get_timezone(latitude, longitude)
    if timezone_info:
        print(""\nTimezone information:"")
        print(f""Time Zone ID: {timezone_info.get('TimezoneId', 'N/A')}"")
        print(f""Local Time Now: {timezone_info.get('LocalTime_Now', 'N/A')}"")
    else:
        print(""No timezone information available."")
",Fail,-1,-1,31,109709
"I enter the geographic coordinates of multiple locations, and the system determines if they're in the same time zone and provides specific time zone information for each location to plan events or communications across regions.","import requests

# API Key and Headers
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""geocodeapi.p.rapidapi.com""
}

def get_timezone(latitude, longitude):
    url = ""https://geocodeapi.p.rapidapi.com/GetTimezone""
    querystring = {""latitude"": latitude, ""longitude"": longitude}
    response = requests.get(url, headers=HEADERS, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching timezone for ({latitude}, {longitude}): {response.status_code}"")
        return None

def check_timezones(locations):
    # èŽ·å–æ¯ä¸ªä½ç½®çš„æ—¶åŒºæ•°æ®
    timezone_data = []
    for lat, lon in locations:
        tz_info = get_timezone(lat, lon)
        if tz_info:
            timezone_data.append({
                ""latitude"": lat,
                ""longitude"": lon,
                ""TimeZoneId"": tz_info.get(""TimeZoneId"", ""N/A""),
                ""LocalTime_Now"": tz_info.get(""LocalTime_Now"", ""N/A""),
                ""GMT_offset"": tz_info.get(""GMT_offset"", ""N/A"")
            })

    # èŽ·å–å”¯ä¸€çš„æ—¶åŒº ID
    unique_timezones = set([tz[""TimeZoneId""] for tz in timezone_data])

    if len(unique_timezones) == 1:
        print(""All locations are in the same time zone:"", timezone_data[0][""TimeZoneId""])
    else:
        print(""Locations are in different time zones:"")
        for tz in timezone_data:
            print(f""Location ({tz['latitude']}, {tz['longitude']}):"")
            print(f"" - Time Zone: {tz['TimeZoneId']}"")
            print(f"" - Local Time: {tz['LocalTime_Now']}"")
            print(f"" - GMT Offset: {tz['GMT_offset']}"")
            print()

if __name__ == '__main__':
    # ç¤ºä¾‹ä½ç½®çš„åæ ‡
    locations = [
        (40.712776, -74.005974),  # New York, USA
        (34.052235, -118.243683), # Los Angeles, USA
        (51.507351, -0.127758)    # London, UK
    ]
    check_timezones(locations)
","Locations are in different time zones:
Location (40.712776, -74.005974):
 - Time Zone: America/New_York
 - Local Time: 10:02:11 AM
 - GMT Offset: -5

Location (34.052235, -118.243683):
 - Time Zone: America/Los_Angeles
 - Local Time: 7:02:12 AM
 - GMT Offset: -8

Location (51.507351, -0.127758):
 - Time Zone: Europe/London
 - Local Time: 3:02:13 PM
 - GMT Offset: 0",1,1,15,46815
"I enter two addresses, convert them to geographic coordinates, and calculate the straight-line distance between them for distance measurement and navigation.

","import requests

# API Key and Headers
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""forward-reverse-geocoding.p.rapidapi.com""
}

def forward_geocode(address):
    url = ""https://forward-reverse-geocoding.p.rapidapi.com/v1/forward""
    querystring = {
        ""format"": ""json"",
        ""street"": address.get(""street"", """"),
        ""city"": address.get(""city"", """"),
        ""state"": address.get(""state"", """"),
        ""postalcode"": address.get(""postalcode"", """"),
        ""country"": address.get(""country"", """"),
        ""addressdetails"": ""1"",
        ""limit"": ""1""
    }
    response = requests.get(url, headers=HEADERS, params=querystring)

    if response.status_code == 200 and response.json():
        data = response.json()[0]
        return float(data[""lat""]), float(data[""lon""])
    else:
        print(f""Error fetching coordinates for address: {address}"")
        return None, None

def get_distance(lat1, lon1, lat2, lon2):
    url = ""https://geocodeapi.p.rapidapi.com/GetDistance""
    querystring = {
        ""lat1"": str(lat1),
        ""lon1"": str(lon1),
        ""lat2"": str(lat2),
        ""lon2"": str(lon2)
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""geocodeapi.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error calculating distance: {response.status_code}"")
        return None

if __name__ == '__main__':
    # å®šä¹‰ä¸¤ä¸ªåœ°å€
    address1 = {
        ""street"": ""34 West 13th Street"",
        ""city"": ""New York City"",
        ""state"": ""NY"",
        ""postalcode"": ""10011"",
        ""country"": ""USA""
    }
    address2 = {
        ""street"": ""1600 Amphitheatre Parkway"",
        ""city"": ""Mountain View"",
        ""state"": ""CA"",
        ""postalcode"": ""94043"",
        ""country"": ""USA""
    }

    # èŽ·å–ç¬¬ä¸€ä¸ªåœ°å€çš„åæ ‡
    lat1, lon1 = forward_geocode(address1)
    # èŽ·å–ç¬¬äºŒä¸ªåœ°å€çš„åæ ‡
    lat2, lon2 = forward_geocode(address2)

    if lat1 is not None and lat2 is not None:
        # è®¡ç®—ä¸¤ç‚¹ä¹‹é—´çš„è·ç¦»
        distance_info = get_distance(lat1, lon1, lat2, lon2)

        # æå–å¹¶æ˜¾ç¤ºå¤šç§è·ç¦»å•ä½
        if distance_info and 'Distance' in distance_info:
            print(f""Distance between the addresses:"")
            print(f""- Meters: {distance_info['Distance']} meters"")
            print(f""- Kilometers: {distance_info['DistanceInKm']} km"")
            print(f""- Miles: {distance_info['DistanceInMiles']} miles"")
        else:
            print(""Failed to retrieve distance information or 'Distance' key not found in response."")
    else:
        print(""Unable to retrieve coordinates for one or both addresses."")
","Distance between the addresses:
- Meters: 4118914.760087681 meters
- Kilometers: 4118.914760087681 km
- Miles: 2559.3749751996347 miles",1,1,14,43806
"I hope that by entering a keyword or song name, the system will return ten related songs and provide similar song recommendations to expand the music playlist.

","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def search_song(query, limit=10):
    url = ""https://shazam-api7.p.rapidapi.com/search""
    querystring = {""term"": query, ""limit"": str(limit)}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""shazam-api7.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)

    # Debugging output to check full API response
    data = response.json()
    print(""Debug: Full API Response for search:"", data)

    if response.status_code == 200:
        return data
    else:
        print(f""Error searching song: {response.status_code}"")
        return None

if __name__ == '__main__':
    # Use default song name ""Blinding Lights"" for testing
    song_query = ""Blinding Lights""
    search_results = search_song(song_query)

    # Verify and parse the results
    if search_results and 'tracks' in search_results['data'] and 'hits' in search_results['data']['tracks']:
        for idx, track in enumerate(search_results['data']['tracks']['hits'][:10], start=1):
            title = track['heading']['title']
            artist = track['heading']['subtitle']
            print(f""{idx}. {title} by {artist}"")
    else:
        print(""No songs found for the provided query."")
",Fail,-1,-1,31,85746
"I want to know the details of an album and see all the tracks in the album and their playback data (e.g. number of plays, duration, etc.).

","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def get_album_details(album_id):
    """"""Retrieve and format album details.""""""
    url = ""https://spotify23.p.rapidapi.com/albums/""
    querystring = {""ids"": album_id}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""spotify23.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    data = response.json()

    album_data = data['albums'][0]
    name = album_data['name']
    artists = "", "".join([artist['name'] for artist in album_data['artists']])
    release_date = album_data['release_date']
    total_tracks = album_data['total_tracks']
    label = album_data['label']

    print(f""Album Name: {name}"")
    print(f""Artist(s): {artists}"")
    print(f""Release Date: {release_date}"")
    print(f""Total Tracks: {total_tracks}"")
    print(f""Label: {label}"")
    print()


def get_album_tracks(album_id):
    """"""Retrieve and format track details for a specific album.""""""
    url = ""https://spotify23.p.rapidapi.com/album_tracks/""
    querystring = {""id"": album_id, ""offset"": ""0"", ""limit"": ""50""}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""spotify23.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    data = response.json()

    # æ£€æŸ¥ 'items' é”®æ˜¯å¦å­˜åœ¨äºŽæ•°æ®ä¸­
    if 'items' not in data:
        print(""No track data available for this album."")
        return

    print(""Track List:"")
    for index, track in enumerate(data['items'], start=1):
        track_name = track.get('name', 'Unknown Track')
        duration_ms = track.get('duration_ms', 0)
        duration_sec = duration_ms // 1000
        print(f""{index}. {track_name} - Duration: {duration_sec} seconds"")
    print()


def get_album_metadata(album_id):
    """"""Retrieve and format metadata for a specific album.""""""
    url = ""https://spotify23.p.rapidapi.com/album_metadata/""
    querystring = {""id"": album_id}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""spotify23.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    data = response.json()

    album_data = data['data']['album']
    cover_art_url = album_data['coverArt']['sources'][0]['url']
    spotify_url = album_data['sharingInfo']['shareUrl']

    print(""Additional Metadata:"")
    print(f""Cover Art URL: {cover_art_url}"")
    print(f""Spotify Link: {spotify_url}"")
    print()


if __name__ == '__main__':
    album_id = ""3IBcauSj5M2A6lTeffJzdv""  # example album ID
    get_album_details(album_id)
    get_album_tracks(album_id)
    get_album_metadata(album_id)
","Album Name: Kan
Artist(s): UZI
Release Date: 2021-03-19
Total Tracks: 10
Label: M.O.B. Entertainment

No track data available for this album.
Additional Metadata:
Cover Art URL: https://i.scdn.co/image/ab67616d00001e0267c738a703dc979f5c3c52ef
Spotify Link: https://open.spotify.com/album/3IBcauSj5M2A6lTeffJzdv?si=OCZfOkEpRp6sx5C7sin6uQ",1,1,11,36575
"I want to get a list of news by topic and click on the first one to see the full content of the article, the system needs to display news by popularity and provide detailed pages for different topics.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

# Fetch trending news topics
def get_news_topics():
    url = ""https://biztoc.p.rapidapi.com/news/topics""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""biztoc.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        return response.json()
    else:
        print(""Error fetching news topics:"", response.status_code)
        return []

# Get detailed articles sorted by popularity/heat
def get_articles_by_topic(topic, page_num=1, limit=10):
    url = f""https://climate-news-feed.p.rapidapi.com/page/{page_num}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""climate-news-feed.p.rapidapi.com""
    }
    querystring = {""limit"": str(limit), ""topic"": topic}
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        try:
            articles = response.json()
            if isinstance(articles, list):
                return sorted(articles, key=lambda x: x.get('heat', 0), reverse=True)  # Sort articles by 'heat' (popularity)
            else:
                print(""Unexpected response format: expected a list of articles."")
                return []
        except json.JSONDecodeError:
            print(""Error decoding JSON response."")
            return []
    else:
        print(""Error fetching articles:"", response.status_code)
        return []

# View the complete content of an article
def get_article_details(article_id):
    url = f""https://climate-news-feed.p.rapidapi.com/articles/{article_id}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""climate-news-feed.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        article = response.json()
        if 'title' in article and 'body' in article:
            return {
                'title': article['title'],
                'body': article['body']
            }
        else:
            print(""Unexpected response format: missing 'title' or 'body'."")
            return {}
    else:
        print(""Error fetching article details:"", response.status_code)
        return {}

if __name__ == '__main__':
    # Step 1: Fetch trending news topics
    news_topics = get_news_topics()
    if news_topics:
        print(""Trending News Topics:"")
        for idx, topic in enumerate(news_topics):
            print(f""{idx + 1}. {topic}"")

        # Step 2: Choose a topic to fetch articles
        selected_topic = news_topics[0]  # For simplicity, we select the first topic
        print(f""\nFetching articles for topic: {selected_topic}\n"")
        articles = get_articles_by_topic(selected_topic)

        if articles:
            for idx, article in enumerate(articles):
                print(f""{idx + 1}. {article['title']} (Heat: {article.get('heat', 'N/A')})"")

            # Step 3: Click on the first article to view details
            first_article_id = articles[0]['id']
            article_details = get_article_details(first_article_id)
            if article_details:
                print(""\nFirst Article Details:"")
                print(f""Title: {article_details['title']}\n"")
                print(f""Body: {article_details['body']}\n"")
        else:
            print(""No articles found for the selected topic."")
    else:
        print(""No trending topics found."")",Fail,-1,-1,27,77247
"When I want to access environmental and climate-related news stories, I need to display the title, URL, publication date, and media source of the news so that I can quickly understand what is relevant to it.

","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
API_HOST = ""climate-news-feed.p.rapidapi.com""

headers = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": API_HOST
}

def get_climate_articles(limit=10, source=None, exclude=None):
    """"""
    Fetch climate-related news articles with specified options.
    Args:
        limit (int): The number of articles to retrieve.
        source (str): The specific source to filter articles from.
        exclude (str): A term to exclude from the results.
    Returns:
        None: Prints article details such as title, URL, published date, and source.
    """"""
    url = f""https://{API_HOST}/""
    querystring = {""limit"": str(limit)}

    if source:
        querystring[""source""] = source
    if exclude:
        querystring[""exclude""] = exclude

    try:
        response = requests.get(url, headers=headers, params=querystring)
        response.raise_for_status()
        articles = response.json().get(""articles"", [])

        if articles:
            for article in articles:
                print(f""Title: {article.get('title')}"")
                print(f""URL: {article.get('url')}"")
                print(f""Published: {article.get('published')}"")
                print(f""Source: {article.get('source')}\n"")
        else:
            print(""No articles found."")

    except requests.exceptions.RequestException as e:
        print(f""Error fetching articles: {e}"")

if __name__ == '__main__':
    # Example call to the optimized function
    print(""Fetching climate-related articles:"")
    get_climate_articles(limit=5, source=""Nasa Climate"")
","Fetching climate-related articles:
Title: Google's â€˜A Passage of Water' Brings NASA's Water Data to Life
URL: https://climate.nasa.gov/news/3293/
Published: 2023-11-30T17:50:23.000Z
Source: Nasa Climate

Title: NASA to Showcase Earth Science Data at COP28
URL: https://climate.nasa.gov/news/3292/
Published: 2023-11-28T00:05:44.000Z
Source: Nasa Climate

Title: NASA Analysis Finds Strong El NiÃ±o Could Bring Extra Floods This Winter
URL: https://climate.nasa.gov/news/3291/
Published: 2023-11-08T19:46:30.000Z
Source: Nasa Climate

Title: NASA Flights Link Methane Plumes to Tundra Fires in Western Alaska
URL: https://climate.nasa.gov/news/3289/
Published: 2023-11-02T15:57:00.000Z
Source: Nasa Climate

Title: See SWOT Mission's Unprecedented View of Global Sea Levels
URL: https://climate.nasa.gov/news/3288/
Published: 2023-10-30T16:15:38.000Z
Source: Nasa Climate",1,1,17,45951
"I want to see the main news feeds for three countries and regions and get the latest news from each source, and the system needs to show a list of the latest news by source or country.

","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def fetch_data(url, headers, params=None):
    try:
        response = requests.get(url, headers=headers, params=params)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f""Error fetching data from {url}: {e}"")
        return None


def get_sources():
    url = ""https://biztoc.p.rapidapi.com/sources""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""biztoc.p.rapidapi.com""
    }
    return fetch_data(url, headers)


def get_articles_by_source(source_id, limit=1):
    url = ""https://climate-news-feed.p.rapidapi.com/""
    querystring = {""source"": source_id, ""limit"": str(limit)}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""climate-news-feed.p.rapidapi.com""
    }
    return fetch_data(url, headers, params=querystring)


def get_latest_news(lang):
    url = ""https://bbc-api2.p.rapidapi.com/latest""
    querystring = {""lang"": lang}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""bbc-api2.p.rapidapi.com""
    }
    return fetch_data(url, headers, params=querystring)


def get_news_by_country(country_list):
    # Step 1: Get sources
    sources = get_sources()
    if not sources:
        print(""Failed to fetch sources."")
        return

    country_news = {}
    for country in country_list:
        country_news[country] = []
        for source in sources:
            if source.get('country') == country:
                # Step 2: Get latest articles from each source
                articles = get_articles_by_source(source_id=source['id'], limit=1)
                if articles and 'articles' in articles:
                    country_news[country].extend(articles['articles'])

    # Step 3: Display news grouped by country
    for country, articles in country_news.items():
        print(f""\nLatest news from {country}:"")
        if articles:
            for article in articles:
                print(f""- {article['title']} (URL: {article['url']}, Published: {article['published']})"")
        else:
            print(""No articles available."")

    if not any(country_news[country] for country in country_list):
        print(""\nNo news available for any of the specified countries."")


def main():
    # Define the list of countries you want to get news for
    country_list = [""USA"", ""UK"", ""Australia""]

    # Get news by country
    get_news_by_country(country_list)

    # Step 4: Get global latest news in English
    latest_news = get_latest_news(lang=""en"")
    if latest_news and 'Latest' in latest_news:
        print(""\nGlobal Latest News:"")
        for news in latest_news.get('Latest', []):
            print(f""News Title: {news['title']}, Summary: {news['summary']}, URL: {news['news_link']}"")
    else:
        print(""\nFailed to fetch global latest news or no news available."")


if __name__ == '__main__':
    main()
","Latest news from USA:
No articles available.

Latest news from UK:
No articles available.

Latest news from Australia:
No articles available.

No news available for any of the specified countries.
Error fetching data from https://bbc-api2.p.rapidapi.com/latest: 400 Client Error: Bad Request for url: https://bbc-api2.p.rapidapi.com/latest?lang=en

Failed to fetch global latest news or no news available.",1,1,18,57060
"I would like to perform lexical and structural analysis on three specific texts to obtain information such as unique lexical statistics, chapter counts, and word counts to better understand the complexity and style of the text.

","import requests
from collections import Counter
import re

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""al-quran1.p.rapidapi.com""
}

def get_base_information():
    """"""Retrieve general information about the text for overview analysis.""""""
    url = ""https://al-quran1.p.rapidapi.com/""
    response = requests.get(url, headers=HEADERS)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Failed to retrieve base information: {response.status_code}"")
        return {}

def get_entire_surah(chapter_id):
    """"""Retrieve the text of a specific surah.""""""
    url = f""https://al-quran1.p.rapidapi.com/{chapter_id}""
    response = requests.get(url, headers=HEADERS)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Failed to retrieve surah information: {response.status_code}"")
        return {}

def search_for_word(search_term):
    """"""Search for occurrences of a specific word to analyze its usage and context.""""""
    url = f""https://al-quran1.p.rapidapi.com/corpus/{search_term}""
    response = requests.get(url, headers=HEADERS)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Failed to search for word '{search_term}': {response.status_code}"")
        return {}

def analyze_text(text):
    """"""Perform lexical and structural analysis of the text.""""""
    words = re.findall(r'\b\w+\b', text.lower())
    unique_words = set(words)
    word_count = len(words)
    unique_word_count = len(unique_words)
    word_freq = Counter(words)

    print(f""Total Words: {word_count}"")
    print(f""Unique Words: {unique_word_count}"")
    print(""Top 10 Most Frequent Words:"", word_freq.most_common(10))

if __name__ == '__main__':
    # Step 1: Retrieve base information for lexical and structural analysis
    base_info = get_base_information()
    if base_info:
        print(""Base Information Retrieved."")

    # Step 2: Retrieve specific surah details for analysis
    chapter_id = 1  # Example Surah (Al-Fatihah)
    surah_info = get_entire_surah(chapter_id)

    # Check if surah_info has 'data' and 'ayahs' keys
    if 'data' in surah_info and 'ayahs' in surah_info['data']:
        surah_text = ' '.join([ayah['text'] for ayah in surah_info['data']['ayahs']])
        analyze_text(surah_text)
    else:
        print(""Surah data not in expected format or missing keys."")

    # Step 3: Search for a specific word in the text to analyze usage and context
    search_term = ""mercy""  # Example search term
    search_results = search_for_word(search_term)
    if search_results:
        print(""Search Results:"", search_results)
",Fail,-1,-1,27,89964
"I would like to inquire about five types of information on biodiversity and specific plants, including their environment, ecological characteristics and conservation status, for ecological conservation or scientific research.

","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
BASE_URL = ""https://daily-knowledge.p.rapidapi.com/""

HEADERS = {
    ""x-rapidapi-key"": API_KEY,
    ""x-rapidapi-host"": ""daily-knowledge.p.rapidapi.com""
}

def fetch_data(endpoint):
    """"""Fetch data from a specified endpoint.""""""
    url = BASE_URL + endpoint
    response = requests.get(url, headers=HEADERS)
    if response.status_code == 200:
        print(f""Raw response from {endpoint}:"", response.json())  # Debug: Print raw response data
        return response.json()
    else:
        print(f""Failed to retrieve data: {response.status_code}"")
        return None

def display_biodiversity_info():
    """"""Display biodiversity and conservation information.""""""
    data = fetch_data(""sustainable-development-goals-en.json"")
    if isinstance(data, dict):  # Verify if data is a dictionary
        print(""\nBiodiversity Goals and Conservation Information:"")
        print(f""Goal: {data.get('goal', 'Unknown Goal')}"")
        print(f""Description: {data.get('description', 'No description available')}"")
    else:
        print(""Data format is incorrect or no biodiversity data available."")

def display_plant_info():
    """"""Display ecological and growth details of specific plants.""""""
    data = fetch_data(""plants-de.json"")
    if isinstance(data, dict):  # Verify if data is a dictionary
        print(""\nPlant Ecological and Growth Details:"")
        name = data.get(""name"", ""Unknown Plant"")
        habitat = data.get(""descrData"", {}).get(""infoGrow"", ""Habitat details not available"")
        description = data.get(""descrData"", {}).get(""infoShort"", ""Description not available"")

        print(f""Name: {name}"")
        print(f""Habitat: {habitat}"")
        print(f""Description: {description}"")
    else:
        print(""Data format is incorrect or no plant data available."")

def display_country_info():
    """"""Display country context information for biodiversity.""""""
    data = fetch_data(""countries-en.json"")
    if isinstance(data, dict):  # Verify if data is a dictionary
        print(""\nCountry Context for Biodiversity:"")
        country_name = data.get(""name"", ""Unknown Country"")
        capital = data.get(""capital"", ""Unknown Capital"")
        language = data.get(""languages"", ""Language information not available"")

        print(f""Country: {country_name}"")
        print(f""Capital: {capital}"")
        print(f""Languages: {language}"")
    else:
        print(""Data format is incorrect or no country data available."")

if __name__ == '__main__':
    # Step 1: Get information on biodiversity and conservation
    display_biodiversity_info()

    # Step 2: Query specific plant details
    display_plant_info()

    # Step 3: (Optional) Get geographical and country context if needed
    display_country_info()
",Fail,-1,-1,32,116224
"I want to know about five different logical fallacies (such as the ""either/or"" fallacy), including the definition, examples, and background of each fallacy in order to better understand common mistakes in logical reasoning.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def logical_fallacies():
    """"""Retrieve details about a specific logical fallacy.""""""
    url = ""https://daily-knowledge.p.rapidapi.com/logical-fallacies-en.json""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""daily-knowledge.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        return response.json()
    else:
        print(""Failed to retrieve logical fallacies:"", response.status_code)
        return None

def display_fallacy(fallacy_data):
    """"""Display details of a logical fallacy.""""""
    if not fallacy_data:
        print(""No data available for logical fallacies."")
        return

    # Display information for a single fallacy
    name = fallacy_data.get(""name"", ""Unknown Fallacy"")
    definition = fallacy_data.get(""head"", ""No definition available."")
    example = fallacy_data.get(""example"", ""No example available."")
    background = fallacy_data.get(""description"", ""No background information available."")

    print(f""Fallacy Name: {name}"")
    print(f""Definition: {definition}"")
    print(f""Example: {example}"")
    print(f""Background: {background}"")

if __name__ == '__main__':
    # Retrieve and display details of a logical fallacy
    fallacy_data = logical_fallacies()

    if fallacy_data:
        display_fallacy(fallacy_data)
    else:
        print(""Could not retrieve logical fallacy information."")
","Fallacy Name: ambiguity
Definition: You used a double meaning or ambiguity of language to mislead or misrepresent the truth.
Example: When the judge asked the defendant why he hadn't paid his parking fines, he said that he shouldn't have to pay them because the sign said 'Fine for parking here' and so he naturally presumed that it would be fine to park there.
Background: Politicians are often guilty of using ambiguity to mislead and will later point to how they were technically not outright lying if they come under scrutiny. The reason that it qualifies as a fallacy is that it is intrinsically&nbsp;misleading.
",1,1,18,47628
"I need to retrieve rentable properties in a specific location and allow filtering based on criteria such as price, number of rooms and whether they are furnished or not. Be able to access detailed information about each property, including price, location and other features. In addition, information about the relevant agents managing each property is displayed, including agent name and location. You also need to be able to handle location information correctly, whether it's a single object or a list.","import requests

# Define API Key
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


# Function to retrieve property list based on given filters
def properties_list(location_external_ids, purpose=""for-rent"", min_price=None, max_price=None, min_rooms=None,
                    max_rooms=None, furnished=None):
    url = ""https://bayut.p.rapidapi.com/properties/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""bayut.p.rapidapi.com""
    }
    querystring = {
        ""locationExternalIDs"": location_external_ids,
        ""purpose"": purpose,
        ""hitsPerPage"": ""10"",
        ""page"": ""0"",
        ""lang"": ""en"",
        ""sort"": ""price-asc""
    }

    # Apply optional filters
    if min_price is not None:
        querystring[""priceMin""] = str(min_price)
    if max_price is not None:
        querystring[""priceMax""] = str(max_price)
    if min_rooms is not None:
        querystring[""roomsMin""] = str(min_rooms)
    if max_rooms is not None:
        querystring[""roomsMax""] = str(max_rooms)
    if furnished is not None:
        querystring[""furnishingStatus""] = ""furnished"" if furnished else ""unfurnished""

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


# Function to retrieve detailed information for a specific property
def properties_detail(external_id):
    url = ""https://bayut.p.rapidapi.com/properties/detail""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""bayut.p.rapidapi.com""
    }
    querystring = {
        ""externalID"": external_id
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


# Function to retrieve agency details based on a query
def agencies_list(agency_query=None):
    url = ""https://bayut.p.rapidapi.com/agencies/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""bayut.p.rapidapi.com""
    }
    querystring = {
        ""hitsPerPage"": ""10"",
        ""page"": ""0"",
        ""lang"": ""en""
    }

    if agency_query:
        querystring[""query""] = agency_query

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


# Main code to demonstrate the functionality
if __name__ == '__main__':
    # Retrieve properties based on specific filters
    properties = properties_list(
        location_external_ids=""5002,6020"",  # Example location IDs
        min_price=50000,
        max_price=150000,
        min_rooms=2,
        furnished=True
    )

    # Iterate through the retrieved properties and get detailed information
    for property_item in properties.get('hits', []):
        external_id = property_item.get('externalID')
        if external_id:
            # Retrieve detailed information about the property
            property_details = properties_detail(external_id)

            # Display property details with corrected location handling
            print(f""\nProperty Title: {property_item.get('title')}"")
            print(f""Property Price: {property_item.get('price')}"")

            # Handle 'location' field correctly if it's a list
            location_data = property_item.get('location', [])
            if isinstance(location_data, list):
                # Join all location names if it's a list of location objects
                location_names = ', '.join([loc.get('name', '') for loc in location_data if loc.get('name')])
                print(f""Property Location: {location_names}"")
            else:
                # If it's not a list, handle as a dictionary
                print(f""Property Location: {location_data.get('name')}"")
","Property Title: Burj + Fountain View I State of Art Furnishing
Property Price: 50000.0
Property Location: UAE, Dubai, Downtown Dubai, Opera Grand

Property Title: Exquisite 2BR Luxury Retreat at Fairmont North, Palm Jumeirah â€“ Ground Floor Elegance
Property Price: 50000.0
Property Location: UAE, Dubai, Palm Jumeirah, The Fairmont Palm Residences, The Fairmont Palm Residence North

Property Title: Luxury Unit in Bluewater | Full Eye View
Property Price: 50000.0
Property Location: UAE, Dubai, Bluewaters Island, Bluewaters Residences, Bluewaters Residences 9

Property Title: Exclusive 4BR Full Burj & Fountain view - Downtown
Property Price: 50000.0
Property Location: UAE, Dubai, Downtown Dubai, Burj Royale

Property Title: Splendid Views | Hotel Facilities | Beach Access
Property Price: 50000.0
Property Location: UAE, Dubai, Jumeirah Beach Residence (JBR), The Address Residences Jumeirah Resort and Spa, Jumeirah Gate Tower 1

Property Title: Fairmont North | Sea View Newly Renovated 2BR | Huge Terrace
Property Price: 50000.0
Property Location: UAE, Dubai, Palm Jumeirah, The Fairmont Palm Residences, The Fairmont Palm Residence North

Property Title: Livbnb Suite - Luxury 3+1 with Burj View, Downtown
Property Price: 50000.0
Property Location: UAE, Dubai, Downtown Dubai, Vida Residence Downtown

Property Title: Utility Bills Included  | Sophisticated 4-Bedroom Villa with Private Garden in Maple 1
Property Price: 50000.0
Property Location: UAE, Dubai, Dubai Hills Estate, Maple, Maple 1

Property Title: Sunrise Bay Suite - 3 + 1 w/ Marina Skyline View Private Beach by Livbnb
Property Price: 50000.0
Property Location: UAE, Dubai, Dubai Harbour, Emaar Beachfront, Sunrise Bay

Property Title: Modern Luxurious 2 Bed Apartment | Bills Included
Property Price: 50000.0
Property Location: UAE, Dubai, Dubai Harbour, Emaar Beachfront, Beach Vista, Beach Vista 1
",1,1,12,35772
"I want to explore geo-location through microsite search and provincial filtering. Once a location is selected, you can search for a list of rooms for rent or commercial properties available in that location. I wanted to filter the room listings based on criteria such as relevance, price and availability, while also being able to browse detailed commercial property information. It should support different languages and allow sorting according to my preferences.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_microsite_locations(microsite_short_name, location_code, locale, operation, location_id):
    url = ""https://idealista7.p.rapidapi.com/getmicrositelocations""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""idealista7.p.rapidapi.com""
    }
    querystring = {
        ""micrositeShortName"": microsite_short_name,
        ""location"": location_code,
        ""locale"": locale,
        ""operation"": operation,
        ""locationId"": location_id
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def list_rooms(order=""relevance"", location_id=None, location_name=None, num_page=1, max_items=10, location=""es"", locale=""en""):
    url = ""https://idealista7.p.rapidapi.com/listrooms""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""idealista7.p.rapidapi.com""
    }
    querystring = {
        ""order"": order,
        ""locationId"": location_id,
        ""locationName"": location_name,
        ""numPage"": str(num_page),
        ""maxItems"": str(max_items),
        ""location"": location,
        ""locale"": locale
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def list_commercial_properties(order=""relevance"", operation=""sale"", location_id=None, max_items=""10"", location_name=None, num_page=1, location=""es"", locale=""en""):
    url = ""https://idealista7.p.rapidapi.com/listcommercialproperties""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""idealista7.p.rapidapi.com""
    }
    querystring = {
        ""order"": order,
        ""operation"": operation,
        ""locationId"": location_id,
        ""maxItems"": str(max_items),
        ""locationName"": location_name,
        ""numPage"": str(num_page),
        ""location"": location,
        ""locale"": locale
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

if __name__ == '__main__':
    # Step 1: Retrieve locations from a specific microsite
    microsite_data = get_microsite_locations(
        microsite_short_name=""sumainmobiliaria"",
        location_code=""es"",
        locale=""en"",
        operation=""sale"",
        location_id=""0-EU-ES-28""
    )

    # Display provinces and municipalities
    if 'provinces' in microsite_data:
        print(""Available Provinces:"")
        for province in microsite_data['provinces']:
            print(f""- {province['locationName']} (ID: {province['locationId']})"")

    if 'municipalities' in microsite_data:
        print(""Available Municipalities:"")
        for municipality in microsite_data['municipalities']:
            print(f""- {municipality['locationName']} (ID: {municipality['locationId']})"")

    # Step 2: List rooms available in a chosen location (e.g., Madrid)
    room_listings = list_rooms(
        order=""price"",
        location_id=""0-EU-ES-28-07-001-079"",  # Example location ID for Madrid
        location_name=""Madrid"",
        num_page=1,
        max_items=5,
        locale=""en""
    )

    print(""Available Room Listings in Madrid:"")
    for idx, room in enumerate(room_listings.get('elementList', []), start=1):
        print(f""{idx}. {room.get('suggestedTexts', {}).get('title', 'No Title')} - Price: {room.get('price')}"")

    # Step 3: List commercial properties in the same location
    commercial_properties = list_commercial_properties(
        order=""relevance"",
        operation=""sale"",
        location_id=""0-EU-ES-28-07-001-079"",
        max_items=""5"",
        location_name=""Madrid"",
        locale=""en""
    )

    print(""Available Commercial Properties in Madrid:"")
    for idx, property_item in enumerate(commercial_properties.get('elementList', []), start=1):
        print(f""{idx}. {property_item.get('suggestedTexts', {}).get('title', 'No Title')} - Price: {property_item.get('price')}"")
","Available Provinces:
- Madrid (ID: 0-EU-ES-28)
Available Municipalities:
- Ciudalcampo, Madrid (ID: 0-EU-ES-28-01-008-504)
- La Moraleja, Madrid (ID: 0-EU-ES-28-01-007-502)
- Madrid, Madrid (ID: 0-EU-ES-28-07-001-079)
Available Room Listings in Madrid:
1. Room in calle ConcepciÃ³n de la Oliva, 12 - Price: 380.0
2. Room in calle de RÃ­os Rosas, 51 - Price: 630.0
3. Room in Braile, 20 - Price: 450.0
4. Room in aligustre, 8 - Price: 730.0
5. Room in calle JosÃ© Silva, 3 - Price: 610.0
Available Commercial Properties in Madrid:
1. Commercial premises in paseo de La Habana - Price: 3440000.0
2. Commercial premises in calle de Fortuny - Price: 700000.0
3. Commercial premises in calle de Agastia - Price: 5950000.0
4. Commercial premises in calle de Ferraz - Price: 850000.0
5. Commercial premises in paseo de San Francisco de Sales, 27 - Price: 675000.0
",1,1,8,26512
"I retrieve listings of properties for sale in the city of Oxford, Oxfordshire, get details about estate agents in the Oxford area, and filter properties according to various criteria such as property type and price range. Be able to explore detailed property characteristics, including images, agent details and geographical location, while allowing sorting by criteria such as current listings or relevance.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_properties_v2_list(location_value, location_identifier, furnished_state=""Any"", sort_order=""newest_listings"", page=1):
    """"""
    Retrieve a detailed list of residential properties for sale in Oxford, Oxfordshire.
    """"""
    url = ""https://zoopla.p.rapidapi.com/properties/v2/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""zoopla.p.rapidapi.com""
    }
    querystring = {
        ""locationValue"": location_value,
        ""locationIdentifier"": location_identifier,
        ""furnishedState"": furnished_state,
        ""sortOrder"": sort_order,
        ""page"": str(page)
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_agents_list(location, search_type=""estate-agents"", agents_sort=""a_z"", page_number=1):
    """"""
    Retrieve detailed information about real estate agents in Oxford, including statistics and contact details.
    """"""
    url = ""https://zoopla.p.rapidapi.com/agents/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""zoopla.p.rapidapi.com""
    }
    querystring = {
        ""location"": location,
        ""search_type"": search_type,
        ""agents_sort"": agents_sort,
        ""pn"": str(page_number)
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_properties_list(search_location, search_location_subtext, region_type, channel=""buy"", sort_type=""relevance"", page=1, page_size=10, minimum_price=None, maximum_price=None):
    """"""
    Retrieve a list of properties based on filters like location, type, and price range.
    """"""
    url = ""https://realty-in-au.p.rapidapi.com/properties/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""realty-in-au.p.rapidapi.com""
    }
    querystring = {
        ""page"": str(page),
        ""pageSize"": str(page_size),
        ""sortType"": sort_type,
        ""channel"": channel,
        ""searchLocation"": search_location,
        ""searchLocationSubtext"": search_location_subtext,
        ""type"": region_type
    }

    # Optional price filters
    if minimum_price:
        querystring[""minimumPrice""] = str(minimum_price)
    if maximum_price:
        querystring[""maximumPrice""] = str(maximum_price)

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

if __name__ == '__main__':
    # Step 1: Retrieve property listings in Oxford, Oxfordshire
    properties_data = get_properties_v2_list(
        location_value=""Oxford, Oxfordshire"",
        location_identifier=""oxford"",
        furnished_state=""Any"",
        sort_order=""newest_listings"",
        page=1
    )

    # Display basic details of property listings
    if properties_data['success']:
        print(""Property Listings in Oxford, Oxfordshire:"")
        for idx, property_item in enumerate(properties_data['data']['listings']['regular'], start=1):
            print(f""{idx}. {property_item['title']} - Price: Â£{property_item['pricing']['value']} at {property_item['address']}"")

    # Step 2: Retrieve information about real estate agents in Oxford
    agents_data = get_agents_list(
        location=""oxford"",
        search_type=""estate-agents"",
        agents_sort=""a_z"",
        page_number=1
    )

    # Display agent details
    print(""\nReal Estate Agents in Oxford:"")
    for idx, agent in enumerate(agents_data['pageProps']['data']['agents']['results'], start=1):
        print(f""{idx}. {agent['displayName']} - Contact: {agent['contactNumber']} - Address: {agent['displayAddress']}"")

    # Step 3: Retrieve properties based on filters (example: buy properties in Oxford with price filter)
    filtered_properties_data = get_properties_list(
        search_location=""Oxford"",
        search_location_subtext=""Oxfordshire"",
        region_type=""region"",
        channel=""buy"",
        sort_type=""relevance"",
        page=1,
        page_size=5,
        minimum_price=250000,
        maximum_price=750000
    )

    # Display filtered properties
    print(""\nFiltered Property Listings in Oxford, Price Range Â£250,000 - Â£750,000:"")
    for idx, property_item in enumerate(filtered_properties_data['tieredResults'][0]['results'], start=1):
        print(f""{idx}. {property_item['title']} - Price: {property_item['price']['display']}"")
",Fail,-1,-1,30,100200
"I want to integrate real estate and school data for potential home buyers in Australia. Find nearby schools based on the specified location, retrieve a list of available properties based on filters such as city, price, and type, and provide detailed information about each selected property. School options can be explored by primary, secondary or all types within a given radius, while also having access to comprehensive details of specific property listings.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_schools_list(lat, lon):
    """"""
    Retrieve a list of schools categorized by type (all, primary, secondary) near a specified GEO location.
    """"""
    url = ""https://realty-in-au.p.rapidapi.com/schools/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""realty-in-au.p.rapidapi.com""
    }
    querystring = {
        ""lat"": str(lat),
        ""lon"": str(lon)
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_real_estate_listings(city=""Cape Coral"", price_max=250000, limit=50, sort=""RELEVANCE""):
    """"""
    Retrieve detailed information about real estate listings in a specified city, with options to filter by price and sorting order.
    """"""
    url = ""https://florida-realty-api1.p.rapidapi.com/realty/listings""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""florida-realty-api1.p.rapidapi.com""
    }
    querystring = {
        ""city"": city,
        ""price_max"": str(price_max),
        ""limit"": str(limit),
        ""sort"": sort
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_listing_details(identifier):
    """"""
    Retrieve comprehensive details about a specific real estate listing using its unique identifier.
    """"""
    url = f""https://florida-realty-api1.p.rapidapi.com/realty/listings/{identifier}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""florida-realty-api1.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers)
    return response.json()

if __name__ == '__main__':
    # Step 1: Retrieve a list of schools near a specified GEO location
    latitude = -37.7987
    longitude = 144.9532
    schools_data = get_schools_list(lat=latitude, lon=longitude)

    # Display details of nearby schools
    print(""Nearby Schools:"")
    for idx, school in enumerate(schools_data['all'], start=1):
        print(f""{idx}. {school['name']} - Type: {school['school_type']} - Address: {school['address']['street']}, {school['address']['suburb']}, {school['address']['state']} {school['address']['postcode']}"")

    # Step 2: Retrieve real estate listings for a specified city and price range
    city = ""Cape Coral""
    max_price = 300000
    real_estate_data = get_real_estate_listings(city=city, price_max=max_price, limit=10, sort=""PRICE_LOW"")

    # Display basic information of the real estate listings
    print(""\nReal Estate Listings in Cape Coral (under $300,000):"")
    for idx, listing in enumerate(real_estate_data['Result']['Offers'], start=1):
        print(f""{idx}. {listing['ItemOffered']['Address']['Name']} - Price: ${listing['Price']} - City: {city}"")

    # Step 3: Retrieve detailed information about a specific listing using its unique identifier
    if real_estate_data['Result']['Offers']:
        first_listing_id = real_estate_data['Result']['Offers'][0]['Identifier']
        listing_details = get_listing_details(identifier=first_listing_id)

        # Display detailed information of the selected listing
        print(""\nDetailed Information for the First Listing:"")
        item_offered = listing_details['Result']['Offers'][0]['ItemOffered']
        address = item_offered['Address']
        print(f""Property: {item_offered['Name']}"")
        print(f""Address: {address['StreetAddress']}, {address['AddressLocality']}, {address['AddressRegion']} {address['PostalCode']}"")
        print(f""Price: ${listing_details['Result']['Offers'][0]['Price']}"")
        print(f""Details: {item_offered['Description']}"")
","Nearby Schools:
1. North Melbourne Primary School - Type: Primary - Address: 210 ERROL STREET, NORTH MELBOURNE, VIC 3051
2. University High School - Type: Secondary - Address: 77 STORY STREET, PARKVILLE, VIC 3052
3. St Joseph's Flexible Learning Centre - Type: Secondary - Address: 385 QUEENSBERRY STREET, NORTH MELBOURNE, VIC 3051
4. Simonds Catholic College - Type: Secondary - Address: 273 VICTORIA STREET, WEST MELBOURNE, VIC 3003
5. St Michael's School - Type: Primary - Address: 8 BROUGHAM ST, NORTH MELBOURNE, VIC 3051
6. St Aloysius College - Type: Secondary - Address: 31 CURRAN ST, NORTH MELBOURNE, VIC 3051
7. Stott's College - Type: Secondary - Address: 43 ELIZABETH STREET, MELBOURNE, VIC 3000
8. Haileybury College - City Campus - Type: Combined - Address: 383 King St, West Melbourne, VIC 3003
9. Carlton Primary School - Type: Primary - Address: CNR PALMERSTON & DRUMMOND STS, CARLTON, VIC 3053
10. Carlton Gardens Primary School - Type: Primary - Address: 215 RATHDOWNE STREET, CARLTON, VIC 3053

Real Estate Listings in Cape Coral (under $300,000):
1. 4513 SW 8th Ct Apt 111, Cape Coral, FL 33914, USA - Price: $132000 - City: Cape Coral
2. 608 SE 6th St Apt F, Cape Coral, FL 33990, USA - Price: $140000 - City: Cape Coral
3. 1100 Pondella Rd Apt 416, Cape Coral, FL 33909, USA - Price: $148000 - City: Cape Coral
4. 1100 Pondella Rd Apt 1001, Cape Coral, FL 33909, USA - Price: $149000 - City: Cape Coral
5. 1113 Cape Coral Pkwy W Apt 103, Cape Coral, FL 33914, USA - Price: $149900 - City: Cape Coral
6. 4608 SE 4th Pl Apt 5, Cape Coral, FL 33904, USA - Price: $149900 - City: Cape Coral
7. 4611 SE 5th Pl Apt 8, Cape Coral, FL 33904, USA - Price: $157000 - City: Cape Coral
8. 804 Mohawk Pkwy Apt 103, Cape Coral, FL 33914, USA - Price: $158400 - City: Cape Coral
9. 1100 Pondella Rd Apt 616, Cape Coral, FL 33909, USA - Price: $159000 - City: Cape Coral
10. 4014 Coronado Pkwy Unit 106, Cape Coral, FL 33904, USA - Price: $159000 - City: Cape Coral
11. 3914 SW 8th Ct Apt 102, Cape Coral, FL 33914, USA - Price: $159500 - City: Cape Coral
12. 1100 Pondella Rd Apt 712, Cape Coral, FL 33909, USA - Price: $159999 - City: Cape Coral
13. 2504 SE 16th Pl Apt 103, Cape Coral, FL 33904, USA - Price: $160000 - City: Cape Coral
14. 2504 SE 16th Pl Apt 101, Cape Coral, FL 33904, USA - Price: $160000 - City: Cape Coral
15. 4608 SE 4th Pl Apt 3, Cape Coral, FL 33904, USA - Price: $160000 - City: Cape Coral
16. 503 Cape Coral Pkwy W Apt 104, Cape Coral, FL 33914, USA - Price: $165000 - City: Cape Coral
17. 607 SE 47th St Apt 6, Cape Coral, FL 33904, USA - Price: $165000 - City: Cape Coral
18. 4616 Skyline Blvd Apt 104, Cape Coral, FL 33914, USA - Price: $169000 - City: Cape Coral
19. 4906 Victoria Dr Apt 103, Cape Coral, FL 33904, USA - Price: $169000 - City: Cape Coral
20. 4912 Vincennes Ct Apt 201, Cape Coral, FL 33904, USA - Price: $169900 - City: Cape Coral
21. 4722 SE 1st Pl Apt 12, Cape Coral, FL 33904, USA - Price: $169900 - City: Cape Coral
22. 4613 SE 5th Ave Apt 104, Cape Coral, FL 33904, USA - Price: $170000 - City: Cape Coral
23. 4613 SE 5th Ave Apt 104, Cape Coral, FL 33904, USA - Price: $170000 - City: Cape Coral
24. 804 Mohawk Pkwy Apt 105, Cape Coral, FL 33914, USA - Price: $171000 - City: Cape Coral
25. 1407 SW 16th Ter Apt 201, Cape Coral, FL 33991, USA - Price: $174000 - City: Cape Coral
26. 4514 SW 8th Pl Unit 11, Cape Coral, FL 33914, USA - Price: $174500 - City: Cape Coral
27. 4616 Skyline Blvd Apt 207, Cape Coral, FL 33914, USA - Price: $174900 - City: Cape Coral
28. 4920 Vincennes Ct Unit 104, Cape Coral, FL 33904, USA - Price: $174900 - City: Cape Coral
29. 950 Hancock Creek South Blvd Apt 213, Cape Coral, FL 33909, USA - Price: $174999 - City: Cape Coral
30. 4516 Santa Barbara Blvd Apt 3, Cape Coral, FL 33914, USA - Price: $175000 - City: Cape Coral
31. 2526 SE 16th Pl Apt 210, Cape Coral, FL 33904, USA - Price: $175000 - City: Cape Coral
32. 2526 SE 16th Pl Apt 210, Cape Coral, FL 33904, USA - Price: $175000 - City: Cape Coral
33. 1040 Hancock Creek South Blvd Apt 101, Cape Coral, FL 33909, USA - Price: $175000 - City: Cape Coral
34. 1865 Concordia Lake Cir Unit 408, Cape Coral, FL 33909, USA - Price: $177900 - City: Cape Coral
35. 1865 Concordia Lake Cir Unit 408, Cape Coral, FL 33909, USA - Price: $177900 - City: Cape Coral
36. 1080 Hancock Creek South Blvd Apt 101, Cape Coral, FL 33909, USA - Price: $178000 - City: Cape Coral
37. 1771 Four Mile Cove Pkwy Apt 1027, Cape Coral, FL 33990, USA - Price: $179000 - City: Cape Coral
38. 3956 Pomodoro Cir Unit 103, Cape Coral, FL 33909, USA - Price: $179000 - City: Cape Coral
39. 4706 SE 4th Pl Apt 13, Cape Coral, FL 33904, USA - Price: $179000 - City: Cape Coral
40. 2813 Del Prado Blvd S Unit A19, Cape Coral, FL 33904, USA - Price: $179900 - City: Cape Coral
41. 423 Cape Coral Pkwy W Apt 107, Cape Coral, FL 33914, USA - Price: $179950 - City: Cape Coral
42. 4641 SW Santa Barbara Pl Apt 2, Cape Coral, FL 33914, USA - Price: $180000 - City: Cape Coral
43. 3907 Del Prado Blvd S Unit 104C, Cape Coral, FL 33904, USA - Price: $180000 - City: Cape Coral
44. 4407 SW 7th Pl Apt 7, Cape Coral, FL 33914, USA - Price: $180000 - City: Cape Coral
45. 1050 Hancock Creek South Blvd Apt 203, Cape Coral, FL 33909, USA - Price: $180000 - City: Cape Coral
46. 3907 Del Prado Blvd S # 104, Cape Coral, FL 33904, USA - Price: $180000 - City: Cape Coral
47. 1141 Gleason Pkwy Unit 206, Cape Coral, FL 33914, USA - Price: $182000 - City: Cape Coral
48. 950 Hancock Creek South Blvd Apt 112, Cape Coral, FL 33909, USA - Price: $183000 - City: Cape Coral
49. 1512 Cape Coral Pkwy W Apt 102, Cape Coral, FL 33914, USA - Price: $183000 - City: Cape Coral
50. 950 Hancock Creek South Blvd Apt 112, Cape Coral, FL 33909, USA - Price: $183000 - City: Cape Coral
51. 1518 SW Courtyards Ln Apt 199, Cape Coral, FL 33914, USA - Price: $184900 - City: Cape Coral
52. 4608 SE 6th Ave Apt 1D, Cape Coral, FL 33904, USA - Price: $184900 - City: Cape Coral
53. 1769 Four Mile Cove Pkwy Apt 927, Cape Coral, FL 33990, USA - Price: $184900 - City: Cape Coral
54. 1769 Four Mile Cove Pkwy Apt 911, Cape Coral, FL 33990, USA - Price: $185000 - City: Cape Coral
55. 4420 Santa Barbara Blvd Apt 105, Cape Coral, FL 33914, USA - Price: $185000 - City: Cape Coral
56. 4109 SE 19th Ave Apt 207, Cape Coral, FL 33904, USA - Price: $185000 - City: Cape Coral
57. 1771 Four Mile Cove Pkwy Apt 1015, Cape Coral, FL 33990, USA - Price: $185000 - City: Cape Coral
58. 3968 Pomodoro Cir, Cape Coral, FL 33909, USA - Price: $185999 - City: Cape Coral
59. 1417 SE 24th Ave Apt D, Cape Coral, FL 33990, USA - Price: $187000 - City: Cape Coral
60. 2510 SE 16th Pl Apt 102, Cape Coral, FL 33904, USA - Price: $187000 - City: Cape Coral
61. 4805 Sorrento Ct Unit 106, Cape Coral, FL 33904, USA - Price: $189000 - City: Cape Coral
62. 1121 Van Loon Commons Cir Apt 203, Cape Coral, FL 33909, USA - Price: $189900 - City: Cape Coral
63. 1507 Cape Coral Pkwy W Apt 5, Cape Coral, FL 33914, USA - Price: $190000 - City: Cape Coral
64. 3969 Pomodoro Cir, Cape Coral, FL 33909, USA - Price: $190000 - City: Cape Coral
65. 3947 Del Sol Ln, Cape Coral, FL 33909, USA - Price: $190000 - City: Cape Coral
66. 1101 Van Loon Commons Cir Apt 302, Cape Coral, FL 33909, USA - Price: $192500 - City: Cape Coral
67. 1100 Pondella Rd, Cape Coral, FL 33909, USA - Price: $193732 - City: Cape Coral
68. Cape Coral 321 Pkwy, Cape Coral, FL 33914, USA - Price: $193900 - City: Cape Coral
69. 804 Mohawk Pkwy Apt 106, Cape Coral, FL 33914, USA - Price: $195000 - City: Cape Coral
70. 3739 SW 9th Ave Apt 109, Cape Coral, FL 33914, USA - Price: $195000 - City: Cape Coral
71. 950 Hancock Creek South Blvd Apt 222, Cape Coral, FL 33909, USA - Price: $196900 - City: Cape Coral
72. 1518 SW Courtyards Ln Apt 200, Cape Coral, FL 33914, USA - Price: $198000 - City: Cape Coral
73. 1503 SW 51st Ln Apt 141, Cape Coral, FL 33914, USA - Price: $198000 - City: Cape Coral
74. 3940 Pomodoro Cir, Cape Coral, FL 33909, USA - Price: $198999 - City: Cape Coral
75. 1524 SW 50th St Apt 203, Cape Coral, FL 33914, USA - Price: $199000 - City: Cape Coral
76. 4213 SE 19th Ave Apt 205, Cape Coral, FL 33904, USA - Price: $199900 - City: Cape Coral
77. 3953 Pomodoro Cir Unit 204, Cape Coral, FL 33909, USA - Price: $199900 - City: Cape Coral
78. 1304 SE 7th St Apt 110, Cape Coral, FL 33990, USA - Price: $199900 - City: Cape Coral
79. 4213 SE 19th Ave Apt 205, Cape Coral, FL 33904, USA - Price: $199900 - City: Cape Coral
80. 4213 SE 19th Ave Apt 106, Cape Coral, FL 33904, USA - Price: $199900 - City: Cape Coral
81. 950 Hancock Creek South Blvd Apt 511, Cape Coral, FL 33909, USA - Price: $199900 - City: Cape Coral
82. 4202 SE 4th Pl Unit 10, Cape Coral, FL 33904, USA - Price: $199900 - City: Cape Coral
83. 950 Hancock Creek South Blvd Apt 411, Cape Coral, FL 33909, USA - Price: $199999 - City: Cape Coral
84. 1519 Cape Coral Pkwy W Apt 6, Cape Coral, FL 33914, USA - Price: $199999 - City: Cape Coral
85. 1141 Van Loon Commons Cir Apt 103, Cape Coral, FL 33909, USA - Price: $200000 - City: Cape Coral
86. 3903 Del Prado Blvd S Unit 101, Cape Coral, FL 33904, USA - Price: $200000 - City: Cape Coral
87. 3911 Country Club Blvd Apt 104, Cape Coral, FL 33904, USA - Price: $200000 - City: Cape Coral
88. 4906 Victoria Dr Apt 413, Cape Coral, FL 33904, USA - Price: $200000 - City: Cape Coral
89. 1005 SE 40th St Apt 4, Cape Coral, FL 33904, USA - Price: $204999 - City: Cape Coral
90. 716 SE 12th Ct Apt 29, Cape Coral, FL 33990, USA - Price: $209000 - City: Cape Coral
91. 5116 Coronado Pkwy Apt 16, Cape Coral, FL 33904, USA - Price: $209000 - City: Cape Coral
92. 3913 SE 11th Ave Apt 206, Cape Coral, FL 33904, USA - Price: $209000 - City: Cape Coral
93. 3944 Pomodoro Cir Unit 203, Cape Coral, FL 33909, USA - Price: $209900 - City: Cape Coral
94. 2904 Tuscan Dr Unit 302, Cape Coral, FL 33909, USA - Price: $209900 - City: Cape Coral
95. 1313 SW 16th Ter Apt 101, Cape Coral, FL 33991, USA - Price: $210000 - City: Cape Coral
96. 1005 SE 40th St Apt 2, Cape Coral, FL 33904, USA - Price: $214900 - City: Cape Coral
97. 1100 Pondella Rd Apt 504, Cape Coral, FL 33909, USA - Price: $214900 - City: Cape Coral
98. 4205 SE 19th Ave Apt 203, Cape Coral, FL 33904, USA - Price: $214900 - City: Cape Coral
99. 621 SE 15th St Apt 116, Cape Coral, FL 33990, USA - Price: $214999 - City: Cape Coral
100. 830 SW 48th Ter Apt 10, Cape Coral, FL 33914, USA - Price: $215000 - City: Cape Coral
101. 3916 SE 11th Pl, Cape Coral, FL 33904, USA - Price: $215000 - City: Cape Coral
102. 3916 SE 11th Pl Apt 503, Cape Coral, FL 33904, USA - Price: $215000 - City: Cape Coral
103. 4508 Santa Barbara Blvd Apt 102, Cape Coral, FL 33914, USA - Price: $215999 - City: Cape Coral
104. 4021 SE 19th Ave Apt 103, Cape Coral, FL 33904, USA - Price: $218000 - City: Cape Coral
105. 4021 SE 19th Ave Apt 201, Cape Coral, FL 33904, USA - Price: $219000 - City: Cape Coral
106. 321 Cape Coral Pkwy W Apt 103, Cape Coral, FL 33914, USA - Price: $219000 - City: Cape Coral
107. 5110 SW Courtyards Way Apt 22, Cape Coral, FL 33914, USA - Price: $219000 - City: Cape Coral
108. 1110 SW 48th Ter Unit 101, Cape Coral, FL 33914, USA - Price: $219000 - City: Cape Coral
109. 5023 SW 16th Pl Unit 202, Cape Coral, FL 33914, USA - Price: $219700 - City: Cape Coral
110. 4007 Palm Tree Blvd Apt 105, Cape Coral, FL 33904, USA - Price: $219900 - City: Cape Coral
111. 708 Victoria Dr Apt 213, Cape Coral, FL 33904, USA - Price: $219900 - City: Cape Coral
112. 4007 Palm Tree Blvd Apt 105, Cape Coral, FL 33904, USA - Price: $219900 - City: Cape Coral
113. 3947 Del Sol Ln Unit 203, Cape Coral, FL 33909, USA - Price: $219900 - City: Cape Coral
114. 4013 Palm Tree Blvd Apt 306, Cape Coral, FL 33904, USA - Price: $220000 - City: Cape Coral
115. 909 SE 46th Ln Apt 210, Cape Coral, FL 33904, USA - Price: $220000 - City: Cape Coral
116. 5110 SW Courtyards Way Apt 22, Cape Coral, FL 33914, USA - Price: $220000 - City: Cape Coral
117. 1514 SW 50th St Apt 204, Cape Coral, FL 33914, USA - Price: $224900 - City: Cape Coral
118. 4205 SE 19th Ave Apt 202, Cape Coral, FL 33904, USA - Price: $225000 - City: Cape Coral
119. 4904 Viceroy St Apt 5, Cape Coral, FL 33904, USA - Price: $225000 - City: Cape Coral
120. 1320 Weeping Willow Ct, Cape Coral, FL 33909, USA - Price: $225000 - City: Cape Coral
121. 1722 Beach Pkwy Apt B3, Cape Coral, FL 33904, USA - Price: $225000 - City: Cape Coral
122. 3952 Pomodoro Cir Unit 103, Cape Coral, FL 33909, USA - Price: $225000 - City: Cape Coral
123. 2904 Tuscan Dr Unit 303, Cape Coral, FL 33909, USA - Price: $225000 - City: Cape Coral
124. 2904 Tuscan Dr Unit 303, Cape Coral, FL 33909, USA - Price: $225000 - City: Cape Coral
125. 4015 Palm Tree Blvd Apt 302, Cape Coral, FL 33904, USA - Price: $225000 - City: Cape Coral
126. 3711 Del Prado Blvd S Apt 2, Cape Coral, FL 33904, USA - Price: $225000 - City: Cape Coral
127. 4018 SE 12th Ave Apt 105, Cape Coral, FL 33904, USA - Price: $225000 - City: Cape Coral
128. 3901 SE 11th Pl Apt 103, Cape Coral, FL 33904, USA - Price: $225000 - City: Cape Coral
129. 4009 Palm Tree Blvd Apt 402, Cape Coral, FL 33904, USA - Price: $227000 - City: Cape Coral
130. 505 SW 47th Ter Apt 202, Cape Coral, FL 33914, USA - Price: $227000 - City: Cape Coral
131. 4013 Palm Tree Blvd Apt 301, Cape Coral, FL 33904, USA - Price: $228500 - City: Cape Coral
132. 1129 Van Loon Commons Cir Apt 303, Cape Coral, FL 33909, USA - Price: $228777 - City: Cape Coral
133. 3964 Pomodoro Cir Unit 104, Cape Coral, FL 33909, USA - Price: $229000 - City: Cape Coral
134. 1770 Concordia Lake Cir Unit 3105, Cape Coral, FL 33909, USA - Price: $229000 - City: Cape Coral
135. 5020 SW Courtyards Way Apt 10, Cape Coral, FL 33914, USA - Price: $229900 - City: Cape Coral
136. 5510 SW 4th Pl Apt 607, Cape Coral, FL 33914, USA - Price: $229900 - City: Cape Coral
137. 909 SE 46th Ln Apt 101, Cape Coral, FL 33904, USA - Price: $229900 - City: Cape Coral
138. 1104 SE 8th St Apt 7, Cape Coral, FL 33990, USA - Price: $230000 - City: Cape Coral
139. 1210 SE 6th Ter Apt 87, Cape Coral, FL 33990, USA - Price: $230000 - City: Cape Coral
140. 3944 Pomodoro Cir Unit 204, Cape Coral, FL 33909, USA - Price: $234900 - City: Cape Coral
141. 3807 SE 11th Pl Unit 1203, Cape Coral, FL 33904, USA - Price: $234900 - City: Cape Coral
142. 3952 Pomodoro Cir Unit 102, Cape Coral, FL 33909, USA - Price: $234975 - City: Cape Coral
143. 521 SW 47th Ter Apt 202, Cape Coral, FL 33914, USA - Price: $234999 - City: Cape Coral
144. 4613 SE 5th Ave Apt 210, Cape Coral, FL 33904, USA - Price: $235000 - City: Cape Coral
145. 1111 SE 8th Ter Apt 3F, Cape Coral, FL 33990, USA - Price: $235000 - City: Cape Coral
146. 5510 SW 4th Pl Apt 501, Cape Coral, FL 33914, USA - Price: $235000 - City: Cape Coral
147. 829 SW 47th Ter Apt 204, Cape Coral, FL 33914, USA - Price: $235000 - City: Cape Coral
148. 1102 SE 39th Ter Apt 105, Cape Coral, FL 33904, USA - Price: $235000 - City: Cape Coral
149. 4613 SE 5th Ave Apt 210, Cape Coral, FL 33904, USA - Price: $235000 - City: Cape Coral
150. 1133 Van Loon Commons Cir Apt 202, Cape Coral, FL 33909, USA - Price: $236000 - City: Cape Coral
151. 505 SW 47th Ter Apt 104, Cape Coral, FL 33914, USA - Price: $237500 - City: Cape Coral
152. 4708 SW 8th Pl Apt 102, Cape Coral, FL 33914, USA - Price: $238999 - City: Cape Coral
153. 4807 Sunset Ct Apt 607, Cape Coral, FL 33904, USA - Price: $239000 - City: Cape Coral
154. 4613 SE 5th Ave Apt 203, Cape Coral, FL 33904, USA - Price: $239000 - City: Cape Coral
155. 1518 SW 50th St Apt 103, Cape Coral, FL 33914, USA - Price: $239000 - City: Cape Coral
156. 4912 Vincennes St Apt 101, Cape Coral, FL 33904, USA - Price: $239000 - City: Cape Coral
157. 1212 SE 6th Ter Apt 82, Cape Coral, FL 33990, USA - Price: $239000 - City: Cape Coral
158. 3939 Pomodoro Cir Unit 204, Cape Coral, FL 33909, USA - Price: $239900 - City: Cape Coral
159. 1512 SW 50th St Apt 303, Cape Coral, FL 33914, USA - Price: $239900 - City: Cape Coral
160. 4802 Tudor Dr Apt 104, Cape Coral, FL 33904, USA - Price: $239900 - City: Cape Coral
161. 4613 SE 5th Ave Apt 101, Cape Coral, FL 33904, USA - Price: $239900 - City: Cape Coral
162. 834 SE 46th St Apt 1A, Cape Coral, FL 33904, USA - Price: $239900 - City: Cape Coral
163. 1220 SE 6th Ter Apt 60, Cape Coral, FL 33990, USA - Price: $240500 - City: Cape Coral
164. 4002 Coronado Pkwy Apt 101, Cape Coral, FL 33904, USA - Price: $244900 - City: Cape Coral
165. 1766 Cape Coral Pkwy E Apt 608, Cape Coral, FL 33904, USA - Price: $244999 - City: Cape Coral
166. 1027 SE 46th Ln Apt 1A, Cape Coral, FL 33904, USA - Price: $245000 - City: Cape Coral
167. 4924 Tudor Dr Apt 202, Cape Coral, FL 33904, USA - Price: $245000 - City: Cape Coral
168. 1109 SE 40th St Apt 102, Cape Coral, FL 33904, USA - Price: $245000 - City: Cape Coral
169. 47th 1521 Ter, Cape Coral, FL 33914, USA - Price: $245000 - City: Cape Coral
170. 4017 Palm Tree Blvd Apt 204, Cape Coral, FL 33904, USA - Price: $245500 - City: Cape Coral
171. 1120 SE 46th St Apt 1C, Cape Coral, FL 33904, USA - Price: $248000 - City: Cape Coral
172. 5023 SW 16th Pl Unit 101, Cape Coral, FL 33914, USA - Price: $248350 - City: Cape Coral
173. 5023 SW 16th Pl Unit 101, Cape Coral, FL 33914, USA - Price: $248350 - City: Cape Coral
174. 5308 Chiquita Blvd S Unit 102A, Cape Coral, FL 33914, USA - Price: $248800 - City: Cape Coral
175. 4629 SE 5th Ave Apt 203, Cape Coral, FL 33904, USA - Price: $249000 - City: Cape Coral
176. 1220 SE 40th St Apt 8, Cape Coral, FL 33904, USA - Price: $249000 - City: Cape Coral
177. 4715 Santa Barbara Blvd Apt F1, Cape Coral, FL 33914, USA - Price: $249000 - City: Cape Coral
178. 853 SE 46th Ln Apt 202, Cape Coral, FL 33904, USA - Price: $249900 - City: Cape Coral
179. 1781 Four Mile Cove Pkwy Unit 134, Cape Coral, FL 33990, USA - Price: $249900 - City: Cape Coral
180. 632 Victoria Dr Apt 104, Cape Coral, FL 33904, USA - Price: $249900 - City: Cape Coral
181. 4563 SE 5th Pl Apt 205, Cape Coral, FL 33904, USA - Price: $249900 - City: Cape Coral
182. 4116 SE 19th Ave Unit 101, Cape Coral, FL 33904, USA - Price: $249900 - City: Cape Coral
183. 1766 Cape Coral Pkwy E Apt 110, Cape Coral, FL 33904, USA - Price: $249900 - City: Cape Coral
184. 1800 Concordia Lake Cir Unit 2302, Cape Coral, FL 33909, USA - Price: $249900 - City: Cape Coral
185. 4921 SW 16th Pl, Cape Coral, FL 33914, USA - Price: $249900 - City: Cape Coral
186. 1518 SW 50th St Apt 204, Cape Coral, FL 33914, USA - Price: $249900 - City: Cape Coral
187. 904 SE 13th St Apt 201, Cape Coral, FL 33990, USA - Price: $249900 - City: Cape Coral
188. 5312 Chiquita Blvd S Unit 101B, Cape Coral, FL 33914, USA - Price: $249900 - City: Cape Coral
189. 1333 SE 40th Ter Apt 2C, Cape Coral, FL 33904, USA - Price: $250000 - City: Cape Coral
190. 1129 Van Loon Commons Cir Apt 301, Cape Coral, FL 33909, USA - Price: $250000 - City: Cape Coral
191. 4924 Tudor Dr Apt 103, Cape Coral, FL 33904, USA - Price: $250000 - City: Cape Coral
192. 5510 SW 4th Pl Apt 601, Cape Coral, FL 33914, USA - Price: $254900 - City: Cape Coral
193. 1514 SW 50th St Apt 301, Cape Coral, FL 33914, USA - Price: $254900 - City: Cape Coral
194. 4924 Tudor Dr Apt 203, Cape Coral, FL 33904, USA - Price: $255000 - City: Cape Coral
195. 1149 Winding Pines Cir Unit 101, Cape Coral, FL 33909, USA - Price: $255900 - City: Cape Coral
196. 1149 Winding Pines County Rd Unit 101, Cape Coral, FL 33909, USA - Price: $255900 - City: Cape Coral
197. 1788 Concordia Lake Cir Unit 2707, Cape Coral, FL 33909, USA - Price: $257500 - City: Cape Coral
198. 1137 Van Loon Commons Cir Apt 204, Cape Coral, FL 33909, USA - Price: $258700 - City: Cape Coral
199. 1137 Van Loon Commons Cir Apt 204, Cape Coral, FL 33909, USA - Price: $258700 - City: Cape Coral
200. 4924 Viceroy St Apt B1, Cape Coral, FL 33904, USA - Price: $258900 - City: Cape Coral

Detailed Information for the First Listing:
Property: 1 Bed, 1 Bath, 753 null
Address: 4513 SW 8th Ct Apt 111, Cape Coral, Lee 33914
Price: $132000
Details: This 1 bedroom 1 bathroom is located in SW Cape Coral, close to shopping, the Westin Hotel, Downtown Cape Coral, Approximately 16 miles to Fort Myers Beach, and 24 miles to Sanibel Island. This condo features an Open Floor plan, and a nice sized lanai. The building was recently painted and has a new roof.
",1,1,9,31788
"Users want to get weekly economic indicators and display listings of rental properties in specific cities and states in the United States. First collect weekly data, analyze recent trends, and filter attributes based on the results. For example, if weekly data shows a decline in economic indicators, focus on properties below a certain price threshold; If the economic indicators are stable, we can consider widening the price range. Users can specify locations by city and state to screen and recommend relevant rental properties based on the latest economic trends.","import requests

# Define the API Key
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


# Function to retrieve weekly economic indicators
def get_weekly_data():
    """"""
    Retrieve weekly economic indicators using the Semanal API.
    """"""
    url = ""https://riesgo-pais.p.rapidapi.com/api/riesgopais/semanal""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""riesgo-pais.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching weekly economic data: {response.status_code}"")
        return None


# Function to retrieve rental property listings
def get_rental_properties(state, city, page, price_max=None):
    """"""
    Retrieve rental property listings using the v1_properties_rent API.
    Optionally filter properties by a maximum price based on economic indicators.
    """"""
    url = ""https://real-estate12.p.rapidapi.com/v1/real-estate/rent""
    querystring = {
        ""state"": state,
        ""city"": city,
        ""page"": page,
        ""sort"": ""newest""
    }

    if price_max:
        querystring[""price_h""] = str(price_max)

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""real-estate12.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers, params=querystring)

    # Debugging output for API response
    if response.status_code == 200:
        # print(""\nAPI Response Data:"")
        # print(response.json())
        return response.json()
    else:
        print(f""Error fetching rental properties: {response.status_code}"")
        return None


# Main code to demonstrate the functionality
if __name__ == '__main__':
    # Step 1: Retrieve weekly economic data
    weekly_data = get_weekly_data()
    if weekly_data:
        print(""\nWeekly Economic Data:"")
        print(weekly_data)

        # Analyze the weekly data to determine the price threshold
        recent_week_values = [entry['value'] for entry in weekly_data[-4:]]  # Consider the last 4 weeks
        average_value = sum(recent_week_values) / len(recent_week_values)
        print(f""\nAverage economic indicator for the last 4 weeks: {average_value}"")

        # Set a price threshold based on economic trend
        if average_value < 100:  # Example threshold: below 100, economic indicators are declining
            max_price = 1500  # Focus on properties below $1500 if economic indicators are declining
            print(""Economic indicators are declining. Focusing on properties below $1500."")
        else:
            max_price = 2000  # Consider broader price range otherwise
            print(""Economic indicators are stable. Considering properties up to $2000."")

        # Step 2: Retrieve rental properties based on economic analysis
        city = ""Los Angeles""
        state = ""CA""
        rental_properties = get_rental_properties(state=state, city=city, page=""1"", price_max=max_price)

        if rental_properties:
            # Correctly extract properties from the 'body' key
            property_list = rental_properties.get('body', [])
            if not property_list:
                print(""No rental properties found in the specified criteria."")
            else:
                print(""\nFiltered Rental Properties:"")
                for idx, property_item in enumerate(property_list, start=1):
                    title = property_item.get('description', {}).get('type', 'No title')
                    price = property_item.get('list_price', 'N/A')
                    address = property_item.get('location', {}).get('address', {}).get('line', 'N/A')
                    print(f""{idx}. {title} - Price: ${price} - Address: {address}"")
        else:
            print(""Failed to retrieve rental properties."")
",Fail,-1,-1,29,92220
"Users want to get popular videos based on the specified region and language, allowing users to search for movies or TV shows related to those video titles and then retrieve popular reviews of popular videos of interest. The service should first get popular videos in a particular region. It should then allow users to select a title from a popular video to search for relevant movies or shows and provide detailed metadata about them. Finally, if the user wants to browse the comments of any selected popular video, the service should be able to fetch and display the popular comments related to that video.","import requests

# Define API Key
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

# Function to retrieve trending videos for a specific region and language
def get_trending_videos(geo='US', lang='en'):
    """"""
    Retrieve trending videos for a specified geo location and language.
    """"""
    url = ""https://yt-api.p.rapidapi.com/trending""
    querystring = {""geo"": geo, ""lang"": lang}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""yt-api.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching trending videos: {response.status_code}"")
        return None

# Function to search related movies or TV shows based on a query
def search_related_content(query):
    """"""
    Search for related movies or TV shows based on a given query.
    """"""
    url = ""https://movies-api14.p.rapidapi.com/search""
    querystring = {""query"": query}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""movies-api14.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error searching for related content: {response.status_code}"")
        return None

# Function to retrieve video comments for a specified video ID
def get_video_comments(video_id, sort_by='newesttop'):
    """"""
    Retrieve comments for a specified video ID, sorted by criteria (e.g., 'newesttop').
    """"""
    url = ""https://yt-api.p.rapidapi.com/comments""
    querystring = {""id"": video_id, ""sort_by"": sort_by}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""yt-api.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching video comments: {response.status_code}"")
        return None

# Main code to demonstrate the functionality
if __name__ == '__main__':
    # Step 1: Get trending videos
    trending_data = get_trending_videos(geo='US', lang='en')
    if trending_data and 'data' in trending_data:
        print(""Trending Videos Data:"")
        for idx, video in enumerate(trending_data['data'][:5], start=1):  # Limit output to top 5 for clarity
            title = video.get('title', 'No title')
            author = video.get('author', 'Unknown channel')
            print(f""{idx}. Title: {title} - Channel: {author}"")

        # Choose a video title from trending data to search for related movies/shows
        first_video_title = trending_data['data'][0].get('title', 'No title')
        print(f""\nSearching for movies or TV shows related to: '{first_video_title}'"")

        # Step 2: Search for related content
        search_results = search_related_content(first_video_title)
        if search_results and 'results' in search_results:
            print(""Search Results:"")
            for idx, result in enumerate(search_results['results'][:3], start=1):  # Limit to top 3 for clarity
                result_title = result.get('title', 'No title')
                result_type = result.get('type', 'Unknown type')
                release_year = result.get('year', 'N/A')
                print(f""{idx}. Title: {result_title} - Type: {result_type} - Release Year: {release_year}"")

        # Choose a video ID to retrieve comments (taking the first video ID from trending)
        first_video_id = trending_data['data'][0].get('videoId', None)
        if first_video_id:
            print(f""\nRetrieving comments for video ID: {first_video_id}"")

            # Step 3: Get comments for the chosen video
            comments_data = get_video_comments(first_video_id)
            if comments_data and 'data' in comments_data:
                print(""Top Video Comments:"")
                for idx, comment in enumerate(comments_data['data'][:3], start=1):  # Limit to top 3 comments for clarity
                    comment_author = comment.get('author', 'Unknown author')
                    comment_text = comment.get('text', 'No comment text')
                    print(f""{idx}. {comment_author} says: {comment_text}"")
            else:
                print(""No comments found for the selected video."")
        else:
            print(""Failed to retrieve a valid video ID from trending data."")
    else:
        print(""Failed to retrieve trending videos."")
","Trending Videos Data:
1. Title: ThatPinkScienceTeacher - Channel: Unknown channel
2. Title: Megan Thee Stallion - Bigger In Texas [Official Video] - Channel: Unknown channel
3. Title: F I N A L ï½œ Alien Stage - Channel: Unknown channel
4. Title: Trending Shorts - Channel: Unknown channel
5. Title: Summer Walker - Heart Of A Woman [Official Lyric Video] - Channel: Unknown channel

Searching for movies or TV shows related to: 'ThatPinkScienceTeacher'
Failed to retrieve a valid video ID from trending data.",1,1,9,24165
"A user who wants to find a nearby cafe or restaurant in Paris uses the location ID in a nearby search to retrieve details about a particular business (such as name, contact details, ratings, and hours of operation) and displays user reviews of the selected business. You should first get a list of cafes or restaurants based on the coordinates specified by the user. After you select a particular business from the list, the service retrieves and displays details about that business and uses its location ID. Finally, users should be able to view reviews and feedback on the selected business","import requests

# Define API Key
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


# Function to search nearby cafes or restaurants
def search_nearby_cafes(lat, lng, query=""cafe""):
    url = ""https://maps-data.p.rapidapi.com/nearby.php""
    querystring = {
        ""query"": query,
        ""lat"": str(lat),
        ""lng"": str(lng),
        ""limit"": ""20"",
        ""country"": ""fr"",
        ""lang"": ""en""
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""maps-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching nearby places: {response.status_code}"")
        return None


# Function to retrieve place information based on place ID
def get_place_info(place_id):
    url = ""https://maps-data.p.rapidapi.com/place.php""
    querystring = {""place_id"": place_id, ""country"": ""fr"", ""lang"": ""en""}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""maps-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching place information: {response.status_code}"")
        return None


# Function to retrieve reviews for a specific business
def get_place_reviews(business_id):
    url = ""https://maps-data.p.rapidapi.com/reviews.php""
    querystring = {
        ""business_id"": business_id,
        ""country"": ""fr"",
        ""lang"": ""en"",
        ""limit"": ""20"",
        ""sort"": ""Relevant""
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""maps-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching reviews: {response.status_code}"")
        return None


# Function to print formatted place details
def print_place_details(place_info):
    if not place_info or 'data' not in place_info:
        print(""No detailed information available."")
        return

    data = place_info['data'][0]
    name = data.get('name', 'Unknown')
    address = data.get('full_address', 'Unknown address')
    rating = data.get('rating', 'N/A')
    review_count = data.get('review_count', 0)
    phone = data.get('phone_number', 'N/A')
    price_level = data.get('price_level', 'N/A')
    website = data.get('website_full', 'N/A')
    working_hours = data.get('working_hours', {})

    print(""\n=== Place Details ==="")
    print(f""Name: {name}"")
    print(f""Address: {address}"")
    print(f""Rating: {rating} / 5 ({review_count} reviews)"")
    print(f""Phone: {phone}"")
    print(f""Price Level: {price_level}"")
    print(f""Website: {website}"")

    print(""\nWorking Hours:"")
    for day, hours in working_hours.items():
        print(f""  {day}: {', '.join(hours)}"")


# Function to print formatted reviews
def print_reviews(reviews_list):
    if not reviews_list or not isinstance(reviews_list, list):
        print(""No reviews available."")
        return

    print(""\n=== Reviews ==="")
    for idx, review in enumerate(reviews_list[:3], start=1):  # Limit to top 3 reviews
        reviewer = review.get('author', 'Anonymous')
        rating = review.get('rating', 'N/A')
        comment = review.get('text', 'No comment available')
        print(f""{idx}. Reviewer: {reviewer}"")
        print(f""   Rating: {rating}/5"")
        print(f""   Comment: {comment}\n"")


# Main code to demonstrate the functionality
if __name__ == '__main__':
    # Step 1: Search for nearby cafes/restaurants
    lat = 48.8577848
    lng = 2.2888379
    nearby_places = search_nearby_cafes(lat, lng)

    if nearby_places and 'data' in nearby_places:
        print(""=== Nearby Cafes/Restaurants ==="")
        for idx, place in enumerate(nearby_places['data'][:5], start=1):  # Limit to top 5 results
            name = place.get('name', 'Unknown')
            address = place.get('address', 'Unknown address')
            place_id = place.get('place_id', 'N/A')
            print(f""{idx}. Name: {name}"")
            print(f""   Address: {address}"")
            print(f""   Place ID: {place_id}\n"")

        # Step 2: Get detailed information about a specific place (using first place ID)
        first_place_id = nearby_places['data'][0].get('place_id', None)
        if first_place_id:
            place_info = get_place_info(first_place_id)
            print_place_details(place_info)

            # Step 3: Get reviews for the specific place
            business_id = nearby_places['data'][0].get('business_id', None)
            if business_id:
                place_reviews = get_place_reviews(business_id)

                # Check if 'data' is a list and iterate safely
                reviews_list = place_reviews.get('data', [])
                print_reviews(reviews_list)
            else:
                print(""No business ID found for the selected place."")
        else:
            print(""No valid place ID found in the response."")
    else:
        print(""No nearby cafes or restaurants found."")
",Fail,-1,-1,31,107601
"Users want to search for job opportunities based on specific job titles and locations, retrieve detailed information about the selected position, and estimate the expected salary for that position in a given region. The service should first obtain a list of job posts based on user-defined criteria such as job title, location, and Posting date. Users can then select a job from the search results to get other details, such as app links and employer information. Finally, the service should provide an estimated salary range for selected positions in the same region to help users assess potential earnings.","import requests

# Define API Key
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

# Function to search jobs based on query and location
def search_jobs(query, location, date_posted=""all"", page=1, num_pages=1):
    url = ""https://jsearch.p.rapidapi.com/search""
    querystring = {
        ""query"": f""{query} in {location}"",
        ""page"": str(page),
        ""num_pages"": str(num_pages),
        ""date_posted"": date_posted
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""jsearch.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching job search results: {response.status_code}"")
        return None

# Function to retrieve detailed job information based on job ID
def get_job_details(job_id):
    url = ""https://jsearch.p.rapidapi.com/job-details""
    querystring = {
        ""job_id"": job_id,
        ""extended_publisher_details"": ""false""
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""jsearch.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching job details: {response.status_code}"")
        return None

# Function to estimate job salary based on job title and location
def estimate_job_salary(job_title, location, radius=50):
    url = ""https://jsearch.p.rapidapi.com/estimated-salary""
    querystring = {
        ""job_title"": job_title,
        ""location"": location,
        ""radius"": str(radius)
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""jsearch.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error estimating job salary: {response.status_code}"")
        return None

# Main code to demonstrate the functionality
if __name__ == '__main__':
    # Step 1: Search for jobs based on job title and location
    job_title = ""Software Engineer""
    location = ""San Francisco, CA, USA""
    jobs_data = search_jobs(job_title, location)
    if jobs_data and 'data' in jobs_data:
        print(""Job Search Results:"")
        for idx, job in enumerate(jobs_data['data'][:5], start=1):  # Limit to top 5 results
            # Improved access with get method and checking for potential None values
            title = job.get('title', 'No title available')
            company = job.get('company_name', 'No company information available')
            job_location = job.get('location', 'No location available')
            job_id = job.get('job_id', 'N/A')
            print(f""{idx}. Job Title: {title} - Company: {company} - Location: {job_location} - Job ID: {job_id}"")

        # Step 2: Retrieve detailed information about a specific job (using first job ID)
        first_job_id = jobs_data['data'][0].get('job_id', None)
        if first_job_id:
            job_details = get_job_details(first_job_id)
            if job_details and 'data' in job_details and len(job_details['data']) > 0:
                detailed_job = job_details['data'][0]
                print(f""\nJob Details for Job ID {first_job_id}:"")
                print(f""Job Title: {detailed_job.get('job_title', 'No job title available')}"")
                print(f""Company: {detailed_job.get('employer_name', 'No company information')}"")
                print(f""Location: {detailed_job.get('job_city', 'No city')}, {detailed_job.get('job_state', 'No state')}"")
                print(f""Description: {detailed_job.get('job_description', 'No description available')}"")
                print(f""Apply Link: {detailed_job.get('job_apply_link', 'No apply link available')}"")

                # Step 3: Estimate salary for the job title in the same location
                salary_estimation = estimate_job_salary(job_title, location)
                if salary_estimation and 'data' in salary_estimation:
                    print(f""\nSalary Estimation for '{job_title}' in {location}:"")
                    for estimate in salary_estimation['data']:
                        min_salary = estimate.get('min_salary', 'N/A')
                        max_salary = estimate.get('max_salary', 'N/A')
                        median_salary = estimate.get('median_salary', 'N/A')
                        publisher = estimate.get('publisher_name', 'N/A')
                        print(f"" - Publisher: {publisher} | Min: ${min_salary} | Max: ${max_salary} | Median: ${median_salary}"")
                else:
                    print(""No salary estimation data available."")
            else:
                print(""No detailed information available for this job."")
        else:
            print(""No valid job ID found."")
    else:
        print(""No job search results found."")
","Job Search Results:
1. Job Title: No title available - Company: No company information available - Location: No location available - Job ID: hQ_qGqqYxnh2nW31AAAAAA==
2. Job Title: No title available - Company: No company information available - Location: No location available - Job ID: qoE2sLWeZIPC_ULHAAAAAA==
3. Job Title: No title available - Company: No company information available - Location: No location available - Job ID: 6JZJ1XpxEeEsghKhAAAAAA==
4. Job Title: No title available - Company: No company information available - Location: No location available - Job ID: 626OB_pvS-CuKZRZAAAAAA==
5. Job Title: No title available - Company: No company information available - Location: No location available - Job ID: wId2R2Yukp4o-KxvAAAAAA==

Job Details for Job ID hQ_qGqqYxnh2nW31AAAAAA==:
Job Title: Software Development Engineer
Company: Adobe
Location: San Francisco, CA
Description: JOB LEVEL
P50

EMPLOYEE ROLE
Individual Contributor

The Challenge

We are seeking an experienced Senior Software Engineer to join a highly motivated and fast-paced team, building a new creativity platform that will help define the future of Creative Cloud and Adobeâ€™s Digital Media business! You will need a keen eye for details and high standards for design, code quality, and efficiency. People who enjoy taking on complex technical challenges, have a real passion for delighting customers, and who are self-motivated to push themselves in a team-oriented culture will thrive in our environment.

What Youâ€™ll Do:

On the Developer Experience team, we are responsible for increasing the velocity and scale of our development team by providing easy to use development environments, infrastructure and collaborating with other peers and product managers to create a compelling and seamless developer experience on a multi-tenant web application platform. These systems are highly reliable, performant, and continuously address the most important development needs while ensuring commits meet the rigorous check-in gates and deployment guidelines.

Youâ€™ll work with some of the best designers and engineers to integrate with 3rd party content partners, extend current Adobe ecosystems.

Responsibilities:
â€¢ Design and develop reliable Continuous Delivery Pipeline for developing, building, testing, and deployment desktop, mobile, and service offerings.
â€¢ Maintain and support internal cloud-based automation systems: infrastructure and environment, build and test tools, automation orchestration, reports, and dashboards.
â€¢ Collect, refine, and prioritize needs from the engineering team.
â€¢ Work closely with engineering team members to integrate automation tools into daily processes.
â€¢ Build tools, dashboards, and other related infrastructure to help improve the development experience for our engineers.
â€¢ Work well both independently and collaboratively within a fast-moving global development team with clear, positive, and constructive communication.
â€¢ Influence the growth of a team DevOps culture and mindset.

Requirements:
â€¢ Based in San Francisco/San Jose/Seattle
â€¢ Educational Background: Bachelorâ€™s degree in computer science or a related field. Masterâ€™s degree preferred.
â€¢ 7+ years of experience working on build and release infrastructure
â€¢ Experience developing software for multiple devices and platforms.
â€¢ Familiarity with automated building and testing of desktop, service, and web-delivered software.
â€¢ Strong written and verbal communication skills
â€¢ Experience administering Linux, Mac, and Windows systems
â€¢ CI/CD Knowledge: Hands-on experience with Continuous Integration and Continuous Deployment tools like Jenkins, CircleCI, Travis CI, and GitHub Actions. Familiarity with container orchestration platforms such as Kubernetes is a plus.
â€¢ Familiarity with Bazel a plus
â€¢ Developer Productivity Tools: Experience with developer productivity and collaboration tools such as Jira, Confluence, Slack, and VS Code extensions or plugins.
â€¢ Familiarity and experience with relevant automation technologies:
â€¢ TypeScript/Node, Python, Groovy, Bash
â€¢ Webpack, RushJS, npm
â€¢ Kubernetes, Docker
â€¢ Familiarity with Cloud platforms like AWS and Azure
â€¢ Experience with Agile development processes including Scrum.

Our compensation reflects the cost of labor across severalâ€¯ U.S. geographic markets, and we pay differently based on those defined markets. The U.S. pay range for this positionâ€¯is $154,000 -- $278,800 annually. Payâ€¯within this range varies by work locationâ€¯and may also depend on job-related knowledge, skills,â€¯and experience. Your recruiter can share more about the specific salary range for the job location during the hiring process.

At Adobe, for sales roles starting salaries are expressed as total target compensation (TTC = base + commission), and short-term incentives are in the form of sales commission plans. Non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the Annual Incentive Plan (AIP).

In addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award.

Adobe will consider qualified applicants with arrest or conviction records for employment in accordance with state and local laws and â€œfair chanceâ€ ordinances.

Internal Opportunities

Creativity, curiosity, and constant learning are celebrated aspects of your career growth journey. Weâ€™re glad that youâ€™re pursuing a new opportunity at Adobe!

Put your best foot forward:

1. Update your Resume/CV and Workday profile â€“ donâ€™t forget to include your uniquely â€˜Adobeâ€™ experiences and volunteer work.

2. Visit the Internal Mobility page on Inside Adobe to learn more about the process and set up a job alert for roles youâ€™re interested in.

3. Check out these tips to help you prep for interviews.

4. If you are applying for a role outside of your current country, ensure you review the International Resources for Relocating Employees on Inside Adobe, including the impacts to your Benefits, AIP, Equity & Payroll.

Once you apply for a role via Workday, the Talent Team will reach out to you within 2 weeks. If you move into the official interview process with the hiring team, make sure you inform your manager so they can champion your career growth.

At Adobe, you will be immersed in an exceptional work environment that is recognized around the world. You will also be surrounded by colleagues who are committed to helping each other grow through our unique Check-In approach where ongoing feedback flows freely. If youâ€™re looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the meaningful benefits we offer.

Adobe is an equal opportunity and affirmative action employer. We welcome and encourage diversity in the workplace regardless of gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other characteristics protected by law.

If you have a disability or special need that requires accommodation to navigate our internal careers site or to complete the application process, please contact accommodations@adobe.com.
Apply Link: https://careers.adobe.com/us/en/job/R147427/Software-Development-Engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic

Salary Estimation for 'Software Engineer' in San Francisco, CA, USA:
 - Publisher: Indeed | Min: $None | Max: $None | Median: $70
 - Publisher: Levels | Min: $153000 | Max: $250000 | Median: $188000
 - Publisher: Builtinsf | Min: $80000 | Max: $300000 | Median: $162000
",1,1,14,39942
"Develop a houseplant exploration service to help users browse different categories of houseplants, especially ferns. Users can view a list of all available plant categories, select a category and get detailed information about ferns in it (including common names, pictures, and climate adaptations). In addition, the service also provides a general overview of houseplants, showing basic information such as classification, scientific name and growing region, so that users can have a more comprehensive understanding of various houseplants.","import requests

# Define API Key
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


# Function to retrieve all available plant categories
def get_all_categories():
    url = ""https://house-plants2.p.rapidapi.com/categories""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""house-plants2.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching plant categories: {response.status_code}"")
        return None


# Function to retrieve species by category
def get_species_by_category(category):
    url = f""https://house-plants2.p.rapidapi.com/category/{category}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""house-plants2.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        print(""\nDEBUG: Species API response data:"")
        print(response.json())  # Debugging output to check the response structure
        return response.json()
    else:
        print(f""Error fetching species for category '{category}': {response.status_code}"")
        return None


# Function to retrieve basic information about all house plants
def get_all_lite_plant_info():
    url = ""https://house-plants2.p.rapidapi.com/all-lite""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""house-plants2.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        # print(""\nDEBUG: All lite plant info API response data:"")
        # print(response.json())  # Debugging output to check the response structure
        return response.json()
    else:
        print(f""Error fetching all lite plant info: {response.status_code}"")
        return None


# Main code to demonstrate the functionality
if __name__ == '__main__':
    # Step 1: Retrieve all available plant categories
    categories = get_all_categories()
    if categories:
        print(""Available Plant Categories:"")
        for idx, category in enumerate(categories[:5], start=1):  # Limit to top 5 categories for clarity
            category_name = category.get('Category', 'Unknown Category')
            print(f""{idx}. Category: \""{category_name}\"""")

        # Attempt to select a category that might have relevant plant information
        # Example: If ""Ferns"" is not available, choose the first available category
        selected_category = next((cat['Category'] for cat in categories if ""Ferns"" in cat['Category']),
                                 categories[0].get('Category', None))

        # Step 2: Get detailed information about species in a selected category
        if selected_category:
            print(f""\nFetching species under category: {selected_category}"")
            species_data = get_species_by_category(selected_category)
            if species_data:
                print(""Species Information:"")
                for plant in species_data:
                    # Display information if it has valid data
                    common_name = plant.get('Common name', ['No common name available'])[0]  # Use first common name
                    scientific_name = plant.get('Latin name', 'No scientific name available')
                    climate = plant.get('Climat', 'No climate information available')
                    image = plant.get('Img', 'No image available')

                    # Print only if relevant fields are available
                    if common_name and scientific_name and climate:
                        print(f"" - {common_name} ({scientific_name}) - Climate: {climate} - Image URL: {image}"")

    # Step 3: Retrieve basic information about all house plants
    all_plants_info = get_all_lite_plant_info()
    if all_plants_info:
        print(""\nBasic Information for All House Plants:"")
        for plant in all_plants_info[:5]:  # Limit to top 5 plants for clarity
            category = plant.get('category', 'Unknown category')
            scientific_name = plant.get('scientific_name', 'No scientific name available')
            growth_region = plant.get('growth_region', 'No growth region available')
            common_name = plant.get('common_name', 'No common name available')

            # Only display if essential fields are present
            if category and scientific_name and common_name:
                print(
                    f"" - Category: {category}, Common Name: {common_name}, Scientific Name: {scientific_name}, Growth Region: {growth_region}"")
",Fail,-1,-1,27,96390
"Explore detailed business and location information based on the address or coordinates provided by the user. First retrieve the geographic details of a given address, including its latitude, longitude, and time zone. Once the location is determined, nearby businesses should be found on the specified coordinates, including operational details and ratings. Finally, for any business selected, details such as name, address, review, and working hours should be retrieved and displayed.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def get_geocode_info(query, country='fr', lang='en'):
    url = ""https://maps-data.p.rapidapi.com/geocoding.php""
    querystring = {""query"": query, ""lang"": lang, ""country"": country}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""maps-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def get_nearby_places(lat, lng, country='fr', lang='en'):
    url = ""https://maps-data.p.rapidapi.com/whatishere.php""
    querystring = {""lat"": str(lat), ""lng"": str(lng), ""lang"": lang, ""country"": country}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""maps-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def get_place_info(business_id, country='fr', lang='en', place_id=None):
    url = ""https://maps-data.p.rapidapi.com/place.php""
    querystring = {""business_id"": business_id, ""country"": country, ""lang"": lang}
    if place_id:
        querystring[""place_id""] = place_id
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""maps-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def format_geocode_info(data):
    print(""\n### Geocode Information ###"")
    if 'data' in data:
        address = data['data'].get('address', 'Unknown')
        lat = data['data'].get('lat', 'Unknown')
        lng = data['data'].get('lng', 'Unknown')
        timezone = data['data'].get('timezone', 'Unknown')

        print(f""Address   : {address}"")
        print(f""Latitude  : {lat}"")
        print(f""Longitude : {lng}"")
        print(f""Timezone  : {timezone}"")
    else:
        print(""No geocode information available. Response:"", data)


def format_nearby_places(data):
    print(""\n### Nearby Places Information ###"")
    if 'data' in data:
        address = data['data'].get('address', 'Unknown')
        town = data['data'].get('town', 'Unknown')
        country = data['data'].get('country', 'Unknown')
        timezone = data['data'].get('timezone', 'Unknown')

        print(f""Address   : {address}"")
        print(f""Town      : {town}"")
        print(f""Country   : {country}"")
        print(f""Timezone  : {timezone}"")

        places = data['data'].get('places', [])
        if places:
            print(""\n### List of Nearby Places ###"")
            for i, place in enumerate(places, start=1):
                business_id = place.get('business_id', 'Unknown')
                place_name = place.get('name', 'Unknown')
                print(f""{i}. {place_name} (Business ID: {business_id})"")
        else:
            print(""No nearby places found."")
    else:
        print(""No nearby places information available. Response:"", data)


def format_place_info(data):
    print(""\n### Detailed Place Information ###"")
    if 'data' in data:
        name = data['data'].get('name', 'Unknown')
        address = data['data'].get('address', 'Unknown')
        phone = data['data'].get('phone', 'Unknown')
        website = data['data'].get('website', 'Unknown')

        print(f""Name     : {name}"")
        print(f""Address  : {address}"")
        print(f""Phone    : {phone}"")
        print(f""Website  : {website}"")
    else:
        print(""No detailed place information available. Response:"", data)


if __name__ == '__main__':
    # Step 1: Geocode information from an address
    address = ""Place de la Navigation""
    geo_data = get_geocode_info(address)
    format_geocode_info(geo_data)

    if 'data' in geo_data:
        lat = geo_data['data']['lat']
        lng = geo_data['data']['lng']

        # Step 2: Get nearby places based on the lat/lng
        nearby_places = get_nearby_places(lat, lng)
        format_nearby_places(nearby_places)

        # Step 3: Get detailed place information for the first business found
        if 'data' in nearby_places and len(nearby_places['data'].get('places', [])) > 0:
            first_business_id = nearby_places['data']['places'][0].get('business_id')
            if first_business_id:
                place_info = get_place_info(first_business_id)
                format_place_info(place_info)
",Fail,-1,-1,32,104128
"Build a service that allows users to search for Reddit posts on a specific topic or within a subreddit, retrieve hot or top posts from a specific subreddit, and get detailed information about individual posts. The service should begin by allowing users to search Reddit posts using specific keywords, subreddit filters, and sort options. After displaying the results, users can retrieve a list of trending posts (hot or top) from a particular subreddit. For selected posts, detailed metadata such as the title, author, and engagement metrics should be fetched.","import requests

# Define API Key
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


# Function to search Reddit posts based on search term, subreddit, and filters
def search_reddit_posts(search_term, subreddit='', filter_type='posts', time_filter='all', sort_type='relevance'):
    url = ""https://reddit3.p.rapidapi.com/v1/reddit/search""
    querystring = {
        ""search"": search_term,
        ""subreddit"": subreddit,
        ""filter"": filter_type,
        ""timeFilter"": time_filter,
        ""sortType"": sort_type
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""reddit3.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching Reddit search results: {response.status_code}"")
        return None


# Function to retrieve posts from a specific subreddit (hot or top)
def get_subreddit_posts(subreddit_url, filter_type='hot'):
    url = ""https://reddit3.p.rapidapi.com/v1/reddit/posts""
    querystring = {
        ""url"": subreddit_url,
        ""filter"": filter_type
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""reddit3.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching subreddit posts: {response.status_code}"")
        return None


# Function to retrieve details of a specific post
def get_post_details(post_url):
    url = ""https://reddit3.p.rapidapi.com/v1/reddit/post""
    querystring = {
        ""url"": post_url
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""reddit3.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    if response.status_code == 200:
        print(""\nDEBUG: Post details API response data:"")
        print(response.json())  # Debugging output to check the response structure
        return response.json()
    else:
        print(f""Error fetching post details: {response.status_code}"")
        return None


# Main code to demonstrate the functionality
if __name__ == '__main__':
    # Step 1: Search Reddit posts based on a search term
    search_term = ""investing""
    subreddit = ""wallstreetbets""
    search_results = search_reddit_posts(search_term, subreddit, filter_type=""posts"", time_filter=""year"",
                                         sort_type=""relevance"")

    if search_results and 'body' in search_results:
        print(""Search Results:"")
        for idx, post in enumerate(search_results['body'][:5], start=1):  # Limit to top 5 results for clarity
            title = post.get('title', 'No title available')
            author = post.get('author', 'Unknown author')
            score = post.get('score', 'N/A')
            permalink = post.get('permalink', 'No URL available')
            print(f""{idx}. Title: {title} - Author: {author} - Score: {score} - URL: {permalink}"")
    else:
        print(""No search results found."")

    # Step 2: Retrieve top or hot posts from a specific subreddit
    subreddit_url = ""https://www.reddit.com/r/wallstreetbets""
    subreddit_posts = get_subreddit_posts(subreddit_url, filter_type=""hot"")

    if subreddit_posts and 'body' in subreddit_posts:
        print(""\nSubreddit Posts (Hot):"")
        for idx, post in enumerate(subreddit_posts['body'][:5], start=1):  # Limit to top 5 results for clarity
            title = post.get('title', 'No title available')
            author = post.get('author', 'Unknown author')
            score = post.get('score', 'N/A')
            post_url = post.get('url', 'No URL available')
            print(f""{idx}. Title: {title} - Author: {author} - Score: {score} - URL: {post_url}"")

    # Step 3: Get detailed information about a specific post
    if 'body' in subreddit_posts and len(subreddit_posts['body']) > 0:
        first_post_url = subreddit_posts['body'][0].get('url', None)
        if first_post_url:
            post_details = get_post_details(first_post_url)

            if post_details and 'body' in post_details:
                post_info = post_details['body'].get('post', {})

                # Extract and display data if available
                detailed_title = post_info.get('title', 'No title available')
                detailed_author = post_info.get('author', 'Unknown author')
                upvotes = post_info.get('ups', 'N/A')
                comments = post_info.get('num_comments', 'N/A')
                post_content = post_info.get('selftext', 'No content available')

                print(""\nPost Details:"")
                print(f""Title: {detailed_title}"")
                print(f""Author: {detailed_author}"")
                print(f""Upvotes: {upvotes}"")
                print(f""Comments: {comments}"")
                print(f""Content: {post_content}"")
            else:
                print(""No valid data found in the post details response."")
        else:
            print(""No valid URL found for the selected post."")
    else:
        print(""No subreddit posts found."")
","Search Results:
1. Title: After 4 years of investing I made $16 lol - Author: raseC_Ceda - Score: 28979 - URL: /r/wallstreetbets/comments/1ctf81t/after_4_years_of_investing_i_made_16_lol/
2. Title: If investing is so easy, why doesn't everybody do it? - Author: precita - Score: 544 - URL: /r/Bogleheads/comments/1aohea2/if_investing_is_so_easy_why_doesnt_everybody_do_it/
3. Title: [OC] Why you should start investing early in life - Author: PieChartPirate - Score: 19619 - URL: /r/dataisbeautiful/comments/wo6fpu/oc_why_you_should_start_investing_early_in_life/
4. Title: Does Investing Actually Work? - Author: Low_Entry5644 - Score: 96 - URL: /r/stocks/comments/1694h71/does_investing_actually_work/
5. Title: How do I start investing? - Author: MirafuCh - Score: 5 - URL: /r/ValueInvesting/comments/18410ic/how_do_i_start_investing/

Subreddit Posts (Hot):
1. Title: Weekend Discussion Thread for the Weekend of October 25, 2024 - Author: wsbapp - Score: 66 - URL: https://www.reddit.com/r/wallstreetbets/comments/1gc374e/weekend_discussion_thread_for_the_weekend_of/
2. Title: Weekly Earnings thread 10/28 - 11/1 - Author: OSRSkarma - Score: 300 - URL: https://i.redd.it/xy8kbewinywd1.jpeg
3. Title: New York Stock Exchange to extend trading hours to 22 hours a day.  - Author: Maximum_Fill987 - Score: 3941 - URL: https://i.redd.it/fk0gh5kvb0xd1.jpeg
4. Title: Turned $10k into $172k in one trade. You guys told me to sell, so I held - Author: vuw960 - Score: 15484 - URL: https://www.reddit.com/gallery/1gc45oi
5. Title: Just protecting retail traders - Author: King-of-Plebss - Score: 2172 - URL: https://i.redd.it/1ye15t7rg0xd1.jpeg

DEBUG: Post details API response data:
{'meta': {'version': 'v1.0', 'status': 200, 'copywrite': 'https://apicalls.io', 'totalComments': 83}, 'body': {'post': {'approved_at_utc': None, 'subreddit': 'wallstreetbets', 'selftext': 'This post contains content not supported on old Reddit. [Click here to view the full post](https://sh.reddit.com/r/wallstreetbets/comments/1gc374e)', 'user_reports': [], 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Weekend Discussion Thread for the Weekend of October 25, 2024', 'link_flair_richtext': [{'e': 'text', 't': 'Weekend Discussion'}], 'subreddit_name_prefixed': 'r/wallstreetbets', 'hidden': False, 'pwls': 7, 'link_flair_css_class': 'weekend', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1gc374e', 'quarantine': False, 'link_flair_text_color': 'dark', 'upvote_ratio': 0.88, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 63, 'total_awards_received': 0, 'media_embed': [], 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'author_fullname': 't2_qbvp0eq8b', 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': [], 'link_flair_text': 'Weekend Discussion', 'can_mod_post': False, 'score': 63, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': [], 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1729886233, 'link_flair_type': 'richtext', 'wls': 7, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.wallstreetbets', 'allow_live_comments': False, 'selftext_html': '&lt;!-- SC_OFF --&gt;&lt;div class=""md""&gt;&lt;p&gt;This post contains content not supported on old Reddit. &lt;a href=""https://sh.reddit.com/r/wallstreetbets/comments/1gc374e""&gt;Click here to view the full post&lt;/a&gt;&lt;/p&gt;\n&lt;/div&gt;&lt;!-- SC_ON --&gt;', 'likes': None, 'suggested_sort': 'new', 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': True, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_2th52', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '', 'id': '1gc374e', 'is_robot_indexable': True, 'num_duplicates': 0, 'report_reasons': None, 'author': 'wsbapp', 'discussion_type': None, 'num_comments': 3972, 'send_replies': False, 'media': None, 'contest_mode': False, 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/wallstreetbets/comments/1gc374e/weekend_discussion_thread_for_the_weekend_of/', 'stickied': True, 'url': 'https://www.reddit.com/r/wallstreetbets/comments/1gc374e/weekend_discussion_thread_for_the_weekend_of/', 'subreddit_subscribers': 16922418, 'created_utc': 1729886233, 'num_crossposts': 0, 'mod_reports': [], 'is_video': False}, 'post_comments': [{'id': 'ltvwgjo', 'author': 'cryptohorn', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729966270, 'content': 'People at the rally thought they were getting bey-once but got willie Nelson instead ![img](emote|t5_2th52|4271)![img](emote|t5_2th52|4271)![img](emote|t5_2th52|4271)'}, {'id': 'ltvw9yh', 'author': 'OB1KENOB', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729966210, 'content': 'Whiplash doesnâ€™t make any sense. No one on earth would leave Supergirl to play jazz drums.'}, {'id': 'ltvw733', 'author': 'BigBoiBenisBlueBalls', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729966183, 'content': 'My grandpa is in his 80s but still has a full head of hair. Itâ€™s thinned a lot but still good. If he were to take fin and minoxidil would it become thicker? This wonâ€™t happen but Iâ€™m just curious'}, {'id': 'ltvw4xr', 'author': 'Anoos_specialist', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729966163, 'content': '![img](emote|t5_2th52|4260)'}, {'id': 'ltvw3q0', 'author': 'Anoos_specialist', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729966152, 'content': 'Carefully waiting for the next dump. And THEN I POUNCE like a total regard!'}, {'id': 'ltvvv5o', 'author': 'lipmanz', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729966072, 'content': 'David Goggins is a hard worker but I think appeals most to dudes with a humiliation fetish'}, {'id': 'ltvvpbd', 'author': 'NotSoTough-Tony', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729966019, 'content': 'Breaking News: ðŸ¥­ announce if he wins, Ivanka will be releasing her porno films on Pornhub every month during his term\xa0'}, {'id': 'ltvvjr8', 'author': 'AquamDeus', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729965968, 'content': ""Don't care who wins the election, simply trying to profit. Calls or puts on DJT?""}, {'id': 'ltvvfma', 'author': 'oregonLov', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729965929, 'content': 'When tf did talking about the leader of the US become banned?\n\nTariffs gon crash my stock'}, {'id': 'ltvv4ly', 'author': 'DeepSweatyButthole', 'up_votes': 4, 'score': 4, 'likes': '', 'created_utc': 1729965827, 'content': 'Lot of mango banners and flags in front of peoples houses. The time is near ![img](emote|t5_2th52|12787)'}, {'id': 'ltvuqko', 'author': '[deleted]', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729965695, 'content': '[removed]'}, {'id': 'ltvuou3', 'author': 'warlock22041', 'up_votes': 4, 'score': 4, 'likes': '', 'created_utc': 1729965679, 'content': ""AMD 170 please god. I'll never gamble again""}, {'id': 'ltvtzx5', 'author': 'mysuruhuduga', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729965448, 'content': 'I told this bmw dealership to give me 3.99% on this used 2023 3 series with &lt; 5k miles else I would wait until jpow will cut the interest rate in Nov/ Dec meeting\n\nThey are offering 3.99% on 2019-2022, either they agree to my deal else they can duck off'}, {'id': 'ltvtzqa', 'author': 'WildConsideration376', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729965446, 'content': 'War is so bullish..next week run fueled by big cap earnings will send spy to $600.Next week will birth thousandnaires &amp; millionaires..all you have to do is drop otm calls &amp; puts on 5 big stocks'}, {'id': 'ltvtyub', 'author': 'spellbadgrammargood', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729965438, 'content': 'i took 2 ""i voted"" stickers.. does this make me a felon?'}, {'id': 'ltvtynq', 'author': 'smart_doge', 'up_votes': 3, 'score': 3, 'likes': '', 'created_utc': 1729965436, 'content': 'Lunch time ðŸ ![img](emote|t5_2th52|4258)'}, {'id': 'ltvtqh2', 'author': 'Totallynotfakenews', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729965361, 'content': 'CVNA has to be a big earnings dump right?'}, {'id': 'ltvtmr0', 'author': 'DeepSweatyButthole', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729965326, 'content': 'What do you think about starting the bike ride off with a little 53rd and 3rd, Youâ€™re Gonna Kill that girl and then going right into Red Fucken Hot ðŸ¤”'}, {'id': 'ltvtjhy', 'author': 'VisualMod', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729965297, 'content': '#Ban Bet Lost\n\n/u/No_Onion_8612 made a bet that TSLA would go to 200.0 within **5 days** when it was 217.865 and it did not, so they were banned for a week.\n\nTheir record is now 0 wins and 1 losses\n\n[**Join WSB Discord**](http://discord.gg/wsbverse)'}, {'id': 'ltvtb5b', 'author': 'lipmanz', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729965220, 'content': 'When I was in college I had a classmate whoâ€™s dad owned some big company in Brazil, drove a Ferrari to class and had his own beachfront condo at 19â€¦we were playing doubles tennis with 2 hot chicks and he just whipped his thing out and starting running around the court as a â€œjokeâ€, wonder whatever happened to him'}, {'id': 'ltvszaw', 'author': 'Rich265', 'up_votes': -1, 'score': -1, 'likes': '', 'created_utc': 1729965110, 'content': 'Democrats blame Republicans for, â€œtax cuts that led to low revenue levels that increased the debt.â€\n\nLet\'s see.. Tax Cuts took effect in 2018.\n\nTax Revenues for 2018:  $3.33 trillion  \nTax Revenues for 2023:  $4.44 trillion\n\nA 33% increase in tax revenue caused debt to increase?\n\nLet\'s see what the non-partisan Congressional Budget Office report said on the tax cuts..\n\n""CBO also estimated the macroeconomic feedback resulting from the tax act. CBO estimated that the economic changes resulting from the actâ€”not including its effects on debt serviceâ€”would reduce primary deficits by $0.571 trillion over the 2018â€“2028 period. The main reason was that the act would boost taxable income and thus federal revenues.""\n\nOh, they say it would reduce deficits due to boosting federal revenues.. weird!'}, {'id': 'ltvsrxk', 'author': 'strictlyPr1mal', 'up_votes': 4, 'score': 4, 'likes': '', 'created_utc': 1729965043, 'content': ""I will be banned next week, but just know I'm still going to be yoloing on earnings every day""}, {'id': 'ltvsraw', 'author': 'lipmanz', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729965037, 'content': 'IMO the pundits claiming no hike on the 7th are wrong, the whole point is to bring rates down which hasnâ€™t happened, in fact they are practically where they were before the last cut'}, {'id': 'ltvskbg', 'author': 'isolated-cat', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729964974, 'content': 'should i call the cute girl who approached me last night and left me her number? she was texting me last night once she left, so i do believe that she was interested. my gut says to just call her today and get it out of the system. what do i got to lose? right fellas? should i wait another day?'}, {'id': 'ltvsgt4', 'author': 'DeepSweatyButthole', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729964943, 'content': 'We are men! We shit in our pants and grab each others cocks!'}, {'id': 'ltvsfce', 'author': 'Coffee_Buzzzz', 'up_votes': 3, 'score': 3, 'likes': '', 'created_utc': 1729964929, 'content': 'I expect things to change for the better without changing myself or doing anything different ![img](emote|t5_2th52|12787)'}, {'id': 'ltvs8vf', 'author': 'AngusMcTibbins', 'up_votes': 4, 'score': 4, 'likes': '', 'created_utc': 1729964869, 'content': 'Beautiful fall day. Got a fresh cup of coffee sitting on the edge of the tub while I sit on the toilet for a relaxing poop. Life is good, my friends ![img](emote|t5_2th52|8882)'}, {'id': 'ltvs1rg', 'author': 'luoyuke', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729964804, 'content': ""feel like it will be a sell on the news no matter what happens. No one knows how to react, in a situation like this it's always sell first and jump in later""}, {'id': 'ltvs1ej', 'author': 'Slow-Information-847', 'up_votes': -1, 'score': -1, 'likes': '', 'created_utc': 1729964801, 'content': 'stocks will be up big time next Monday. Iran just said they will retaliate.'}, {'id': 'ltvrrvj', 'author': 'Blazzck7', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729964715, 'content': 'Rolling the dice on some otm tesla calls for 3 weeks out \U0001fae1'}, {'id': 'ltvr8rk', 'author': 'goodbuddyedb', 'up_votes': 3, 'score': 3, 'likes': '', 'created_utc': 1729964540, 'content': ""Plunge protection ain't got shit on my puts . Suck it you dumb sheep that think you are 'bulls'""}, {'id': 'ltvqt7g', 'author': 'SwiftSG1', 'up_votes': 3, 'score': 3, 'likes': '', 'created_utc': 1729964403, 'content': 'Election pump. Rate cut pump. Santa pump. Then fomo pump because itâ€™s up so much. ![img](emote|t5_2th52|4258)'}, {'id': 'ltvqeew', 'author': 'GenericUser4104', 'up_votes': 4, 'score': 4, 'likes': '', 'created_utc': 1729964271, 'content': 'Bers furiously refreshing BBC News for more war porn'}, {'id': 'ltvqb49', 'author': 'wildcat1055', 'up_votes': 5, 'score': 5, 'likes': '', 'created_utc': 1729964242, 'content': 'I thought I was one of the worst, horniest and most degenerate gamblers out there, but I just found out about the regards losing money on Taylor Mathis meme coin. There are levels to this shit.\n\nGod Bless America!'}, {'id': 'ltvq4oz', 'author': 'Razvanell95', 'up_votes': 5, 'score': 5, 'likes': '', 'created_utc': 1729964186, 'content': 'I sold a big part of my portfolio right before this great month. I want to stop smelling like disappointment. ![img](emote|t5_2th52|4260)\n\nCan someone recommend me a good fragrance?'}, {'id': 'ltvpvnu', 'author': 'necarpenter417', 'up_votes': 4, 'score': 4, 'likes': '', 'created_utc': 1729964105, 'content': 'Dang. Somehow I lost 12k last week.'}, {'id': 'ltvpv5l', 'author': 'SilentHillFan12', 'up_votes': 4, 'score': 4, 'likes': '', 'created_utc': 1729964101, 'content': 'So what - they ban you now just for posting 600 times a day in the daily thread? This fucking country'}, {'id': 'ltvpeln', 'author': 'rufus170', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729963951, 'content': 'When is LUNR gonna moon? So far iâ€™ve only seen jumps to around 8,50 but iâ€™ve been expecting much more, just like a lot of the ppl on this sub\n\nIâ€™m patient, itâ€™s just taking longer than what i was thinking'}, {'id': 'ltvp9wd', 'author': 'EnginrA', 'up_votes': 6, 'score': 6, 'likes': '', 'created_utc': 1729963909, 'content': 'They used to call me Johnny Thickloads ![img](emote|t5_2th52|4275)'}, {'id': 'ltvovtr', 'author': 'rc5193', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729963783, 'content': 'Who else pops a large chunk of their port into corn for the weekend for the pumps? Is this addiction? ![img](emote|t5_2th52|4271)'}, {'id': 'ltvohan', 'author': 'BigBoiBenisBlueBalls', 'up_votes': -3, 'score': -3, 'likes': '', 'created_utc': 1729963650, 'content': 'Anyone excited for Tyler the creators new album? I havenâ€™t listened to any of the songs from it yet but I donâ€™t really care for it or him anymore. What bugs me about him is his clothing brand. Golf wang. Shit is expensive AND shit quality. The t shirts felt like paper. Itâ€™s awful. Also he kept releasing terrible designs for at least 2 years. He kept putting out shit so I stopped listening to him'}, {'id': 'ltvo83y', 'author': 'sloanemonroe', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729963568, 'content': 'I have covid. Anyone want to come over and hang out?'}, {'id': 'ltvo43f', 'author': 'Diamondhands4dagainz', 'up_votes': 3, 'score': 3, 'likes': '', 'created_utc': 1729963531, 'content': 'Watching top gun, it better have my favourite plane ever: F-22 raptor $LMT'}, {'id': 'ltvnv12', 'author': 'StockMarketDavid', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729963448, 'content': 'I invested 15K at 10.50$ into the ****ware stock that seems to follow DJT ðŸ‘€ im thinking leading into election it go up then fall off day before election and the week after ( regardless of winner ) what yall thinking lol'}, {'id': 'ltvnttd', 'author': 'VisualMod', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729963437, 'content': '#Ban Bet Lost\n\n/u/Cantthinkofacoolnam made a bet that NOW would go to 1000.0 within **1 week** when it was 922.375 and it did not, so they were banned for a week.\n\nTheir record is now 1 wins and 2 losses\n\n[**Join WSB Discord**](http://discord.gg/wsbverse)'}, {'id': 'ltvnq8d', 'author': 'ScoopityWoop89', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729963404, 'content': 'Are apple calls regarded?'}, {'id': 'ltvnm5p', 'author': 'Funny_Story2759', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729963366, 'content': 'when i roll these options to shares after moass ima petition to rename purecycle to PlasticStop'}, {'id': 'ltvnhgs', 'author': 'mcs5280', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729963325, 'content': ""tfw your choice in the Halloween candy basket is a candy that is obviously poisoned and bad for you or a candy that isn't obviously poisoned but\xa0you strongly dislike it and you'd rather not choose either""}, {'id': 'ltvnbxn', 'author': 'joe_bidens_underwear', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729963275, 'content': '\nThink MARA gets back to $20 soon?'}, {'id': 'ltvn8gz', 'author': 'WSBNon-Believer', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729963244, 'content': 'All around me are familiar faces'}, {'id': 'ltvn858', 'author': 'Learningcurvve', 'up_votes': 3, 'score': 3, 'likes': '', 'created_utc': 1729963241, 'content': '#$311.53\n\nTSLA hits a $1T market cap'}, {'id': 'ltvn1r4', 'author': 'alittlerogue', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729963185, 'content': 'Thinking aloud, in my play acc I am up 45% on NVDA shares with avg cost $99. I have $17k loss in fucking POS Nio. Should I sell both and collect profit or sell Nio and ride out NVDA? Still have NVDA in my retirement since 2017. Just weary on elections playing out and the selling chips to China sentiment.'}, {'id': 'ltvmtds', 'author': 'spy69eod', 'up_votes': 3, 'score': 3, 'likes': '', 'created_utc': 1729963108, 'content': 'Market sell off has just begun bols. Enjoy being exit liquidity'}, {'id': 'ltvlp5t', 'author': 'Ok_Ambassador_4311', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729962743, 'content': 'https://www.instagram.com/reel/DArBEPjPHDA/?igsh=Y3N0dWRpdHdmZTNz me at Wall St in 5 years'}, {'id': 'ltvlevn', 'author': 'Accomplished_Dot9815', 'up_votes': 4, 'score': 4, 'likes': '', 'created_utc': 1729962651, 'content': 'Sup faggets'}, {'id': 'ltvkwp3', 'author': 'No-Improvement3164', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729962485, 'content': 'What are the chances of Meta Netflix and NVDA ever selling off like they did a couple years ago.'}, {'id': 'ltvkuhh', 'author': 'ChildhoodOpening3360', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729962465, 'content': 'Started my account at the beg of month with 1.5k. Consistently was up until 13.8k then got cocky and bought dumb shit. Lost 11k of it yesterday and for some reason bought calls going into the weekend. Usually only make money flipping between 945am and 11am each day but might be fucked with these series of abysmal decisions.\n\nHolding options for any length of time is how I lose my port'}, {'id': 'ltvjylc', 'author': 'E39_CBX', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729962175, 'content': 'I need to turn 8k into 30k by EOY whatâ€™s the play'}, {'id': 'ltvjx1e', 'author': 'Ok_Pineapple_5700', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729962160, 'content': ""Don't be so humble, you're not that great""}, {'id': 'ltvj4ab', 'author': 'cinJESUS', 'up_votes': -1, 'score': -1, 'likes': '', 'created_utc': 1729961904, 'content': 'can this Elon fucker let me invest in spacex? ![img](emote|t5_2th52|27421)![img](emote|t5_2th52|27421)![img](emote|t5_2th52|27421)'}, {'id': 'ltvitop', 'author': 'yeetwagon', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729961809, 'content': 'Canâ€™t wait to buy $7,000 of RTX on January 1st ![img](emote|t5_2th52|4258)'}, {'id': 'ltvim0h', 'author': 'Nomynametoday', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729961740, 'content': 'How do u guys play weekly calls, if the companyâ€™s earnings is on friday?'}, {'id': 'ltvi984', 'author': 'Heedunk', 'up_votes': 5, 'score': 5, 'likes': '', 'created_utc': 1729961625, 'content': 'The next 1.5 weeks are going to be crazy with tech earnings, Halloween, and the election!'}, {'id': 'ltvi5df', 'author': 'ManBearPig_1983', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729961590, 'content': 'McDonaldâ€™s frosty machine down AGAIN!! Wtf'}, {'id': 'ltvi4fr', 'author': 'BigDiplomacy', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729961582, 'content': ""Nothing like a relaxing Saturday day, meal-prepping some protein packed meals for your wife's boyfriend.""}, {'id': 'ltvi3kp', 'author': 'LarryStink', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729961574, 'content': 'Going to close all open positions this week, except my TLT puts, and open a single SPX 1wte long condor betting on volatility from earnings this week. Skewed with lower put deltas as I see risk to the downside as greater and the move would be larger and faster than to the upside. And I will hesitate to get directional with any size until after this week is over.\xa0'}, {'id': 'ltvi0w2', 'author': 'Ok_Pineapple_5700', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729961551, 'content': ""Easy to spot when he's full of shit""}, {'id': 'ltvhhp1', 'author': 'digdugbug', 'up_votes': 4, 'score': 4, 'likes': '', 'created_utc': 1729961377, 'content': 'If I had a million could have lost them yesterday! Glad I only had 5k.'}, {'id': 'ltvh8e1', 'author': 'Resident_Pudding_681', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729961293, 'content': 'sad boi hours already startin I see'}, {'id': 'ltvgs14', 'author': 'Slizano12', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729961144, 'content': ""Still waiting for we ride to go public and make an official. In the meantime been buying up chubb and Amazon because it's almost Christmas I have a feeling Amazon is just got rocket along with Alibaba and Bank of America because everybody's going to get swipe happy. Plus I don't know why I have a feeling that Bitcoin my skyrocket.""}, {'id': 'ltvgjbp', 'author': 'LarryStink', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729961066, 'content': 'Had I not been a brokie and used more money for that trade, say 17,000 instead of 170, I would have made 200,000$ in a day instead of 2,000'}, {'id': 'ltvfy5z', 'author': 'BushLov3r', 'up_votes': 3, 'score': 3, 'likes': '', 'created_utc': 1729960873, 'content': 'Sentiment is pretty shit on google, might be the perfect call play because of it'}, {'id': 'ltvfnto', 'author': 'yaboiRich', 'up_votes': 4, 'score': 4, 'likes': '', 'created_utc': 1729960779, 'content': 'Just jacked off. What do now?'}, {'id': 'ltvf8dr', 'author': 'Slizano12', 'up_votes': 1, 'score': 1, 'likes': '', 'created_utc': 1729960640, 'content': 'This last week the market was all over the place wtf!!!'}, {'id': 'ltvf3yi', 'author': 'Hwangoutpltr', 'up_votes': 5, 'score': 5, 'likes': '', 'created_utc': 1729960599, 'content': '#ðŸ¥­'}, {'id': 'ltveuwi', 'author': 'UsernameTaken_123', 'up_votes': 7, 'score': 7, 'likes': '', 'created_utc': 1729960517, 'content': 'Penis day'}, {'id': 'ltvepp6', 'author': 'felipesizzosmoes', 'up_votes': 3, 'score': 3, 'likes': '', 'created_utc': 1729960469, 'content': 'Back day'}, {'id': 'ltvedf5', 'author': 'Glowie-in-Chief', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729960358, 'content': '#The WEAVE'}, {'id': 'ltveb9d', 'author': 'nerdy-nate', 'up_votes': 2, 'score': 2, 'likes': '', 'created_utc': 1729960339, 'content': 'I bought a TOGA costume from Spirit Halloween for $20 last night and it FUCKINNNN SUCKSSS (I know, I know, $20). Anyways, there is a costume/halloween party theme at this nightclub me and my friends are going to tonight and I need a new costume ASAP! Is anyone here doing a low-effort costume or anyone have any ideas on what I could quickly throw together for a half-ass costume? ![img](emote|t5_2th52|4271)'}, {'id': 'ltve9v6', 'author': 'MSTR_CallsAtOpen', 'up_votes': 3, 'score': 3, 'likes': '', 'created_utc': 1729960326, 'content': 'I loaded up on MSTR calls for over the weekend because I believe Bitcoin could be about to break out. If I am right I am rich'}, {'id': 'ltve0wa', 'author': 'DeepSweatyButthole', 'up_votes': 0, 'score': 0, 'likes': '', 'created_utc': 1729960245, 'content': '#McCREAM!!!!!!!!!'}, {'id': 'ltvdzir', 'author': 'isolated-cat', 'up_votes': 3, 'score': 3, 'likes': '', 'created_utc': 1729960233, 'content': 'anyone else feeling that mag7 earnings will be mild and not very strong bringing the market further down?'}, {'id': 'ltvcpb6', 'author': '', 'up_votes': '', 'score': '', 'likes': '', 'created_utc': '', 'content': ''}]}}

Post Details:
Title: Weekend Discussion Thread for the Weekend of October 25, 2024
Author: wsbapp
Upvotes: 63
Comments: 3972
Content: This post contains content not supported on old Reddit. [Click here to view the full post](https://sh.reddit.com/r/wallstreetbets/comments/1gc374e)
",1,1,8,28824
"The service allows users to retrieve and explore global language information, get a list of all available languages, and see the country information for each language. In addition, users can choose to view a list of all countries in the world, regardless of language, to support a wider range of references. The service validates the API response format to ensure that it can accommodate different data return structures.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_all_languages():
    url = ""https://auto-parts-catalog.p.rapidapi.com/languages/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""auto-parts-catalog.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        return response.json()
    else:
        print(f""Error fetching languages: {response.status_code}"")
        return None

def get_all_countries():
    url = ""https://auto-parts-catalog.p.rapidapi.com/countries/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""auto-parts-catalog.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        data = response.json()
        # Debug information for data structure
        return data
    else:
        print(f""Error fetching countries: {response.status_code}"")
        return None

def get_countries_by_language(lang_id):
    url = f""https://auto-parts-catalog.p.rapidapi.com/countries/list-countries-by-lang-id/{lang_id}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""auto-parts-catalog.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    if response.status_code == 200:
        data = response.json()
        # Debug information for data structure
        return data
    else:
        print(f""Error fetching countries for language ID {lang_id}: {response.status_code}"")
        return None

if __name__ == '__main__':
    # Step 1: Retrieve a list of all available languages
    languages = get_all_languages()
    if languages:
        print(""### Available Languages ###"")
        # Show up to 20 languages
        for idx, lang in enumerate(languages[:20], start=1):
            print(f""{idx}. Language: {lang.get('name', 'Unknown')} - ID: {lang.get('lngId', 'N/A')}"")

    # Step 2: Retrieve a list of all countries
    all_countries = get_all_countries()
    if all_countries and isinstance(all_countries, dict) and 'countries' in all_countries:
        print(""\n### All Countries ###"")
        # Show up to 20 countries
        for idx, country in enumerate(all_countries['countries'][:20], start=1):
            print(f""{idx}. Country: {country.get('countryName', 'Unknown')} - Code: {country.get('couCode', 'N/A')}"")
    else:
        print(""\nNo valid country data available."")

    # Step 3: Retrieve countries associated with a selected language ID
    if languages and len(languages) > 0:
        first_lang_id = languages[0].get('lngId')
        if first_lang_id:
            countries_by_language = get_countries_by_language(first_lang_id)
            if countries_by_language and isinstance(countries_by_language, dict) and 'countries' in countries_by_language:
                print(f""\n### Countries for Language ID {first_lang_id} ###"")
                # Show up to 20 countries associated with the language
                for idx, country in enumerate(countries_by_language['countries'][:20], start=1):
                    print(f""{idx}. Country: {country.get('countryName', 'Unknown')} - Code: {country.get('couCode', 'N/A')}"")
            else:
                print(f""\nNo valid country data available for language ID {first_lang_id}."")
",Fail,-1,-1,31,108686
"This service retrieves detailed information about vehicle types and engine specifications. It first lists all available vehicle types, each identified by a unique ID and category. For a selected vehicle type, the service fetches technical specifications such as manufacturer, model, and engine attributes. Additionally, the service allows for retrieving specific engine details for a selected model, focusing on attributes like engine power, production dates, fuel type, and engine codes.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def list_vehicle_types():
    url = ""https://auto-parts-catalog.p.rapidapi.com/types/list-vehicles-type""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""auto-parts-catalog.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()


def get_vehicle_type_details(vehicle_id, manufacturer_id, lang_id, country_filter_id, type_id):
    url = f""https://auto-parts-catalog.p.rapidapi.com/types/vehicle-type-details/{vehicle_id}/manufacturer-id/{manufacturer_id}/lang-id/{lang_id}/country-filter-id/{country_filter_id}/type-id/{type_id}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""auto-parts-catalog.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()


def get_vehicle_engine_types(model_series_id, manufacturer_id, lang_id, country_filter_id, type_id):
    url = f""https://auto-parts-catalog.p.rapidapi.com/types/list-vehicles-types/{model_series_id}/manufacturer-id/{manufacturer_id}/lang-id/{lang_id}/country-filter-id/{country_filter_id}/type-id/{type_id}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""auto-parts-catalog.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()


def display_engine_types(engine_data, max_examples=20):
    if 'modelTypes' in engine_data:
        for idx, engine in enumerate(engine_data['modelTypes'][:max_examples], start=1):
            print(f""\n### Example {idx}: {engine['typeEngineName']} ###"")
            print(f""- Manufacturer: {engine.get('manufacturerName', 'Unknown')}"")
            print(f""- Model: {engine.get('modelName', 'Unknown')}"")
            print(f""- Engine Name: {engine.get('typeEngineName', 'Unknown')}"")
            print(
                f""- Production Date: {engine.get('constructionIntervalStart', 'Unknown')} - {engine.get('constructionIntervalEnd', 'Unknown')}"")
            print(f""- Power: {engine.get('powerKw', 'N/A')} kW / {engine.get('powerPs', 'N/A')} PS"")
            print(f""- Fuel Type: {engine.get('fuelType', 'Unknown')}"")
            print(f""- Body Type: {engine.get('bodyType', 'Unknown')}"")
            print(f""- Number of Cylinders: {engine.get('numberOfCylinders', 'Unknown')}"")
            print(
                f""- Engine Capacity: {engine.get('capacityLt', 'Unknown')} liters ({engine.get('capacityTech', 'Unknown')} cc)"")
            print(f""- Engine Codes: {engine.get('engineCodes', 'Unknown')}"")
            print(""---"")


if __name__ == '__main__':
    # Step 1: Retrieve a list of all available vehicle types
    vehicle_types = list_vehicle_types()
    print(""### Available Vehicle Types ###"")
    if vehicle_types:
        for idx, v_type in enumerate(vehicle_types[:20], start=1):
            print(f""{idx}. Type: {v_type['vehicleType']} - ID: {v_type['id']}"")

    # Step 2: Retrieve detailed information for a specific vehicle type
    # Assuming the first vehicle type from the list for demonstration
    if vehicle_types and len(vehicle_types) > 0:
        vehicle_id = vehicle_types[0]['id']
        manufacturer_id = ""184""  # Example manufacturer ID
        lang_id = ""4""  # English language
        country_filter_id = ""62""  # Example for Germany
        type_id = ""1""  # Type ID for Automobile
        vehicle_details = get_vehicle_type_details(vehicle_id, manufacturer_id, lang_id, country_filter_id, type_id)
        print(""\n### Vehicle Type Details ###"")
        if 'vehicleTypeDetails' in vehicle_details:
            details = vehicle_details['vehicleTypeDetails']
            print(f""- Brand: {details.get('brand', 'Unknown')}"")
            print(f""- Model Type: {details.get('modelType', 'Unknown')}"")
            print(f""- Engine Type: {details.get('typeEngine', 'Unknown')}"")
            print(f""- Production Date: {details.get('constructionIntervalStart', 'Unknown')} - {details.get('constructionIntervalEnd', 'Unknown')}"")
            print(f""- Power: {details.get('powerKw', 'N/A')} kW / {details.get('powerPs', 'N/A')} PS"")
            print(f""- Body Type: {details.get('bodyType', 'Unknown')}"")
            print(f""- Fuel Type: {details.get('fuelType', 'Unknown')}"")
            print(f""- Engine Codes: {details.get('engCodes', 'Unknown')}"")
        else:
            print(""No vehicle details available."")

    # Step 3: Retrieve engine types for a specific model series
    # Example model series parameters for KIA CEE'D Hatchback (ED)
    model_series_id = ""5626""  # Example model series ID
    engine_types = get_vehicle_engine_types(model_series_id, manufacturer_id, lang_id, country_filter_id, type_id)

    # Display the engine details in a formatted way
    display_engine_types(engine_types)
","### Available Vehicle Types ###
1. Type: Automobile - ID: 1
2. Type: Commercial-Vehicles - ID: 2
3. Type: Moto - ID: 3

### Vehicle Type Details ###
- Brand: OPEL
- Model Type: CORSA D (S07)
- Engine Type: 1.4 (L08, L68)
- Production Date: 2006-07-01 - 2014-08-01
- Power: 66.0000 kW / 90.0000 PS
- Body Type: Hatchback
- Fuel Type: Petrol
- Engine Codes: Z 14 XEP

### Example 1: 1.4 CVVT ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.4 CVVT
- Production Date: 2009-08-01 - 2012-12-01
- Power: 66.0000 kW / 90.0000 PS
- Fuel Type: Petrol
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.4000 liters (1396.0000 cc)
- Engine Codes: G4FA
---

### Example 2: 1.6 CVVT ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.6 CVVT
- Production Date: 2010-10-01 - 2012-12-01
- Power: 92.0000 kW / 125.0000 PS
- Fuel Type: Petrol
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.6000 liters (1591.0000 cc)
- Engine Codes: G4FC
---

### Example 3: 1.6 CRDi 128 ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.6 CRDi 128
- Production Date: 2010-10-01 - 2012-12-01
- Power: 94.0000 kW / 128.0000 PS
- Fuel Type: Diesel
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.6000 liters (1582.0000 cc)
- Engine Codes: D4FB
---

### Example 4: 1.6 ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.6
- Production Date: 2007-09-01 - 2012-12-01
- Power: 85.0000 kW / 115.0000 PS
- Fuel Type: Petrol
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.6000 liters (1591.0000 cc)
- Engine Codes: G4FC
---

### Example 5: 1.4 ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.4
- Production Date: 2006-12-01 - 2012-12-01
- Power: 77.0000 kW / 105.0000 PS
- Fuel Type: Petrol
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.4000 liters (1396.0000 cc)
- Engine Codes: G4FA
---

### Example 6: 1.4 ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.4
- Production Date: 2006-12-01 - 2012-12-01
- Power: 80.0000 kW / 109.0000 PS
- Fuel Type: Petrol
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.4000 liters (1396.0000 cc)
- Engine Codes: G4FA
---

### Example 7: 1.6 ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.6
- Production Date: 2006-12-01 - 2012-12-01
- Power: 90.0000 kW / 122.0000 PS
- Fuel Type: Petrol
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.6000 liters (1591.0000 cc)
- Engine Codes: G4FC
---

### Example 8: 2.0 ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 2.0
- Production Date: 2006-12-01 - 2012-12-01
- Power: 105.0000 kW / 143.0000 PS
- Fuel Type: Petrol
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 2.0000 liters (1975.0000 cc)
- Engine Codes: G4GC
---

### Example 9: 1.6 CRDi 90 ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.6 CRDi 90
- Production Date: 2006-12-01 - 2012-12-01
- Power: 66.0000 kW / 90.0000 PS
- Fuel Type: Diesel
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.6000 liters (1582.0000 cc)
- Engine Codes: D4FB
---

### Example 10: 1.6 CRDi 115 ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.6 CRDi 115
- Production Date: 2006-12-01 - 2012-12-01
- Power: 85.0000 kW / 115.0000 PS
- Fuel Type: Diesel
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.6000 liters (1582.0000 cc)
- Engine Codes: D4FB
---

### Example 11: 2.0 CRDi 140 ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 2.0 CRDi 140
- Production Date: 2007-09-01 - 2012-12-01
- Power: 103.0000 kW / 140.0000 PS
- Fuel Type: Diesel
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 2.0000 liters (1991.0000 cc)
- Engine Codes: D4EA
---

### Example 12: 1.6 ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.6
- Production Date: 2008-02-01 - 2012-12-01
- Power: 93.0000 kW / 126.0000 PS
- Fuel Type: Petrol
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.6000 liters (1591.0000 cc)
- Engine Codes: G4FC
---

### Example 13: 2.0 CRDi ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 2.0 CRDi
- Production Date: 2007-06-01 - 2012-12-01
- Power: 100.0000 kW / 136.0000 PS
- Fuel Type: Diesel
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 2.0000 liters (1991.0000 cc)
- Engine Codes: D4EA
---

### Example 14: 1.6 LPG ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.6 LPG
- Production Date: 2007-06-01 - 2011-12-01
- Power: 91.0000 kW / 124.0000 PS
- Fuel Type: Petrol/Liquified Petroleum Gas (LPG)
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.6000 liters (1591.0000 cc)
- Engine Codes: G4FC
---

### Example 15: 1.4 LPG ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.4 LPG
- Production Date: 2009-01-01 - 2010-12-01
- Power: 80.0000 kW / 109.0000 PS
- Fuel Type: Petrol/Liquified Petroleum Gas (LPG)
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.4000 liters (1396.0000 cc)
- Engine Codes: G4FA
---

### Example 16: 1.6 LPG ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.6 LPG
- Production Date: 2011-06-01 - 2011-12-01
- Power: 87.0000 kW / 118.0000 PS
- Fuel Type: Petrol/Liquified Petroleum Gas (LPG)
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.6000 liters (1591.0000 cc)
- Engine Codes: G4FC
---

### Example 17: 1.4 LPG ###
- Manufacturer: KIA
- Model: CEE'D Hatchback (ED)
- Engine Name: 1.4 LPG
- Production Date: 2011-06-01 - 2012-04-01
- Power: 64.0000 kW / 87.0000 PS
- Fuel Type: Petrol/Liquified Petroleum Gas (LPG)
- Body Type: Hatchback
- Number of Cylinders: 4
- Engine Capacity: 1.4000 liters (1396.0000 cc)
- Engine Codes: G4FA
---",1,1,9,24642
"Users can query a list of countries that speak that language with a specific language ID and retrieve related historical events based on a specified year. If no eligible country or historical event is found, the system will kindly inform the user ""No valid country data"" or ""No historical event found for that year."" The service is designed to help users filter important historical information by language and year.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_countries_by_language_id(lang_id):
    url = f""https://auto-parts-catalog.p.rapidapi.com/countries/list-countries-by-lang-id/{lang_id}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""auto-parts-catalog.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def get_historical_events_by_year(year):
    url = ""https://world-history-timeline.p.rapidapi.com/History-By-Year""
    querystring = {""year"": year}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""world-history-timeline.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def display_countries(countries_data, max_display=20):
    if 'countries' in countries_data:
        print(""\n### Countries for Specified Language ID ###"")
        for idx, country in enumerate(countries_data['countries'][:max_display], start=1):
            print(f""{idx}. {country['countryName']} (Code: {country['couCode']})"")
    else:
        print(""No countries found for the specified language ID."")

def display_historical_events(events_data):
    if 'results' in events_data:
        print(""\n### Historical Events ###"")
        for event in events_data['results']:
            print(f""- Year: {event['Year']}"")
            print(f""  Event: {event['Event']}"")
            print(f""  Event ID: {event['id']}"")
            print(""---"")
    else:
        print(""No historical events found for the specified year."")

if __name__ == '__main__':
    # Step 1: Retrieve all countries based on a specified language ID
    lang_id = ""4""  # Example language ID for English
    countries = get_countries_by_language_id(lang_id)
    display_countries(countries)

    # Step 2: Retrieve historical events for a specific year
    year = ""400""  # Example year
    historical_events = get_historical_events_by_year(year)
    display_historical_events(historical_events)
","### Countries for Specified Language ID ###
1. Austria (Code: A)
2. Yemen (People's Democratic Republic) (Code: ADN)
3. Except Europe (Code: AEU)
4. Afghanistan (Code: AFG)
5. Anguilla (Code: AIA)
6. Africa (Code: AK)
7. Albania (Code: AL)
8. Central America (Code: AM)
9. Netherlands Antilles (Code: AN)
10. Andorra (Code: AND)
11. Angola (Code: ANG)
12. Asia / Pacific (Code: APA)
13. Antarctica (Code: AQ)
14. Armenia (Code: ARM)
15. South America (Code: AS)
16. ASEAN (Code: ASE)
17. American Samoa (Code: ASM)
18. Antigua (Code: ATG)
19. Australia (Code: AUS)
20. Aruba (Code: AW)

### Historical Events ###
- Year: 400
  Event: Begins the decline of Maya culture and language in some parts of the highlands.
  Event ID: 215
---",1,1,20,52080
"I want to create a service that retrieves yarn brands and their details, including brand name, brand ID, number of available yarns, and number of color variations. In addition, the service should support the retrieval of a list of available yarn weights and display each weight category and its corresponding yarn quantity. Users can use the service to browse different yarn brands and filter according to the desired yarn weight to find specific yarn types more efficiently.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_yarn_brands():
    url = ""https://yarn-colorways.p.rapidapi.com/v1/brands""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""yarn-colorways.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def get_yarn_weights():
    url = ""https://yarn-colorways.p.rapidapi.com/v1/weights""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""yarn-colorways.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

if __name__ == '__main__':
    # Step 1: Retrieve a list of yarn brands
    brands_data = get_yarn_brands()
    print(""Yarn Brands:"")
    if brands_data and 'data' in brands_data:
        if brands_data['data']:
            for idx, brand in enumerate(brands_data['data'], start=1):
                print(f""{idx}. Brand: {brand.get('brandName', 'Unknown')} - ID: {brand.get('brandId', 'N/A')} - Yarns: {brand.get('yarns', 0)} - Colorways: {brand.get('colorways', 0)}"")
        else:
            print(""No yarn brand data found."")
    else:
        print(""No yarn brand data found."")

    # Step 2: Retrieve yarn weights for filtering or categorizing yarns
    weights_data = get_yarn_weights()
    print(""\nYarn Weights:"")
    if weights_data and 'data' in weights_data:
        if weights_data['data']:
            for idx, weight in enumerate(weights_data['data'], start=1):
                print(f""{idx}. Weight: {weight.get('name', 'Unknown')} - ID: {weight.get('id', 'N/A')} - Yarns: {weight.get('yarns', 0)}"")
        else:
            print(""No yarn weight data found."")
    else:
        print(""No yarn weight data found."")
",Fail,-1,-1,32,83808
"Users calculate a new date by adding or subtracting a specified number of business days from a given start date, taking into account holidays and non-working days in a particular country. In addition, a list of non-working days (such as holidays) between two dates in the specified country should be retrieved. Allows users to calculate future or past work dates and learn which dates are considered invalid in a given country.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def calculate_new_date(country_code, start_date, increment):
    url = ""https://working-days.p.rapidapi.com/1.3/add_working_days""
    querystring = {
        ""country_code"": country_code,
        ""start_date"": start_date,
        ""increment"": str(increment)
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""working-days.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def get_non_working_days(country_code, start_date, end_date):
    url = ""https://working-days.p.rapidapi.com/1.3/list_non_working_days""
    querystring = {
        ""country_code"": country_code,
        ""start_date"": start_date,
        ""end_date"": end_date
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""working-days.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def display_new_date(data):
    print(""\n### New Date Calculation ###"")
    end_date = data.get('end_date', 'N/A')
    working_days = data.get('working_days', {})
    public_holidays = data.get('public_holidays', {})

    print(f""- New Date After Increment: {end_date}"")
    print(f""- Total Working Days in Period: {working_days.get('total', 'N/A')}"")
    print(f""- Work Hours in Period: {working_days.get('work_hours', 'N/A')}"")
    print(f""- Public Holidays in Period: {public_holidays.get('total', 'N/A')}"")
    if 'list' in public_holidays:
        for holiday in public_holidays['list']:
            print(f""  - {holiday['date']}: {holiday['description']}"")


def display_non_working_days(data):
    print(""\n### Non-Working Days ###"")
    non_working_days = data.get('non_working_days', [])

    if non_working_days:
        for day in non_working_days:
            date = day.get('date', 'N/A')
            description = day.get('description', 'N/A')
            day_type = day.get('type', 'N/A')
            day_type_desc = ""Weekend"" if day_type == 1 else ""Public Holiday"" if day_type == 2 else ""Special Day""
            print(f""- {date} ({day_type_desc}): {description}"")
    else:
        print(""No non-working days found for the specified period."")


if __name__ == '__main__':
    # Step 1: Calculate a new date by adding 10 working days from the given start date
    country_code = ""CA""
    start_date = ""2023-10-01""
    increment = 10
    new_date_info = calculate_new_date(country_code, start_date, increment)
    display_new_date(new_date_info)

    # Step 2: Retrieve non-working days in Australia between two dates
    country_code = ""AU""
    start_date = ""2023-01-01""
    end_date = ""2023-12-31""
    non_working_days_info = get_non_working_days(country_code, start_date, end_date)
    display_non_working_days(non_working_days_info)
","### New Date Calculation ###
- New Date After Increment: 2023-10-16
- Total Working Days in Period: 10
- Work Hours in Period: 80
- Public Holidays in Period: 1
  - 2023-10-09: Thanksgiving

### Non-Working Days ###
- 2023-01-01 (Special Day): New Year's Day
- 2023-01-02 (Public Holiday): New Year's Day
- 2023-01-07 (Weekend): weekend day
- 2023-01-08 (Weekend): weekend day
- 2023-01-14 (Weekend): weekend day
- 2023-01-15 (Weekend): weekend day
- 2023-01-21 (Weekend): weekend day
- 2023-01-22 (Weekend): weekend day
- 2023-01-26 (Public Holiday): Australia Day
- 2023-01-28 (Weekend): weekend day
- 2023-01-29 (Weekend): weekend day
- 2023-02-04 (Weekend): weekend day
- 2023-02-05 (Weekend): weekend day
- 2023-02-11 (Weekend): weekend day
- 2023-02-12 (Weekend): weekend day
- 2023-02-18 (Weekend): weekend day
- 2023-02-19 (Weekend): weekend day
- 2023-02-25 (Weekend): weekend day
- 2023-02-26 (Weekend): weekend day
- 2023-03-04 (Weekend): weekend day
- 2023-03-05 (Weekend): weekend day
- 2023-03-11 (Weekend): weekend day
- 2023-03-12 (Weekend): weekend day
- 2023-03-13 (Public Holiday): Canberra Day
- 2023-03-18 (Weekend): weekend day
- 2023-03-19 (Weekend): weekend day
- 2023-03-25 (Weekend): weekend day
- 2023-03-26 (Weekend): weekend day
- 2023-04-01 (Weekend): weekend day
- 2023-04-02 (Weekend): weekend day
- 2023-04-07 (Public Holiday): Good Friday
- 2023-04-08 (Special Day): Easter Saturday
- 2023-04-09 (Special Day): Easter Sunday
- 2023-04-10 (Public Holiday): Easter Monday
- 2023-04-15 (Weekend): weekend day
- 2023-04-16 (Weekend): weekend day
- 2023-04-22 (Weekend): weekend day
- 2023-04-23 (Weekend): weekend day
- 2023-04-25 (Public Holiday): Anzac Day
- 2023-04-29 (Weekend): weekend day
- 2023-04-30 (Weekend): weekend day
- 2023-05-06 (Weekend): weekend day
- 2023-05-07 (Weekend): weekend day
- 2023-05-13 (Weekend): weekend day
- 2023-05-14 (Weekend): weekend day
- 2023-05-20 (Weekend): weekend day
- 2023-05-21 (Weekend): weekend day
- 2023-05-27 (Weekend): weekend day
- 2023-05-28 (Weekend): weekend day
- 2023-05-29 (Public Holiday): Reconciliation Day
- 2023-06-03 (Weekend): weekend day
- 2023-06-04 (Weekend): weekend day
- 2023-06-10 (Weekend): weekend day
- 2023-06-11 (Weekend): weekend day
- 2023-06-12 (Public Holiday): King's Birthday
- 2023-06-17 (Weekend): weekend day
- 2023-06-18 (Weekend): weekend day
- 2023-06-24 (Weekend): weekend day
- 2023-06-25 (Weekend): weekend day
- 2023-07-01 (Weekend): weekend day
- 2023-07-02 (Weekend): weekend day
- 2023-07-08 (Weekend): weekend day
- 2023-07-09 (Weekend): weekend day
- 2023-07-15 (Weekend): weekend day
- 2023-07-16 (Weekend): weekend day
- 2023-07-22 (Weekend): weekend day
- 2023-07-23 (Weekend): weekend day
- 2023-07-29 (Weekend): weekend day
- 2023-07-30 (Weekend): weekend day
- 2023-08-05 (Weekend): weekend day
- 2023-08-06 (Weekend): weekend day
- 2023-08-12 (Weekend): weekend day
- 2023-08-13 (Weekend): weekend day
- 2023-08-19 (Weekend): weekend day
- 2023-08-20 (Weekend): weekend day
- 2023-08-26 (Weekend): weekend day
- 2023-08-27 (Weekend): weekend day
- 2023-09-02 (Weekend): weekend day
- 2023-09-03 (Weekend): weekend day
- 2023-09-09 (Weekend): weekend day
- 2023-09-10 (Weekend): weekend day
- 2023-09-16 (Weekend): weekend day
- 2023-09-17 (Weekend): weekend day
- 2023-09-23 (Weekend): weekend day
- 2023-09-24 (Weekend): weekend day
- 2023-09-30 (Weekend): weekend day
- 2023-10-01 (Weekend): weekend day
- 2023-10-02 (Public Holiday): Labour Day
- 2023-10-07 (Weekend): weekend day
- 2023-10-08 (Weekend): weekend day
- 2023-10-14 (Weekend): weekend day
- 2023-10-15 (Weekend): weekend day
- 2023-10-21 (Weekend): weekend day
- 2023-10-22 (Weekend): weekend day
- 2023-10-28 (Weekend): weekend day
- 2023-10-29 (Weekend): weekend day
- 2023-11-04 (Weekend): weekend day
- 2023-11-05 (Weekend): weekend day
- 2023-11-11 (Weekend): weekend day
- 2023-11-12 (Weekend): weekend day
- 2023-11-18 (Weekend): weekend day
- 2023-11-19 (Weekend): weekend day
- 2023-11-25 (Weekend): weekend day
- 2023-11-26 (Weekend): weekend day
- 2023-12-02 (Weekend): weekend day
- 2023-12-03 (Weekend): weekend day
- 2023-12-09 (Weekend): weekend day
- 2023-12-10 (Weekend): weekend day
- 2023-12-16 (Weekend): weekend day
- 2023-12-17 (Weekend): weekend day
- 2023-12-23 (Weekend): weekend day
- 2023-12-24 (Weekend): weekend day
- 2023-12-25 (Public Holiday): Christmas Day
- 2023-12-26 (Public Holiday): Boxing Day
- 2023-12-30 (Weekend): weekend day
- 2023-12-31 (Weekend): weekend day",1,1,13,38948
"Helps users analyze a specified date range for a given country, retrieving detailed breakdowns of dates, weekdays, weekends, public holidays and custom dates. Also include the ability to calculate a new date by adding or subtracting a specified number of working days, taking into account holidays in specific countries. In addition, users should be allowed to obtain details of any particular date, including whether that date is a weekday, public holiday, weekend, or custom date, as well as the corresponding working hours, salary, and remote working details.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def analyse_date_range(start_date, end_date, country_code):
    url = ""https://working-days.p.rapidapi.com/1.3/analyse""
    querystring = {
        ""start_date"": start_date,
        ""end_date"": end_date,
        ""country_code"": country_code
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""working-days.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def calculate_new_date(country_code, start_date, increment):
    url = ""https://working-days.p.rapidapi.com/1.3/add_working_days""
    querystring = {
        ""country_code"": country_code,
        ""start_date"": start_date,
        ""increment"": str(increment)
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""working-days.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def get_day_info(country_code, date):
    url = ""https://working-days.p.rapidapi.com/1.3/get_info_day""
    querystring = {
        ""country_code"": country_code,
        ""date"": date
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""working-days.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def display_date_analysis(data):
    print(""\n### Date Range Analysis ###"")
    days = data.get('days', {})
    working_days = data.get('working_days', {})
    weekend_days = data.get('weekend_days', {})
    public_holidays = data.get('public_holidays', {})

    print(""\n** Days Overview **"")
    print(f""- Total Days: {days.get('total', 'N/A')}"")
    print(f""- Mondays: {days.get('mondays', 'N/A')}"")
    print(f""- Tuesdays: {days.get('tuesdays', 'N/A')}"")
    print(f""- Wednesdays: {days.get('wednesdays', 'N/A')}"")
    print(f""- Thursdays: {days.get('thursdays', 'N/A')}"")
    print(f""- Fridays: {days.get('fridays', 'N/A')}"")
    print(f""- Saturdays: {days.get('saturdays', 'N/A')}"")
    print(f""- Sundays: {days.get('sundays', 'N/A')}"")

    print(""\n** Working Days Overview **"")
    print(f""- Total Working Days: {working_days.get('total', 'N/A')}"")
    print(f""- Work Hours: {working_days.get('work_hours', 'N/A')}"")
    print(f""- Wages: ${working_days.get('wages', 'N/A')}"")

    print(""\n** Public Holidays **"")
    print(f""- Total Public Holidays: {public_holidays.get('total', 'N/A')}"")
    if 'list' in public_holidays:
        for holiday in public_holidays['list']:
            print(f""  - {holiday['date']}: {holiday['description']}"")


def display_new_date(data):
    print(""\n### New Date Calculation ###"")
    end_date = data.get('end_date', 'N/A')
    working_days = data.get('working_days', {})

    print(f""- New Date After Increment: {end_date}"")
    print(f""- Total Working Days in Period: {working_days.get('total', 'N/A')}"")
    print(f""- Work Hours in Period: {working_days.get('work_hours', 'N/A')}"")


def display_day_info(data):
    print(""\n### Specific Day Information ###"")
    print(f""- Working Day: {'Yes' if data.get('working_day', 0) == 1 else 'No'}"")
    print(f""- Public Holiday: {'Yes' if data.get('public_holiday', '0') == '1' else 'No'}"")
    if data.get('public_holiday', '0') == '1':
        print(f""  - Holiday Description: {data.get('public_holiday_description', 'N/A')}"")
    print(f""- Weekend Day: {'Yes' if data.get('weekend_day', 0) == 1 else 'No'}"")


if __name__ == '__main__':
    # Step 1: Analyze a date range for detailed breakdowns
    start_date = ""2023-01-01""
    end_date = ""2023-12-31""
    country_code = ""US""
    analysis = analyse_date_range(start_date, end_date, country_code)
    display_date_analysis(analysis)

    # Step 2: Calculate a new date by adding 5 working days from the given start date
    start_date = ""2023-10-01""
    increment = 5
    new_date = calculate_new_date(country_code, start_date, increment)
    display_new_date(new_date)

    # Step 3: Get detailed information about a specific date
    specific_date = ""2023-12-25""
    day_info = get_day_info(country_code, specific_date)
    display_day_info(day_info)
","### Date Range Analysis ###

** Days Overview **
- Total Days: 365
- Mondays: 52
- Tuesdays: 52
- Wednesdays: 52
- Thursdays: 52
- Fridays: 52
- Saturdays: 52
- Sundays: 53

** Working Days Overview **
- Total Working Days: 249
- Work Hours: 1992
- Wages: $39840

** Public Holidays **
- Total Public Holidays: 13
  - 2023-01-01: New Year's Day
  - 2023-01-02: New Year's Day (observance)
  - 2023-01-16: Martin Luther King Day
  - 2023-02-20: Washington's Birthday
  - 2023-05-29: Memorial Day
  - 2023-06-19: Juneteenth National Independence Day
  - 2023-07-04: Independence Day
  - 2023-09-04: Labor Day
  - 2023-10-09: Columbus Day
  - 2023-11-10: Veterans Day (observance)
  - 2023-11-11: Veterans Day
  - 2023-11-23: Thanksgiving
  - 2023-12-25: Christmas

### New Date Calculation ###
- New Date After Increment: N/A
- Total Working Days in Period: N/A
- Work Hours in Period: N/A

### Specific Day Information ###
- Working Day: No
- Public Holiday: No
- Weekend Day: No",1,1,17,59585
"I want to retrieve real-time stock quotes from multiple tickers, focusing on key trade indicators such as high, low, volume, and last trade data. Details of dividend-paying stocks should also be provided, including trend stocks with high dividend yields, recent dividend increases, and stocks that are going ex-dividend. This gives investors a comprehensive view of stock performance and dividend trends, helping them make informed investment decisions.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_realtime_quotes(sa_ids):
    url = ""https://seeking-alpha.p.rapidapi.com/market/get-realtime-quotes""
    querystring = {
        ""sa_ids"": sa_ids  # Comma-separated list of IDs for stock tickers
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""seeking-alpha.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_dividend_investing_info():
    url = ""https://seeking-alpha.p.rapidapi.com/market/get-dividend-investing""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""seeking-alpha.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def display_realtime_quotes(data):
    print(""\n### Real-Time Stock Quotes ###"")
    quotes = data.get('real_time_quotes', [])
    if quotes:
        for idx, quote in enumerate(quotes, start=1):
            print(f""\nStock {idx}:"")
            print(f""- Symbol: {quote.get('symbol', 'N/A')}"")
            print(f""- High: {quote.get('high', 'N/A')}"")
            print(f""- Low: {quote.get('low', 'N/A')}"")
            print(f""- Open: {quote.get('open', 'N/A')}"")
            print(f""- Close: {quote.get('close', 'N/A')}"")
            print(f""- Last: {quote.get('last', 'N/A')}"")
            print(f""- Volume: {quote.get('volume', 'N/A')}"")
            print(f""- Market Info: {quote.get('info', 'N/A')}"")
            print(f""- Last Updated: {quote.get('updated_at', 'N/A')}"")
    else:
        print(""No real-time quotes found."")

def display_dividend_info(data):
    print(""\n### Dividend Investing Information ###"")
    attributes = data.get('data', {}).get('attributes', {})

    # Display Trending Dividend Stocks
    trending_stocks = attributes.get('trending_dividend_stocks', [])
    if trending_stocks:
        print(""\n** Trending Dividend Stocks **"")
        for stock in trending_stocks:
            print(f""- Name: {stock.get('name', 'N/A')}"")
            print(f""  - Dividend Yield (Forward): {stock.get('div_yield_fwd', 'N/A')}%"")

    # Display Dividend Increases
    dividend_increases = attributes.get('dividend_increases', [])
    if dividend_increases:
        print(""\n** Dividend Increases **"")
        for increase in dividend_increases:
            print(f""- {increase.get('name', 'N/A')}"")

    # Display Upcoming Ex-Dates
    upcoming_exdates = attributes.get('upcoming_exdates', [])
    if upcoming_exdates:
        print(""\n** Upcoming Ex-Dates **"")
        for exdate in upcoming_exdates:
            print(f""- {exdate.get('name', 'N/A')}"")
            print(f""  - Ex-Date: {exdate.get('date', 'N/A')}"")

if __name__ == '__main__':
    # Step 1: Retrieve real-time quotes for specified tickers
    ticker_ids = ""612888,16123""  # Example ticker IDs, replace with desired IDs
    realtime_quotes = get_realtime_quotes(ticker_ids)
    display_realtime_quotes(realtime_quotes)

    # Step 2: Retrieve information on dividend-paying stocks
    dividend_info = get_dividend_investing_info()
    display_dividend_info(dividend_info)
",Fail,-1,-1,30,109020
"Please provide an overall view of the daily activity in the stock market, highlighting the biggest winners, losers, most active stocks and noteworthy sectors. Includes metadata on specific financial instruments and companies, sorted by index, and covers global markets. In addition, an integrated news feed for the latest market news provides users with current updates on financial trends and events affecting their investments.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def get_market_day_watch():
    url = ""https://seeking-alpha.p.rapidapi.com/market/get-day-watch""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""seeking-alpha.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()


def get_equity_metadata(filter_category):
    url = ""https://seeking-alpha.p.rapidapi.com/market/get-equity""
    querystring = {
        ""filterCategory"": filter_category  # e.g., 'global-equity'
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""seeking-alpha.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def get_hot_news(limit=10):
    url = ""https://yahoo-finance-api-data.p.rapidapi.com/news/hot-news""
    querystring = {
        ""limit"": str(limit)  # Number of news notifications to retrieve
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""yahoo-finance-api-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def display_market_day_watch(data):
    print(""\n### Market Day Watch Overview ###"")
    if 'data' in data and 'attributes' in data['data']:
        attributes = data['data']['attributes']

        # Display Top Gainers
        print(""\n** Top Gainers **"")
        for item in attributes.get('top_gainers', []):
            print(f""- {item.get('name', 'N/A')} (Slug: {item.get('slug', 'N/A')})"")

        # Display Top Losers
        print(""\n** Top Losers **"")
        for item in attributes.get('top_losers', []):
            print(f""- {item.get('name', 'N/A')} (Slug: {item.get('slug', 'N/A')})"")

        # Display Cryptocurrencies
        print(""\n** Cryptocurrencies **"")
        for item in attributes.get('cryptocurrencies', []):
            print(f""- {item.get('name', 'N/A')} (Slug: {item.get('slug', 'N/A')})"")

        # Display Most Active Stocks
        print(""\n** Most Active Stocks **"")
        for item in attributes.get('most_active', []):
            print(f""- {item.get('name', 'N/A')} (Slug: {item.get('slug', 'N/A')})"")
    else:
        print(""No market day watch data found."")


def display_equity_metadata(data):
    print(""\n### Equity Metadata for Global Markets ###"")
    if 'data' in data:
        for idx, item in enumerate(data['data'], start=1):
            attributes = item.get('attributes', {})
            print(f""\nEquity {idx}:"")
            print(f""- Name: {attributes.get('name', 'N/A')}"")
            print(f""- Company: {attributes.get('company', 'N/A')}"")
            print(f""- Alias: {attributes.get('alias_name', 'N/A')}"")
            print(f""- Tag Kind: {attributes.get('tagKind', 'N/A')}"")
    else:
        print(""No equity metadata found."")


def display_hot_news(data):
    print(""\n### Latest Financial Market News ###"")
    if data.get('success') and 'data' in data:
        for idx, news in enumerate(data['data'], start=1):
            print(f""\nNews {idx}:"")
            print(f""- Title: {news.get('notificationTitle', 'N/A')}"")
            print(f""- Published Date: {news.get('publishTs', 'N/A')}"")
            print(f""- Link: {news.get('articleUrl', 'N/A')}"")
    else:
        print(""No hot news found."")


if __name__ == '__main__':
    # Step 1: Retrieve market day watch data for top movers and active stocks
    market_day_watch = get_market_day_watch()
    display_market_day_watch(market_day_watch)

    # Step 2: Retrieve equity metadata based on the global equity filter
    equity_metadata = get_equity_metadata(""global-equity"")
    display_equity_metadata(equity_metadata)

    # Step 3: Retrieve the latest hot news for the financial market
    hot_news = get_hot_news(limit=3)
    display_hot_news(hot_news)
",Fail,-1,-1,32,89408
"I need a service that aggregates and presents financial market news and data from a variety of sources. The service should offer the latest articles and videos related to specific stock symbols, grouped by region, and allow users to access specific market stories focused on currencies. The goal is to keep users informed of global market conditions, currency trends and stock-specific news through a unified interface, with the option to filter by region and specific ticker symbol.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def get_news_by_symbol(symbol, limit=5):
    url = ""https://yahoo-finance-api-data.p.rapidapi.com/news/list""
    querystring = {
        ""symbol"": symbol,  # Stock symbol (e.g., 'AAPL')
        ""limit"": str(limit)  # Maximum number of news items to return
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""yahoo-finance-api-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def display_news(data, symbol):
    print(f""\n### News Articles for Symbol '{symbol}' ###"")
    if data.get('success') and data.get('data') and data['data'].get('main') and data['data']['main'].get('stream'):
        for idx, news_item in enumerate(data['data']['main']['stream'], start=1):
            content = news_item.get('content', {})
            # å®‰å…¨æ£€æŸ¥ clickThroughUrl æ˜¯å¦ä¸º None
            link = content.get('clickThroughUrl', {})
            link_url = link.get('url', 'N/A') if link else 'N/A'

            print(f""\nNews {idx}:"")
            print(f""- Title: {content.get('title', 'N/A')}"")
            print(f""- Published Date: {content.get('pubDate', 'N/A')}"")
            print(f""- Source: {content.get('provider', {}).get('displayName', 'N/A')}"")
            print(f""- Link: {link_url}"")
    else:
        print(""No news found."")
    print(""---"")


if __name__ == '__main__':
    # Step 1: Retrieve news articles related to a specific stock symbol (e.g., AAPL)
    symbol_news = get_news_by_symbol(""AAPL"", limit=5)
    display_news(symbol_news, ""AAPL"")
",Fail,-1,-1,27,82323
"Users need to search for securities by name or keyword, retrieve detailed information about related news articles, and obtain comprehensive metadata for specific news items. Take advantage of autocomplete to assist in finding relevant securities and provide the ability to access in-depth content details of selected news articles. The goal is to enable users to conduct detailed investigations of security-related news, explore the metadata of articles, and identify stock-specific data through a user-friendly interface.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_auto_complete_suggestions(query):
    url = ""https://bb-finance.p.rapidapi.com/market/auto-complete""
    querystring = {
        ""query"": query  # Search term or phrase (e.g., 'apple')
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""bb-finance.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_news_detail(news_id):
    url = ""https://yahoo-finance-api-data.p.rapidapi.com/news/detail""
    querystring = {
        ""id"": news_id  # Unique identifier of the news article
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""yahoo-finance-api-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def display_auto_complete_suggestions(data, query, limit=5):
    print(f""\n### Auto-complete Suggestions for '{query}' ###"")
    if 'quote' in data:
        for idx, item in enumerate(data['quote'][:limit], start=1):
            print(f""\nSuggestion {idx}:"")
            print(f""- Name: {item.get('name', 'N/A')}"")
            print(f""- Symbol: {item.get('symbol', 'N/A')}"")
            print(f""- Exchange: {item.get('exchange', 'N/A')}"")
            print(f""- Country: {item.get('country', 'N/A')}"")
            print(f""- Currency: {item.get('currency', 'N/A')}"")
            print(f""- Security Type: {item.get('securityType', 'N/A')}"")
    else:
        print(""No suggestions found."")
    print(""---"")

def display_news_details(data, news_id):
    print(f""\n### News Details for ID '{news_id}' ###"")
    if data.get('success'):
        news_item = data.get('result', {})
        print(f""- Title: {news_item.get('title', 'N/A')}"")
        print(f""- Source: {news_item.get('source', 'N/A')}"")
        print(f""- Published Date: {news_item.get('published_date', 'N/A')}"")
        print(f""- Summary: {news_item.get('summary', 'N/A')}"")
        print(f""- Content: {news_item.get('content', 'N/A')}"")
    else:
        print(""No news details found."")
    print(""---"")

if __name__ == '__main__':
    # Step 1: Use auto-complete to search for securities related to a query (e.g., 'apple')
    search_query = ""apple""
    auto_complete_data = get_auto_complete_suggestions(search_query)
    display_auto_complete_suggestions(auto_complete_data, search_query, limit=5)  # Limit to 5 suggestions

    # Step 2: Assume a news item ID is retrieved from auto-complete results, fetch detailed news info
    example_news_id = ""4cd7395e-588c-4a64-a33b-64371b30dc95""
    news_detail = get_news_detail(example_news_id)
    display_news_details(news_detail, example_news_id)
","### Auto-complete Suggestions for 'apple' ###

Suggestion 1:
- Name: Apple Inc
- Symbol: AAPL
- Exchange: NASDAQ GS
- Country: United States
- Currency: USD
- Security Type: Common Stock

Suggestion 2:
- Name: Apple Hospitality REIT Inc
- Symbol: APLE
- Exchange: New York
- Country: United States
- Currency: USD
- Security Type: REIT

Suggestion 3:
- Name: Apple International Co Ltd
- Symbol: 2788
- Exchange: Tokyo
- Country: Japan
- Currency: JPY
- Security Type: Common Stock

Suggestion 4:
- Name: Apple Green Holding Inc
- Symbol: AGPL
- Exchange: N/A
- Country: Malaysia
- Currency: USD
- Security Type: Common Stock

Suggestion 5:
- Name: Apple Flavor & Fragrance Group Co Ltd
- Symbol: 603020
- Exchange: Shanghai
- Country: China
- Currency: CNY
- Security Type: Common Stock
---

### News Details for ID '4cd7395e-588c-4a64-a33b-64371b30dc95' ###
- Title: N/A
- Source: N/A
- Published Date: N/A
- Summary: N/A
- Content: N/A
---",1,0,12,31656
"Users need to obtain real-time exchange rates for multiple currencies relative to the specified base currency and convert from one currency to another based on the latest exchange rate. First retrieve the real-time exchange rate for a set of selected currencies using the base currency, and then use the obtained exchange rate to convert the specified amount between the two selected currencies. Allow users to monitor current exchange rates and seamlessly perform real-time currency conversions.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_live_rates(base_currency, target_currencies):
    url = ""https://forex-apised1.p.rapidapi.com/live-rates""
    querystring = {
        ""base_currency_code"": base_currency,
        ""currency_codes"": target_currencies
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""forex-apised1.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def convert_currency(from_currency, to_currency, amount):
    url = ""https://currency-converter18.p.rapidapi.com/api/v1/convert""
    querystring = {
        ""from"": from_currency,
        ""to"": to_currency,
        ""amount"": amount
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""currency-converter18.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def display_live_rates(data, base_currency):
    if data.get('success'):
        print(f""\n### Live Rates for {base_currency} ###"")
        for code, details in data.get('rates', {}).items():
            rate = details['rate']
            currency_name = details['currency_name']
            print(f""- {currency_name} ({code}): Exchange Rate: {rate}"")
        print(""---"")
    else:
        print(""Failed to retrieve live rates."")

def display_conversion_result(data, from_currency, to_currency, amount):
    if data.get('success'):
        converted_amount = data['result']['convertedAmount']
        print(f""\n### Conversion Result ###"")
        print(f""- {amount} {from_currency} to {to_currency}: {converted_amount:.2f}"")
        print(""---"")
    else:
        print(""Failed to retrieve conversion data."")

if __name__ == '__main__':
    # Step 1: Get live exchange rates for a base currency to multiple target currencies
    base_currency = ""USD""
    target_currencies = ""GBP,EUR,JPY""
    live_rates = get_live_rates(base_currency, target_currencies)
    display_live_rates(live_rates, base_currency)

    # Step 2: Convert an amount from one currency to another using the obtained rates
    from_currency = ""EUR""
    to_currency = ""KWD""
    amount_to_convert = 10
    conversion_result = convert_currency(from_currency, to_currency, amount_to_convert)
    display_conversion_result(conversion_result, from_currency, to_currency, amount_to_convert)
",Fail,-1,-1,33,112596
"Users need a currency conversion service that first retrieves a list of all supported currencies and then converts the specified amount from one base currency to multiple target currencies. Users should be able to select a list of base and target currencies. The service will first obtain the available currency to ensure that a valid currency code is used, and then perform the conversion of the specified amount across the selected target currency, providing detailed conversion rates and amounts.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_supported_currencies():
    url = ""https://currency-converter18.p.rapidapi.com/api/v1/supportedCurrencies""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""currency-converter18.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def multi_currency_convert(base_currency, amount, target_currencies):
    url = ""https://forex-apised1.p.rapidapi.com/multi-convert""
    querystring = {
        ""from"": base_currency,
        ""amount"": amount,
        ""to"": target_currencies
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""forex-apised1.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def display_supported_currencies(data):
    if data:
        print(""\n### Supported Currencies ###"")
        for currency in data:
            print(f""- {currency['name']} (Symbol: {currency['symbol']})"")
    else:
        print(""Failed to retrieve supported currencies."")

def display_conversion_result(data, base_currency, amount):
    if data.get('success'):
        print(f""\n### Conversion Result for {amount} {base_currency} ###"")
        for code, details in data.get('rates', {}).items():
            converted_amount = details['converted_amount']
            rate = details['rate']
            currency_name = details['currency_name']
            print(f""- {currency_name} ({code}):"")
            print(f""  - Conversion Rate: {rate}"")
            print(f""  - Converted Amount: {converted_amount:.2f}"")
            print(""---"")
    else:
        print(""Failed to retrieve conversion data."")

if __name__ == '__main__':
    # Step 1: Retrieve the list of supported currencies
    supported_currencies = get_supported_currencies()
    display_supported_currencies(supported_currencies)

    # Step 2: Convert a specific amount from a base currency to multiple target currencies
    base_currency = ""EUR""
    amount_to_convert = 2
    target_currencies = ""GBP,USD,EUR""
    conversion_result = multi_currency_convert(base_currency, amount_to_convert, target_currencies)
    display_conversion_result(conversion_result, base_currency, amount_to_convert)
","### Supported Currencies ###
- United States Dollar (Symbol: USD)
- Albania Lek (Symbol: ALL)
- Algeria Dinar (Symbol: DZD)
- Angola Kwanza (Symbol: AOA)
- Argentina Peso (Symbol: ARS)
- Armenia Dram (Symbol: AMD)
- Aruba Guilder (Symbol: AWG)
- Australia Dollar (Symbol: AUD)
- Azerbaijan Manat (Symbol: AZN)
- Bahamas Dollar (Symbol: BSD)
- Bahrain Dinar (Symbol: BHD)
- Bangladesh Taka (Symbol: BDT)
- Barbados Dollar (Symbol: BBD)
- Belarus Ruble (Symbol: BYN)
- Belize Dollar (Symbol: BZD)
- Bermuda Dollar (Symbol: BMD)
- Bhutan Ngultrum (Symbol: BTN)
- Bolivia BolÃ­viano (Symbol: BOB)
- Bosnia and Herzegovina Convertible Marka (Symbol: BAM)
- Botswana Pula (Symbol: BWP)
- Brazil Real (Symbol: BRL)
- Brunei Darussalam Dollar (Symbol: BND)
- Bulgaria Lev (Symbol: BGN)
- Burundi Franc (Symbol: BIF)
- Cambodia Riel (Symbol: KHR)
- Canada Dollar (Symbol: CAD)
- Cape Verde Escudo (Symbol: CVE)
- Cayman Islands Dollar (Symbol: KYD)
- Chile Peso (Symbol: CLP)
- China Yuan Renminbi (Symbol: CNY)
- Colombia Peso (Symbol: COP)
- CommunautÃ© FinanciÃ¨re Africaine (BCEAO) Franc (Symbol: XOF)
- CommunautÃ© FinanciÃ¨re Africaine (BEAC) CFA Franc BEAC (Symbol: XAF)
- Comorian Franc (Symbol: KMF)
- Comptoirs FranÃ§ais du Pacifique (CFP) Franc (Symbol: XPF)
- Congo/Kinshasa Franc (Symbol: CDF)
- Costa Rica Colon (Symbol: CRC)
- Croatia Kuna (Symbol: HRK)
- Cuba Peso (Symbol: CUP)
- Czech Republic Koruna (Symbol: CZK)
- Denmark Krone (Symbol: DKK)
- Djibouti Franc (Symbol: DJF)
- Dominican Republic Peso (Symbol: DOP)
- East Caribbean Dollar (Symbol: XCD)
- Egypt Pound (Symbol: EGP)
- El Salvador Colon (Symbol: SVC)
- Ethiopia Birr (Symbol: ETB)
- Euro Member Countries (Symbol: EUR)
- Fiji Dollar (Symbol: FJD)
- Gambia Dalasi (Symbol: GMD)
- Georgia Lari (Symbol: GEL)
- Ghana Cedi (Symbol: GHS)
- Guatemala Quetzal (Symbol: GTQ)
- Guinea Franc (Symbol: GNF)
- Guyana Dollar (Symbol: GYD)
- Haiti Gourde (Symbol: HTG)
- Honduras Lempira (Symbol: HNL)
- Hong Kong Dollar (Symbol: HKD)
- Hungary Forint (Symbol: HUF)
- Iceland Krona (Symbol: ISK)
- India Rupee (Symbol: INR)
- Indonesia Rupiah (Symbol: IDR)
- Iran Rial (Symbol: IRR)
- Iraq Dinar (Symbol: IQD)
- Israel Shekel (Symbol: ILS)
- Jamaica Dollar (Symbol: JMD)
- Japan Yen (Symbol: JPY)
- Jordan Dinar (Symbol: JOD)
- Kazakhstan Tenge (Symbol: KZT)
- Kenya Shilling (Symbol: KES)
- Korea (South) Won (Symbol: KRW)
- Kuwait Dinar (Symbol: KWD)
- Kyrgyzstan Som (Symbol: KGS)
- Laos Kip (Symbol: LAK)
- Lebanon Pound (Symbol: LBP)
- Lesotho Loti (Symbol: LSL)
- Liberia Dollar (Symbol: LRD)
- Libya Dinar (Symbol: LYD)
- Macau Pataca (Symbol: MOP)
- Macedonia Denar (Symbol: MKD)
- Madagascar Ariary (Symbol: MGA)
- Malawi Kwacha (Symbol: MWK)
- Malaysia Ringgit (Symbol: MYR)
- Maldives (Maldive Islands) Rufiyaa (Symbol: MVR)
- Mauritius Rupee (Symbol: MUR)
- Mexico Peso (Symbol: MXN)
- Moldova Leu (Symbol: MDL)
- Morocco Dirham (Symbol: MAD)
- Mozambique Metical (Symbol: MZN)
- Myanmar (Burma) Kyat (Symbol: MMK)
- Namibia Dollar (Symbol: NAD)
- Nepal Rupee (Symbol: NPR)
- Netherlands Antilles Guilder (Symbol: ANG)
- New Zealand Dollar (Symbol: NZD)
- Nicaragua Cordoba (Symbol: NIO)
- Nigeria Naira (Symbol: NGN)
- Norway Krone (Symbol: NOK)
- Oman Rial (Symbol: OMR)
- Pakistan Rupee (Symbol: PKR)
- Panama Balboa (Symbol: PAB)
- Papua New Guinea Kina (Symbol: PGK)
- Paraguay Guarani (Symbol: PYG)
- Peru Sol (Symbol: PEN)
- Philippines Peso (Symbol: PHP)
- Poland Zloty (Symbol: PLN)
- Qatar Riyal (Symbol: QAR)
- Romania Leu (Symbol: RON)
- Russia Ruble (Symbol: RUB)
- Rwanda Franc (Symbol: RWF)
- Saudi Arabia Riyal (Symbol: SAR)
- Serbia Dinar (Symbol: RSD)
- Seychelles Rupee (Symbol: SCR)
- Sierra Leone Leone (Symbol: SLL)
- Singapore Dollar (Symbol: SGD)
- Solomon Islands Dollar (Symbol: SBD)
- Somalia Shilling (Symbol: SOS)
- South Africa Rand (Symbol: ZAR)
- Sri Lanka Rupee (Symbol: LKR)
- Sudan Pound (Symbol: SDG)
- Suriname Dollar (Symbol: SRD)
- Swaziland Lilangeni (Symbol: SZL)
- Sweden Krona (Symbol: SEK)
- Switzerland Franc (Symbol: CHF)
- Taiwan New Dollar (Symbol: TWD)
- Tajikistan Somoni (Symbol: TJS)
- Tanzania Shilling (Symbol: TZS)
- Thailand Baht (Symbol: THB)
- Tonga Pa'anga (Symbol: TOP)
- Trinidad and Tobago Dollar (Symbol: TTD)
- Tunisia Dinar (Symbol: TND)
- Turkey Lira (Symbol: TRY)
- Turkmenistan Manat (Symbol: TMT)
- Uganda Shilling (Symbol: UGX)
- Ukraine Hryvnia (Symbol: UAH)
- United Arab Emirates Dirham (Symbol: AED)
- United Kingdom Pound (Symbol: GBP)
- Uruguay Peso (Symbol: UYU)
- Uzbekistan Som (Symbol: UZS)
- Viet Nam Dong (Symbol: VND)
- Yemen Rial (Symbol: YER)
- Zambia Kwacha (Symbol: ZMW)
- Afghan Afghani (Symbol: AFN)
- Unidad de Fomento (Symbol: CLF)
- Chinese Renminbi Yuan Offshore (Symbol: CNH)
- Belarusian Ruble (Old) (Symbol: BYR)
- Belarusian Ruble (New) (Symbol: BYN)
- Cuban Convertible Peso (Symbol: CUC)
- Estonian Kroon (Symbol: EEK)
- Eritrean Nakfa (Symbol: ERN)
- Falkland Pound (Symbol: FKP)
- Guernsey Pound (Symbol: GGP)
- Gibraltar Pound (Symbol: GIP)
- Isle of Man Pound (Symbol: IMP)
- Jersey Pound (Symbol: JEP)
- Lithuanian Litas (Symbol: LTL)
- Maltese Lira (Symbol: MTL)
- Saint Helenian Pound (Symbol: SHP)
- East Caribbean Dollar (Symbol: XCD)

### Conversion Result for 2 EUR ###
- United Kingdom Pound (GBP):
  - Conversion Rate: 0.8332473675684432
  - Converted Amount: 1.67
---
- United States Dollar (USD):
  - Conversion Rate: 1.0799719207300609
  - Converted Amount: 2.16
---
- Euro Member Countries (EUR):
  - Conversion Rate: 0.9999999999999999
  - Converted Amount: 2.00
---",1,1,12,34668
"Users need to create a comprehensive commodity market data service by first retrieving a list of supported commodities and currencies. Use this list to get the latest exchange rate for the selected item in the specified base currency. In addition, retrieve real-time or historical market data, including the opening, highest, lowest, previous and current prices of the commodity. The service should allow users to select base currencies and commodities of interest to get the latest exchange rates and detailed market analysis.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_supported_commodities_and_currencies():
    url = ""https://commodities-apised.p.rapidapi.com/v1/supported""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""commodities-apised.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def get_latest_exchange_rates(base_currency, symbols):
    url = ""https://commodities-apised.p.rapidapi.com/v1/latest""
    querystring = {
        ""base"": base_currency,
        ""symbols"": symbols
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""commodities-apised.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_commodity_market_data(base_currency, symbols):
    url = ""https://commodities-apised.p.rapidapi.com/v1/market-data""
    querystring = {
        ""base"": base_currency,
        ""symbols"": symbols
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""commodities-apised.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def display_supported_commodities_and_currencies(data):
    if data.get('success'):
        print(""\n### Supported Commodities ###"")
        for commodity in data.get('supported_commodities', []):
            print(f""- {commodity['commodity_name']} (Code: {commodity['commodity_code']}, Measurement: {commodity['commodity_weight_measurement']})"")

        print(""\n### Supported Currencies ###"")
        for currency in data.get('supported_currencies', []):
            print(f""- {currency['currency_name']} (Code: {currency['currency_code']})"")
    else:
        print(""Failed to retrieve supported commodities and currencies."")

def display_latest_exchange_rates(data, base_currency):
    if data.get('success'):
        print(f""\n### Latest Exchange Rates in {base_currency} ###"")
        for commodity, rate in data.get('rates', {}).items():
            print(f""- {commodity}: {rate} {base_currency}"")
    else:
        print(""Failed to retrieve latest exchange rates."")

def display_commodity_market_data(data, base_currency):
    if data.get('success'):
        print(f""\n### Market Data in {base_currency} ###"")
        for commodity, details in data.get('rates', {}).items():
            print(f""\nCommodity: {commodity}"")
            print(f""- Open: {details['open']} {base_currency}"")
            print(f""- High: {details['high']} {base_currency}"")
            print(f""- Low: {details['low']} {base_currency}"")
            print(f""- Previous Close: {details['prev']} {base_currency}"")
            print(f""- Current: {details['current']} {base_currency}"")
            print(""---"")
    else:
        print(""Failed to retrieve market data."")

if __name__ == '__main__':
    # Step 1: Retrieve supported commodities and currencies
    supported_data = get_supported_commodities_and_currencies()
    display_supported_commodities_and_currencies(supported_data)

    # Step 2: Get latest exchange rates for selected commodities
    base_currency = ""USD""
    commodity_symbols = ""COCOA,COFFEE,CORN,COTTON,GASOLINE,LUMBER""
    latest_rates = get_latest_exchange_rates(base_currency, commodity_symbols)
    display_latest_exchange_rates(latest_rates, base_currency)

    # Step 3: Retrieve market data for selected commodities
    market_data = get_commodity_market_data(base_currency, commodity_symbols)
    display_commodity_market_data(market_data, base_currency)
",Fail,-1,-1,30,96600
"The user first retrieves a comprehensive list of supported currencies, mapping each currency code to its full name. This data is then used to provide a real-time exchange rate relative to the specified base currency, with the ability to choose the response format (JSON or XML). The service should allow users to choose their preferred base currency for conversion and provide up-to-date exchange rates for supported currencies.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_supported_currencies(output_format=""JSON""):
    url = ""https://currencyapi-net.p.rapidapi.com/currencies""
    querystring = {""output"": output_format}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""currencyapi-net.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_exchange_rates(base_currency=""USD"", output_format=""JSON""):
    url = ""https://currencyapi-net.p.rapidapi.com/rates""
    querystring = {
        ""output"": output_format,
        ""base"": base_currency  # Specify the base currency, e.g., 'USD'
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""currencyapi-net.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def format_supported_currencies(data):
    print(""\n### Supported Currencies ###"")
    if data.get('valid'):
        currencies = data.get('currencies', {})
        if currencies:
            for code, name in currencies.items():
                print(f""- {code}: {name}"")
        else:
            print(""No supported currencies available."")
    else:
        print(""Failed to retrieve supported currencies. Response:"", data)

def format_exchange_rates(data, base_currency):
    print(f""\n### Live Exchange Rates relative to {base_currency} ###"")
    if data.get('valid'):
        rates = data.get('rates', {})
        if rates:
            for currency, rate in rates.items():
                print(f""- {currency}: {rate}"")
        else:
            print(""No exchange rate data available."")
    else:
        print(""Failed to retrieve exchange rates. Response:"", data)

if __name__ == '__main__':
    # Step 1: Retrieve a list of supported currencies
    supported_currencies = get_supported_currencies()
    format_supported_currencies(supported_currencies)

    # Step 2: Fetch exchange rates using a specified base currency
    base_currency = ""USD""  # Example base currency
    exchange_rates = get_exchange_rates(base_currency)
    format_exchange_rates(exchange_rates, base_currency)
","### Supported Currencies ###
- AED: United Arab Emirates Dirham
- AFN: Afghan Afghani
- ALL: Albanian Lek
- AMD: Armenian Dram
- ANG: Netherlands Antillean Guilder
- AOA: Angolan Kwanza
- ARS: Argentine Peso
- AUD: Australian Dollar
- AWG: Aruban Florin
- AZN: Azerbaijani Manat
- BAM: Bosnia-Herzegovina Convertible Mark
- BBD: Barbadian Dollar
- BCH: Bitcoin Cash
- BDT: Bangladeshi Taka
- BGN: Bulgarian Lev
- BHD: Bahraini Dinar
- BIF: Burundian Franc
- BMD: Bermudan Dollar
- BND: Brunei Dollar
- BOB: Bolivian Boliviano
- BRL: Brazilian Real
- BSD: Bahamian Dollar
- BTC: Bitcoin
- BTG: Bitcoin Gold
- BWP: Botswanan Pula
- BZD: Belize Dollar
- CAD: Canadian Dollar
- CDF: Congolese Franc
- CHF: Swiss Franc
- CLP: Chilean Peso
- CNH: Chinese Yuan (offshore)
- CNY: Chinese Yuan
- COP: Colombian Peso
- CRC: Costa Rican ColÃ³n
- CUC: Cuban Convertible Peso
- CUP: Cuban Peso
- CVE: Cape Verdean Escudo
- CZK: Czech Republic Koruna
- DASH: DASH
- DJF: Djiboutian Franc
- DKK: Danish Krone
- DOP: Dominican Peso
- DZD: Algerian Dinar
- EGP: Egyptian Pound
- EOS: EOS
- ETB: Ethiopian Birr
- ETH: Ethereum
- EUR: Euro
- FJD: Fijian Dollar
- GBP: British Pound Sterling
- GEL: Georgian Lari
- GHS: Ghanaian Cedi
- GIP: Gibraltar Pound
- GMD: Gambian Dalasi
- GNF: Guinean Franc
- GTQ: Guatemalan Quetzal
- GYD: Guyanaese Dollar
- HKD: Hong Kong Dollar
- HNL: Honduran Lempira
- HRK: Croatian Kuna
- HTG: Haitian Gourde
- HUF: Hungarian Forint
- IDR: Indonesian Rupiah
- ILS: Israeli New Sheqel
- INR: Indian Rupee
- IQD: Iraqi Dinar
- IRR: Iranian Rial
- ISK: Icelandic KrÃ³na
- JMD: Jamaican Dollar
- JOD: Jordanian Dinar
- JPY: Japanese Yen
- KES: Kenyan Shilling
- KGS: Kyrgystani Som
- KHR: Cambodian Riel
- KMF: Comorian Franc
- KRW: South Korean Won
- KWD: Kuwaiti Dinar
- KYD: Cayman Islands Dollar
- KZT: Kazakhstani Tenge
- LAK: Laotian Kip
- LBP: Lebanese Pound
- LKR: Sri Lankan Rupee
- LRD: Liberian Dollar
- LSL: Lesotho Loti
- LTC: Litecoin
- LYD: Libyan Dinar
- MAD: Moroccan Dirham
- MDL: Moldovan Leu
- MKD: Macedonian Denar
- MMK: Myanma Kyat
- MOP: Macanese Pataca
- MUR: Mauritian Rupee
- MVR: Maldivian Rufiyaa
- MWK: Malawian Kwacha
- MXN: Mexican Peso
- MYR: Malaysian Ringgit
- MZN: Mozambican Metical
- NAD: Namibian Dollar
- NGN: Nigerian Naira
- NIO: Nicaraguan CÃ³rdoba
- NOK: Norwegian Krone
- NPR: Nepalese Rupee
- NZD: New Zealand Dollar
- OMR: Omani Rial
- PAB: Panamanian Balboa
- PEN: Peruvian Nuevo Sol
- PGK: Papua New Guinean Kina
- PHP: Philippine Peso
- PKR: Pakistani Rupee
- PLN: Polish Zloty
- PYG: Paraguayan Guarani
- QAR: Qatari Rial
- RON: Romanian Leu
- RSD: Serbian Dinar
- RUB: Russian Ruble
- RWF: Rwandan Franc
- SAR: Saudi Riyal
- SBD: Solomon Islands Dollar
- SCR: Seychellois Rupee
- SDG: Sudanese Pound
- SEK: Swedish Krona
- SGD: Singapore Dollar
- SLL: Sierra Leonean Leone
- SOS: Somali Shilling
- SRD: Surinamese Dollar
- SVC: Salvadoran ColÃ³n
- SZL: Swazi Lilangeni
- THB: Thai Baht
- TJS: Tajikistani Somoni
- TMT: Turkmenistani Manat
- TND: Tunisian Dinar
- TOP: Tongan PaÊ»anga
- TRY: Turkish Lira
- TTD: Trinidad and Tobago Dollar
- TWD: New Taiwan Dollar
- TZS: Tanzanian Shilling
- UAH: Ukrainian Hryvnia
- UGX: Ugandan Shilling
- USD: United States Dollar
- UYU: Uruguayan Peso
- UZS: Uzbekistan Som
- VND: Vietnamese Dong
- XAF: CFA Franc BEAC
- XAG: Silver (troy ounce)
- XAU: Gold (troy ounce)
- XCD: East Caribbean Dollar
- XLM: Stellar Lumens
- XOF: CFA Franc BCEAO
- XRP: Ripple
- YER: Yemeni Rial
- ZAR: South African Rand
- ZMW: Zambian Kwacha

### Live Exchange Rates relative to USD ###
- AED: 3.673
- AFN: 66.69506
- ALL: 91.1980228
- AMD: 385.831049
- ANG: 1.806062
- AOA: 909.051
- ARS: 984.393427
- AUD: 1.51377536
- AWG: 1.79994445
- AZN: 1.7
- BAM: 1.810691
- BBD: 2.02228968
- BCH: 0.002856098818
- BDT: 119.753
- BGN: 1.810985
- BHD: 0.376744184
- BIF: 2910.271
- BMD: 1
- BND: 1.32303483
- BOB: 6.92014225
- BRL: 5.7075
- BSD: 1.002092
- BTC: 1.49187599e-05
- BTG: 0.04765539576
- BWP: 13.3957001
- BZD: 2.01886264
- CAD: 1.3892499
- CDF: 2847.86857
- CHF: 0.8668313
- CLP: 952.380952
- CNH: 7.1343
- CNY: 7.12110218
- COP: 4331.00297
- CRC: 515.809031
- CUC: 1
- CUP: 24.0502207
- CVE: 102.083994
- CZK: 23.43
- DASH: 0.0456
- DJF: 178.351833
- DKK: 6.9113
- DOP: 60.34375
- DZD: 133.489712
- EGP: 48.74983
- EOS: 2.250032932
- ETB: 121.858949
- ETH: 0.0004037104367
- EUR: 0.92595
- FJD: 2.24825
- GBP: 0.7715454
- GEL: 2.72
- GHS: 16.10439
- GIP: 0.7715454
- GMD: 69.5
- GNF: 8641.59337
- GTQ: 7.751773
- GYD: 209.658
- HKD: 7.77035
- HNL: 25.2549104
- HRK: 6.8253273
- HTG: 131.920848
- HUF: 374.28
- IDR: 15695.1
- ILS: 3.7886
- INR: 84.102238
- IQD: 1312.20525
- IRR: 42105
- ISK: 138.12
- JMD: 158.730436
- JOD: 0.7089
- JPY: 152.284998
- KES: 129.27
- KGS: 85.8
- KHR: 4067.73594
- KMF: 454.949532
- KRW: 1388.66
- KWD: 0.306185592
- KYD: 0.834666713
- KZT: 487.4553
- LAK: 22010.1423
- LBP: 89739.2178
- LKR: 294.2369
- LRD: 192.4102
- LSL: 17.69021
- LTC: 0.01459148194
- LYD: 4.82699332
- MAD: 9.88528131
- MDL: 17.8906879
- MKD: 56.96367
- MMK: 3247.83199
- MOP: 8.01587143
- MUR: 46.0883227
- MVR: 15.3593481
- MWK: 1737.645
- MXN: 19.9839
- MYR: 4.33999899
- MZN: 63.9033985
- NAD: 17.68996
- NGN: 1645
- NIO: 36.87485
- NOK: 10.9731987
- NPR: 134.8032
- NZD: 1.67322
- OMR: 0.38499
- PAB: 1.00156705
- PEN: 3.76148004
- PGK: 4.010591
- PHP: 58.565
- PKR: 278.1873
- PLN: 4.016349
- PYG: 8020.257
- QAR: 3.654804
- RON: 4.60739893
- RSD: 108.3564
- RUB: 97.19037
- RWF: 1355.287
- SAR: 3.75588
- SBD: 8.351058
- SCR: 13.1487546
- SDG: 601.5
- SEK: 10.6211
- SGD: 1.3209
- SLL: 19750
- SOS: 572.7035
- SRD: 34.0505
- SVC: 8.768308
- SZL: 17.68382
- THB: 33.6599923
- TJS: 10.66511
- TMT: 3.5
- TND: 3.108764
- TOP: 2.3421
- TRY: 34.2914008
- TTD: 6.808282
- TWD: 32.0788
- TZS: 2727.879
- UAH: 41.4378606
- UGX: 3675.449
- USD: 1
- UYU: 41.6146484
- UZS: 12842.8231
- VND: 25369.9574
- XAF: 607.288802
- XAG: 0.029673942717420982
- XAU: 0.00036388976323512554
- XCD: 2.70255
- XLM: 10.65551307
- XOF: 607.288802
- XRP: 1.954115569
- YER: 250.348798
- ZAR: 17.67098
- ZMW: 26.5307373",1,1,8,29496
"The user wants to retrieve real-time pricing data for a list of items that use US dollars as the default currency. After obtaining the price of the item, it provides the ability to convert the specified amount from one currency to another. This allows users to see the prices of goods in US dollars and, if needed, to convert those prices into different currencies for further analysis or comparison.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_market_data(symbols=""COCOA,COFFEE,CORN,COTTON,GASOLINE,LUMBER,NATURALGAS,OATS,OIL,ORANGEJUICE,SOYBEAN,SUGAR,WHEAT"", base_currency=""USD""):
    url = ""https://commodities-apised.p.rapidapi.com/v1/market-data""
    querystring = {
        ""symbols"": symbols,
        ""base"": base_currency
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""commodities-apised.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def convert_currency(from_currency=""USD"", to_currency=""EUR"", amount=100):
    url = ""https://currency-converter18.p.rapidapi.com/api/v1/convert""
    querystring = {
        ""from"": from_currency,
        ""to"": to_currency,
        ""amount"": str(amount)
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""currency-converter18.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def format_market_data(data):
    print(""\n### Market Data for Commodities (in USD) ###"")
    rates = data.get('rates', {})
    if rates:
        for commodity, details in rates.items():
            open_price = details.get('open', 'N/A')
            high = details.get('high', 'N/A')
            low = details.get('low', 'N/A')
            prev = details.get('prev', 'N/A')
            current = details.get('current', 'N/A')
            print(f""\nCommodity: {commodity}"")
            print(f""  - Open Price    : {open_price} USD"")
            print(f""  - High Price    : {high} USD"")
            print(f""  - Low Price     : {low} USD"")
            print(f""  - Previous Close: {prev} USD"")
            print(f""  - Current Price : {current} USD"")
    else:
        print(""No market data available. Response:"", data)

def format_currency_conversion(data, from_currency, to_currency, amount):
    print(f""\n### Currency Conversion: {amount} {from_currency} to {to_currency} ###"")
    result = data.get('result', {})
    if result:
        converted_amount = result.get('convertedAmount', 'Unknown')
        print(f""Converted Amount: {amount} {from_currency} = {converted_amount} {to_currency}"")
    else:
        print(""Currency conversion data is not available. Response:"", data)

if __name__ == '__main__':
    # Step 1: Retrieve market data for commodities in USD
    market_data = get_market_data()
    format_market_data(market_data)

    # Step 2: Convert a specified amount from USD to EUR
    conversion_result = convert_currency(from_currency=""USD"", to_currency=""EUR"", amount=150)
    format_currency_conversion(conversion_result, ""USD"", ""EUR"", 150)","### Market Data for Commodities (in USD) ###

Commodity: COCOA
  - Open Price    : 6696 USD
  - High Price    : 6916 USD
  - Low Price     : 6421 USD
  - Previous Close: 6724 USD
  - Current Price : 6782 USD

Commodity: COFFEE
  - Open Price    : 246.27 USD
  - High Price    : 248.37 USD
  - Low Price     : 243.98 USD
  - Previous Close: 244.28 USD
  - Current Price : 247.37 USD

Commodity: CORN
  - Open Price    : 420.72 USD
  - High Price    : 420.72 USD
  - Low Price     : 413.7 USD
  - Previous Close: 420.72 USD
  - Current Price : 414.43 USD

Commodity: COTTON
  - Open Price    : 71.33 USD
  - High Price    : 71.35 USD
  - Low Price     : 70.49 USD
  - Previous Close: 71.44 USD
  - Current Price : 70.54 USD

Commodity: GASOLINE
  - Open Price    : 2.0444 USD
  - High Price    : 2.0922 USD
  - Low Price     : 2.0342 USD
  - Previous Close: 2.0464 USD
  - Current Price : 2.0889 USD

Commodity: LUMBER
  - Open Price    : 522.8 USD
  - High Price    : 525.16 USD
  - Low Price     : 520.91 USD
  - Previous Close: 521.29 USD
  - Current Price : 522.59 USD

Commodity: NATURALGAS
  - Open Price    : 2.982 USD
  - High Price    : 3.079 USD
  - Low Price     : 2.942 USD
  - Previous Close: 2.972 USD
  - Current Price : 3.038 USD

Commodity: OATS
  - Open Price    : 3.8587 USD
  - High Price    : 3.8725 USD
  - Low Price     : 3.8311 USD
  - Previous Close: 3.8638 USD
  - Current Price : 3.8522 USD

Commodity: OIL
  - Open Price    : 70.295 USD
  - High Price    : 71.925 USD
  - Low Price     : 69.96 USD
  - Previous Close: 70.295 USD
  - Current Price : 71.675 USD

Commodity: ORANGEJUICE
  - Open Price    : 4.60363 USD
  - High Price    : 4.72281 USD
  - Low Price     : 4.57388 USD
  - Previous Close: 4.60307 USD
  - Current Price : 4.70631 USD

Commodity: SOYBEAN
  - Open Price    : 1002.5 USD
  - High Price    : 1004.6 USD
  - Low Price     : 992.9 USD
  - Previous Close: 1012.5 USD
  - Current Price : 996.7 USD

Commodity: SUGAR
  - Open Price    : 0.21874 USD
  - High Price    : 0.22004 USD
  - Low Price     : 0.216 USD
  - Previous Close: 0.21824 USD
  - Current Price : 0.21796 USD

Commodity: WHEAT
  - Open Price    : 583.35 USD
  - High Price    : 583.39 USD
  - Low Price     : 565.13 USD
  - Previous Close: 580.15 USD
  - Current Price : 567.41 USD

### Currency Conversion: 150 USD to EUR ###
Converted Amount: 150 USD = 138.8925 EUR",1,1,9,29601
"Develop a system that allows users to search for recipes based on specific criteria such as ingredients, preparation time, nutritional values, and tags, and then retrieve detailed information about any selected recipe, including ingredients, steps, and nutritional data. This system should provide a seamless interface for filtering recipes and displaying comprehensive recipe details.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def search_recipes(name=None, tags=None, includeIngredients=None, excludeIngredients=None, maxPrepareTime=None,
                   maxCookTime=None, maxCalories=None, maxNetCarbs=None, maxSugar=None, maxAddedSugar=None, limit=10):
    url = ""https://low-carb-recipes.p.rapidapi.com/search""
    querystring = {
        ""name"": name,
        ""tags"": tags,
        ""includeIngredients"": includeIngredients,
        ""excludeIngredients"": excludeIngredients,
        ""maxPrepareTime"": maxPrepareTime,
        ""maxCookTime"": maxCookTime,
        ""maxCalories"": maxCalories,
        ""maxNetCarbs"": maxNetCarbs,
        ""maxSugar"": maxSugar,
        ""maxAddedSugar"": maxAddedSugar,
        ""limit"": limit
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""low-carb-recipes.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params={k: v for k, v in querystring.items() if v is not None})
    return response.json()


def get_recipe_by_id(recipe_id):
    url = f""https://low-carb-recipes.p.rapidapi.com/recipes/{recipe_id}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""low-carb-recipes.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()


def format_search_results(results):
    print(""\n### Recipe Search Results ###"")
    if results:
        for idx, recipe in enumerate(results, 1):
            title = recipe.get('name', 'No title available')
            tags = ', '.join(recipe.get('tags', []))
            calories = recipe.get('caloriesKCal', 'Unknown')
            cook_time = recipe.get('cookTime', 'Unknown')
            net_carbs = recipe.get('netCarbs', 'Unknown')
            print(f""\nRecipe {idx}:"")
            print(f""  - Title       : {title}"")
            print(f""  - Tags        : {tags}"")
            print(f""  - Calories    : {calories} kcal"")
            print(f""  - Cook Time   : {cook_time} minutes"")
            print(f""  - Net Carbs   : {net_carbs}g"")
    else:
        print(""No recipes found based on the search criteria."")


def format_detailed_recipe(recipe):
    print(""\n### Detailed Recipe Information ###"")
    title = recipe.get('name', 'No title available')
    description = recipe.get('description', 'No description available')
    prepare_time = recipe.get('prepareTime', 'Unknown')
    cook_time = recipe.get('cookTime', 'Unknown')
    ingredients = recipe.get('ingredients', [])
    steps = recipe.get('steps', [])
    calories = recipe.get('nutrients', {}).get('caloriesKCal', 'Unknown')
    net_carbs = recipe.get('nutrients', {}).get('netCarbs', 'Unknown')
    image = recipe.get('image', 'No image available')

    print(f""Recipe Title : {title}"")
    print(f""Description  : {description[:100]}..."")  # Show only the first 100 characters
    print(f""Prepare Time : {prepare_time} minutes"")
    print(f""Cook Time    : {cook_time} minutes"")
    print(f""Calories     : {calories} kcal"")
    print(f""Net Carbs    : {net_carbs}g"")

    print(""\nIngredients  :"")
    for ingredient in ingredients:
        name = ingredient.get('name', 'Unknown ingredient')
        desc = ingredient.get('servingSize', {}).get('desc', 'No description')
        qty = ingredient.get('servingSize', {}).get('qty', 'Unknown quantity')
        print(f""  - {name}: {desc} ({qty}g)"")

    print(""\nSteps        :"")
    for idx, step in enumerate(steps, 1):
        print(f""  {idx}. {step}"")

    print(f""\nImage        : {image}"")


if __name__ == '__main__':
    # Step 1: Search for recipes based on specific criteria
    search_results = search_recipes(
        name=""cake"",
        tags=""keto;dairy-free"",
        includeIngredients=""egg;butter"",
        excludeIngredients=""cinnamon"",
        maxPrepareTime=""10"",
        maxCookTime=""20"",
        maxCalories=""500"",
        maxNetCarbs=""5"",
        maxSugar=""3"",
        maxAddedSugar=""0"",
        limit=""5""
    )
    format_search_results(search_results)

    # Step 2: Retrieve details for the first recipe in the search results
    if search_results and 'id' in search_results[0]:
        recipe_id = search_results[0]['id']
        detailed_recipe = get_recipe_by_id(recipe_id)
        format_detailed_recipe(detailed_recipe)
",Fail,-1,-1,31,86490
"Users navigate the list of Chinese food recipes by displaying basic information such as the title, difficulty level and picture of each recipe. Users can select a specific recipe from the list to see details, including ingredients, preparation time, step-by-step cooking instructions, and other details such as serving size and difficulty.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_chinese_food_list():
    url = ""https://chinese-food-db.p.rapidapi.com/""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""chinese-food-db.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def get_chinese_food_details(recipe_id):
    url = f""https://chinese-food-db.p.rapidapi.com/{recipe_id}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""chinese-food-db.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def format_chinese_food_list(food_list):
    print(""\n### Chinese Food List (First Item Example) ###"")
    if food_list:
        first_item = food_list[0]
        title = first_item.get('title', 'No title available')
        category = first_item.get('category', 'No category available')
        difficulty = first_item.get('difficulty', 'No difficulty level')
        image = first_item.get('image', 'No image available')
        print(f""Recipe Title: {title}"")
        print(f""Category    : {category}"")
        print(f""Difficulty  : {difficulty}"")
        print(f""Image       : {image}"")
    else:
        print(""No Chinese food available."")

def format_chinese_food_details(recipe):
    print(""\n### Detailed Recipe Information ###"")
    title = recipe.get('title', 'No title available')
    description = recipe.get('description', 'No description available')
    ingredients = recipe.get('ingredients', [])
    instructions = recipe.get('instructions', [])
    category = recipe.get('category', 'No category available')
    difficulty = recipe.get('difficulty', 'No difficulty level')
    image = recipe.get('image', 'No image available')

    print(f""Recipe Title : {title}"")
    print(f""Category     : {category}"")
    print(f""Difficulty   : {difficulty}"")
    print(f""Description  : {description[:100]}..."")  # Truncate to the first 100 characters

    print(""\nIngredients  :"")
    for ingredient in ingredients:
        print(f""  - {ingredient}"")

    print(""\nInstructions :"")
    for idx, step in enumerate(instructions, 1):
        print(f""  {idx}. {step}"")

    print(f""\nImage        : {image}"")

if __name__ == '__main__':
    # Step 1: Retrieve and display the list of Chinese recipes
    chinese_food_list = get_chinese_food_list()
    format_chinese_food_list(chinese_food_list)

    # Step 2: Retrieve and display detailed information for the first recipe in the list
    if chinese_food_list and len(chinese_food_list) > 0:
        first_recipe_id = chinese_food_list[0]['id']
        detailed_recipe_info = get_chinese_food_details(first_recipe_id)
        format_chinese_food_details(detailed_recipe_info)
",Fail,-1,-1,33,98307
"Users are asked to view a list of vegetarian recipes, including title, difficulty, and a representative image of each recipe. In addition, provide the ability to access random recipes, displaying comprehensive information such as preparation time, cooking time, ingredients, step-by-step instructions, and nutritional details.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def get_vegan_food_list():
    url = ""https://the-vegan-recipes-db.p.rapidapi.com/""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""the-vegan-recipes-db.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()


def get_random_vegan_recipe():
    url = ""https://low-carb-recipes.p.rapidapi.com/random""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""low-carb-recipes.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()


def format_vegan_food_list(food_list):
    print(""\n### Vegan Food List (First Item Example) ###"")
    if food_list:
        first_item = food_list[0]
        title = first_item.get('title', 'No title available')
        difficulty = first_item.get('difficulty', 'No difficulty level')
        image = first_item.get('image', 'No image available')
        print(f""Recipe Title: {title}"")
        print(f""Difficulty   : {difficulty}"")
        print(f""Image        : {image}"")
    else:
        print(""No vegan food available."")


def format_random_vegan_recipe(recipe):
    print(""\n### Random Vegan Recipe ###"")
    title = recipe.get('name', 'No title available')
    tags = ', '.join(recipe.get('tags', []))
    description = recipe.get('description', 'No description available')
    prepare_time = recipe.get('prepareTime', 'Unknown')
    ingredients = recipe.get('ingredients', [])
    steps = recipe.get('steps', [])
    servings = recipe.get('servings', 'Unknown')
    calories = recipe.get('nutrients', {}).get('caloriesKCal', 'Unknown')
    image = recipe.get('image', 'No image available')

    print(f""Recipe Title : {title}"")
    print(f""Tags         : {tags}"")
    print(f""Description  : {description[:100]}..."")  # Show only the first 100 characters
    print(f""Prepare Time : {prepare_time} minutes"")

    print(""\nIngredients  :"")
    for ingredient in ingredients:
        name = ingredient.get('name', 'Unknown ingredient')
        desc = ingredient.get('servingSize', {}).get('desc', 'No description')
        qty = ingredient.get('servingSize', {}).get('qty', 'Unknown quantity')
        print(f""  - {name}: {desc} ({qty}g)"")

    print(""\nSteps        :"")
    for idx, step in enumerate(steps, 1):
        print(f""  {idx}. {step}"")

    print(f""\nServings     : {servings}"")
    print(f""Calories     : {calories} kcal per serving"")
    print(f""Image        : {image}"")


if __name__ == '__main__':
    # Step 1: Retrieve and display the list of vegan recipes
    vegan_food_list = get_vegan_food_list()
    format_vegan_food_list(vegan_food_list)

    # Step 2: Retrieve and display details of a random vegan recipe
    random_vegan_recipe = get_random_vegan_recipe()
    format_random_vegan_recipe(random_vegan_recipe)
","### Vegan Food List (First Item Example) ###
Recipe Title: Dark chocolate bark with sea salt, rosemary, pistachios and candied citrus
Difficulty   : Easy
Image        : https://apipics.s3.amazonaws.com/vegan_api/1.jpg

### Random Vegan Recipe ###
Recipe Title : Keto Coconut Chai Smoothie
Tags         : meal-plan-ok, quick-easy, paleo, vegan, vegetarian, pescatarian, dairy-free, gluten-free, wheat-free, egg-free, peanut-free, soy-free, fish-free, shellfish-free, pork-free, beef-free, chicken-free, lchf, relevant-meal--beverages, snacks, beverages
Description  : If you need a light breakfast or you simply want a drinkable treat, this coconut chai smoothie recip...
Prepare Time : 15 minutes

Ingredients  :
  - Almond Milk, Vanilla Or Other Flavors, Unsweetened: 2 cup (2.0g)
  - Spiced Chai Tea Bags by Trader Joe's: 1 bag (1.0g)
  - Frozen Coconut Chunks By Trader Joe's: 1 ounce (1.0g)

Steps        :
  1. Heat the almond milk over the stove to a strong simmer. Steep the chai tea in the almond milk for 5-10 minutes, depending on how strong you want the flavor to be. After the milk has steeped, discard the tea bag and store the milk in the freezer until it becomes cold.
  2. For the coconut, cut it into cubes if necessary. The coconut must be frozen for this recipe, so have this prepare in advance. When the almond milk is cold, blend it with the frozen coconut cubes.
  3. You may refrigerate your smoothie for a thicker consistency before consuming. Pour it in a tall glass. Enjoy !

Servings     : 1
Calories     : 128.935 kcal per serving
Image        : https://tinyurl.com/2p82zzca/7c3d80d3-c7c7-46ab-9f3c-6d5a77284474.png",1,0,15,45645
"Users verify the halal status of animal and food additives using their name or designated E number. Users can enter the name of an animal to determine if it is halal, or they can enter an E number to check if a particular food additive meets halal standards. This ensures that users have access to halal information on animal-based products and common food additives.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def check_halal_animal(animal_name):
    url = f""https://halal-food-checker.p.rapidapi.com/check-halal-animal/{animal_name}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""halal-food-checker.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def check_halal_e_number(e_number):
    url = f""https://halal-food-checker.p.rapidapi.com/check-halal-number/{e_number}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""halal-food-checker.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def print_halal_animal_result(result):
    if result and 'animal' in result:
        animal = result['animal']
        halal_status = ""Halal"" if result['is_halal'] else ""Not Halal""
        print(f""Animal Check Result:"")
        print(f""  - Animal: {animal}"")
        print(f""  - Status: {halal_status}\n"")

def print_halal_e_number_result(result):
    if result and 'e_number' in result:
        e_number = result['e_number']
        name = result.get('name', 'Unknown')
        halal_status = ""Halal"" if result['is_halal'] else ""Not Halal""
        print(f""E Number Check Result:"")
        print(f""  - E Number: {e_number}"")
        print(f""  - Name: {name}"")
        print(f""  - Status: {halal_status}\n"")

if __name__ == '__main__':
    # Step 1: Check if a given animal is halal
    animal_to_check = ""Dog""  # Example animal
    animal_halal_status = check_halal_animal(animal_to_check)
    print_halal_animal_result(animal_halal_status)

    # Step 2: Check if a given E number is halal
    e_number_to_check = ""E101""  # Example E number
    e_number_halal_status = check_halal_e_number(e_number_to_check)
    print_halal_e_number_result(e_number_halal_status)
","Animal Check Result:
  - Animal: Dog
  - Status: Not Halal

E Number Check Result:
  - E Number: E101
  - Name: Riboflavin/Lactofavin/Vitamin B2 *
  - Status: Not Halal
",1,1,9,31653
Give me some hotel information in New York City.,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


# API 1: Search Hotels in New York City
def search_hotels(query=""new york""):
    url = ""https://tripadvisor-scraper.p.rapidapi.com/hotels/search""
    querystring = {""query"": query}

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""tripadvisor-scraper.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        return {""error"": f""Failed to search hotels: {response.status_code}""}


# API 2: List Hotels with Detailed Information
def list_hotels(query=""new york"", page=1):
    url = ""https://tripadvisor-scraper.p.rapidapi.com/hotels/list""
    querystring = {""query"": query, ""page"": str(page)}

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""tripadvisor-scraper.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        return {""error"": f""Failed to list hotels: {response.status_code}""}


if __name__ == '__main__':
    # Step 1: Search for hotels in New York
    search_results = search_hotels()

    if search_results and isinstance(search_results, list) and len(search_results) > 0:
        for result in search_results:

            # If the result type is an accommodation, fetch detailed information
            if result['type'] == 'accommodation':

                # Step 2: List hotels with detailed information (pagination starts from page 1)
                hotel_list = list_hotels(query=result['name'], page=1)

                if 'results' in hotel_list and len(hotel_list['results']) > 0:
                    for hotel in hotel_list['results']:
                        print(""=== Hotel Details ==="")
                        print(f""Name: {hotel['name']}"")
                        print(f""Rating: {hotel['rating']} / 5"")
                        print(f""Reviews: {hotel['reviews']}"")

                        # Check if price range information is available
                        if 'price_range_usd' in hotel:
                            print(
                                f""Price Range: ${hotel['price_range_usd']['min']} - ${hotel['price_range_usd']['max']}"")
                        else:
                            print(""Price Range: Information not available"")

                        # Check if detailed address is available
                        if 'detailed_address' in hotel:
                            address = hotel['detailed_address']
                            street = address.get('street', 'Street info not available')
                            city = address.get('city', 'City info not available')
                            print(f""Address: {street}, {city}"")
                        else:
                            print(""Address: Information not available"")

                        print(f""Phone: {hotel.get('phone', 'Phone number not available')}"")
                        print(f""Featured Image: {hotel.get('featured_image', 'Image not available')}"")
                        print(
                            f""Amenities: {', '.join(hotel['amenities']) if hotel.get('amenities') else 'No amenities listed'}"")
                        print(f""Link to Hotel: {hotel['link']}"")
                        print(""====================\n"")
                else:
                    print(""No detailed hotel information found for the specified query."")
    else:
        print(""No hotels found in the search results."")
","=== Hotel Details ===
Name: The New Yorker, A Wyndham Hotel
Rating: 4 / 5
Reviews: 16534
Price Range: Information not available
Address: Information not available
Phone: None
Featured Image: https://dynamic-media-cdn.tripadvisor.com/media/photo-o/2b/87/f3/7b/welcome-to-the-wyndham.jpg
Amenities: No amenities listed
Link to Hotel: https://www.tripadvisor.com/Hotel_Review-g60763-d122005-Reviews-The_New_Yorker_A_Wyndham_Hotel-New_York_City_New_York.html
====================

=== Hotel Details ===
Name: New York Hilton Midtown
Rating: 4 / 5
Reviews: 14987
Price Range: Information not available
Address: Information not available
Phone: 1 (855) 605-0316
Featured Image: https://dynamic-media-cdn.tripadvisor.com/media/photo-o/2d/dd/2c/6c/guest-room.jpg
Amenities: No amenities listed
Link to Hotel: https://www.tripadvisor.com/Hotel_Review-g60763-d611947-Reviews-New_York_Hilton_Midtown-New_York_City_New_York.html
====================",1,1,19,61978
Give me some information about restaurants in New York City.,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


# API 1: List Restaurants by Search Query
def list_restaurants_by_query(query=""new york"", page=1):
    url = ""https://tripadvisor-scraper.p.rapidapi.com/restaurants/list""
    querystring = {""query"": query, ""page"": str(page)}

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""tripadvisor-scraper.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        return {""error"": f""Failed to list restaurants by query: {response.status_code}""}


# API 2: List Restaurants by ID
def list_restaurants_by_id(restaurant_id, page=1):
    url = ""https://tripadvisor-scraper.p.rapidapi.com/restaurants/list""
    querystring = {""query"": str(restaurant_id), ""page"": str(page)}

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""tripadvisor-scraper.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        return {""error"": f""Failed to list restaurants by ID: {response.status_code}""}


if __name__ == '__main__':
    # Step 1: List restaurants in New York by a search query
    search_query = ""new york""
    page = 1
    restaurant_list = list_restaurants_by_query(query=search_query, page=page)

    if 'results' in restaurant_list and len(restaurant_list['results']) > 0:
        print(""List of Restaurants:"")
        for restaurant in restaurant_list['results'][:6]:
            print(
                f""Name: {restaurant['name']}, Rating: {restaurant['rating']}, Cuisines: {', '.join(restaurant['cuisines'])}"")
            print(f""Link: {restaurant['link']}"")
            print(f""Featured Image: {restaurant['featured_image']}\n"")

            # Use the restaurant ID to fetch additional details if needed
            restaurant_id = restaurant['id']

            # Step 2: Retrieve detailed restaurant information using the ID
            detailed_info = list_restaurants_by_id(restaurant_id, page=1)

            if 'results' in detailed_info and len(detailed_info['results']) > 0:
                for detailed_restaurant in detailed_info['results']:
                    print(""=== Detailed Restaurant Information ==="")
                    print(f""Name: {detailed_restaurant['name']}"")
                    print(f""Rating: {detailed_restaurant['rating']} / 5"")
                    print(f""Reviews: {detailed_restaurant['reviews']}"")

                    # Handle possible null fields gracefully
                    price_range = detailed_restaurant.get('price_range_usd', 'Price information not available')
                    menu_link = detailed_restaurant.get('menu_link', 'Menu link not available')
                    reservation_link = detailed_restaurant.get('reservation_link', 'Reservation link not available')

                    print(f""Price Range: {price_range}"")
                    print(f""Menu: {menu_link}"")
                    print(f""Reservation: {reservation_link}"")
                    print(f""Has Delivery: {'Yes' if detailed_restaurant['has_delivery'] else 'No'}"")
                    print(f""Latitude: {detailed_restaurant['latitude']}, Longitude: {detailed_restaurant['longitude']}"")
                    print(f""Cuisines: {', '.join(detailed_restaurant['cuisines'])}"")
                    print(f""Link to Restaurant: {detailed_restaurant['link']}"")
                    print(""============================\n"")

",Fail,-1,-1,28,96264
I want to know some sports data.,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def list_of_sports():
    url = ""https://pinnacle-odds.p.rapidapi.com/kit/v1/sports""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""pinnacle-odds.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def sports():
    url = ""https://odds-api1.p.rapidapi.com/sports""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""odds-api1.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def merge_sports_data():
    list_of_sports_data = list_of_sports()
    sports_data = sports()

    merged_data = []

    # Convert sports_data from object-like to a list of dictionaries for easier processing
    sports_list = [v for k, v in sports_data.items() if k.isdigit()]

    # Match data based on sport name
    for sport in list_of_sports_data:
        matched_sport = next((s for s in sports_list if s['name'].lower() == sport['name'].lower()), None)
        if matched_sport:
            merged_data.append({
                ""ID"": sport.get('id'),
                ""Sport Name"": sport.get('name'),
                ""Slug"": matched_sport.get('slug'),
                ""Last API Call Timestamp"": sport.get('last_call'),
                ""Last Event Timestamp"": sport.get('last'),
                ""Special Event Timestamp"": sport.get('special_last')
            })
        else:
            # Sport from List of Sports not found in Sports data
            merged_data.append({
                ""ID"": sport.get('id'),
                ""Sport Name"": sport.get('name'),
                ""Slug"": ""Not Available"",
                ""Last API Call Timestamp"": sport.get('last_call'),
                ""Last Event Timestamp"": sport.get('last'),
                ""Special Event Timestamp"": sport.get('special_last')
            })

    return merged_data

def display_user_friendly(data):
    for entry in data:
        print(f""Sport ID: {entry['ID']}"")
        print(f""Sport Name: {entry['Sport Name']}"")
        print(f""Slug: {entry['Slug']}"")
        print(f""Last API Call Timestamp: {entry['Last API Call Timestamp']}"")
        print(f""Last Event Timestamp: {entry['Last Event Timestamp']}"")
        print(f""Special Event Timestamp: {entry['Special Event Timestamp']}"")
        print("""")

if __name__ == '__main__':
    unified_sports_data = merge_sports_data()
    display_user_friendly(unified_sports_data)
",Fail,-1,-1,27,68769
Tell me about some sports.,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_sports():
    url = ""https://odds-api1.p.rapidapi.com/sports""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""odds-api1.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def get_sports_list():
    url = ""https://global-data.p.rapidapi.com/sport/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""global-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def merge_sports_data():
    sports_data = get_sports()
    sports_list_data = get_sports_list()

    merged_data = []

    # Convert sports_data to a list of dictionaries
    sports_list = [v for k, v in sports_data.items() if k.isdigit()]

    # Merge data from both sources
    for sport in sports_list:
        sport_name = sport.get('name')
        sport_slug = sport.get('slug')

        # Find matching data in sports_list_data based on sport name
        matching_sport = next((item for item in sports_list_data if item['name'].lower() == sport_name.lower()), None)

        if matching_sport:
            merged_data.append({
                ""Full Name"": matching_sport.get('name'),
                ""Slug"": sport_slug,
                ""Short Name"": matching_sport.get('shortName', 'N/A'),
                ""Sport ID"": matching_sport.get('id')
            })
        else:
            # If no match found, use available data
            merged_data.append({
                ""Full Name"": sport_name,
                ""Slug"": sport_slug,
                ""Short Name"": ""N/A"",
                ""Sport ID"": ""Not Available""
            })

    return merged_data

def display_user_friendly_data(data):
    if not data:
        print(""No data to display."")
        return

    for entry in data[:10]:
        print(""==================================="")
        print(f""Full Name: {entry['Full Name']}"")
        print(f""Slug: {entry['Slug']}"")
        print(f""Short Name: {entry['Short Name']}"")
        print(f""Sport ID: {entry['Sport ID']}"")
        print(""===================================\n"")

if __name__ == '__main__':
    unified_sports_data = merge_sports_data()
    display_user_friendly_data(unified_sports_data)
",Fail,-1,-1,28,85736
I want to know all about the New York Knicks team in the NBA.,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_team_details(team_id):
    url = ""https://nba-api-free-data.p.rapidapi.com/nba-team-info/v1/data""
    querystring = {""id"": team_id}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""nba-api-free-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_team_players(team_id):
    url = ""https://nba-api-free-data.p.rapidapi.com/nba-player-listing/v1/data""
    querystring = {""id"": team_id}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""nba-api-free-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def merge_team_and_player_data(team_id):
    team_details = get_team_details(team_id)
    player_details = get_team_players(team_id)

    merged_data = {
        ""Team Information"": {
            ""Name"": team_details['team'].get('displayName'),
            ""Location"": team_details['team'].get('location'),
            ""Abbreviation"": team_details['team'].get('abbreviation'),
            ""Record"": team_details['team'].get('recordSummary'),
            ""Standing"": team_details['team'].get('standingSummary'),
            ""Season"": team_details['season'].get('displayName'),
            ""Coach"": [
                {
                    ""Name"": f""{coach.get('firstName')} {coach.get('lastName')}"",
                    ""Experience"": coach.get('experience')
                } for coach in team_details.get('coach', [])
            ],
            ""Team Logo"": team_details['team'].get('logo'),
            ""Team Color"": team_details['team'].get('color')
        },
        ""Players"": []
    }

    # Add player details if available
    if 'athletes' in player_details:
        for player in player_details['athletes']:
            player_info = {
                ""Full Name"": player.get('fullName'),
                ""Position"": player.get('position', {}).get('displayName', 'N/A'),
                ""Jersey Number"": player.get('jersey', 'N/A'),
                ""Height"": player.get('displayHeight', 'N/A'),
                ""Weight"": player.get('displayWeight', 'N/A'),
                ""Age"": player.get('age', 'N/A'),
                ""Experience"": player.get('experience', {}).get('years', 'N/A'),
                ""Contract Status"": {
                    ""Active"": player.get('contract', {}).get('active', False),
                    ""Salary"": player.get('contract', {}).get('salary', 'N/A'),
                    ""Years Remaining"": player.get('contract', {}).get('yearsRemaining', 'N/A')
                },
                ""Headshot"": player.get('headshot', {}).get('href', 'N/A')
            }
            merged_data['Players'].append(player_info)

    return merged_data

def display_user_friendly_data(data):
    if not data:
        print(""No data to display."")
        return

    print(""==================================="")
    print(""Team Information:"")
    print(f""Name: {data['Team Information']['Name']}"")
    print(f""Location: {data['Team Information']['Location']}"")
    print(f""Abbreviation: {data['Team Information']['Abbreviation']}"")
    print(f""Record: {data['Team Information']['Record']}"")
    print(f""Standing: {data['Team Information']['Standing']}"")
    print(f""Season: {data['Team Information']['Season']}"")
    print(""Coach Information:"")
    for coach in data['Team Information']['Coach']:
        print(f""  - Name: {coach['Name']}, Experience: {coach['Experience']} years"")
    print(f""Team Logo: {data['Team Information']['Team Logo']}"")
    print(f""Team Color: {data['Team Information']['Team Color']}"")
    print(""\nPlayers:"")
    for player in data['Players']:
        print(""-----------------------------------"")
        print(f""Full Name: {player['Full Name']}"")
        print(f""Position: {player['Position']}"")
        print(f""Jersey Number: {player['Jersey Number']}"")
        print(f""Height: {player['Height']}"")
        print(f""Weight: {player['Weight']}"")
        print(f""Age: {player['Age']}"")
        print(f""Experience: {player['Experience']} years"")
        print(""Contract Status:"")
        print(f""  - Active: {player['Contract Status']['Active']}"")
        print(f""  - Salary: {player['Contract Status']['Salary']}"")
        print(f""  - Years Remaining: {player['Contract Status']['Years Remaining']}"")
        print(f""Headshot URL: {player['Headshot']}"")
        print(""-----------------------------------"")
    print(""==================================="")

if __name__ == '__main__':
    team_id = ""18""  # Example Team ID
    unified_team_data = merge_team_and_player_data(team_id)
    display_user_friendly_data(unified_team_data)
",Fail,-1,-1,29,92249
I want logos for all NBA teams.,"import requests
API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
def get_all_nba_teams():
    url = ""https://nba-api-free-data.p.rapidapi.com/nba-team-list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""nba-api-free-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def get_team_logo(team_id):
    url = ""https://nba-api-free-data.p.rapidapi.com/football-team-logo""
    querystring = {""teamid"": team_id}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""nba-api-free-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

if __name__ == '__main__':
    # Step 1: Get all NBA teams
    nba_teams = get_all_nba_teams()
    teams = nba_teams.get('teams', [])

    # Step 2: For each team, get the logo and display it along with the team details
    for team in teams:
        team_id = team['id']
        team_logo_data = get_team_logo(team_id)

        # Extract the logo URL if available
        if team_logo_data.get('status') == 'success':
            team_logo_url = team_logo_data['response']['url']
        else:
            team_logo_url = ""No logo available""

        # Print team information including the logo URL
        print(f""Team Name: {team['displayName']}"")
        print(f""Abbreviation: {team['abbreviation']}"")
        print(f""Location: {team['location']}"")
        print(f""Logo URL: {team_logo_url}\n"")
",Fail,-1,-1,30,83940
I would like to know holiday information in Brazil,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
def get_country_list():
    url = ""https://global-data.p.rapidapi.com/country/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""global-data.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()


def get_city_holidays(cidade, estado, ano):
    url = ""https://feriados-brasileiros1.p.rapidapi.com/read""
    querystring = {""cidade"": cidade, ""estado"": estado, ""ano"": ano}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""feriados-brasileiros1.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


if __name__ == '__main__':
    # Step 1: Get the list of countries
    country_list = get_country_list()

    # Step 2: Print out the country list for user to identify if Brazil is present
    brazil_id = None
    for country in country_list:
        country_name = country.get(""name"")
        country_short_name = country.get(""shortName"")
        country_id = country.get(""id"")

        # Checking if the country is Brazil
        if country_name.lower() == ""brazil"":
            brazil_id = country_id


    # Step 3: If Brazil is in the list, proceed to get holidays for a city in Brazil
    if brazil_id is not None:
        # For demonstration purposes, let's assume we want holidays for Belo Horizonte, Minas Gerais, in 2023
        cidade = ""belo_horizonte""
        estado = ""MG""
        ano = ""2023""
        holidays = get_city_holidays(cidade, estado, ano)

        # Step 4: Display the holidays in a user-friendly format
        print(""\n*** Holidays in Belo Horizonte, Minas Gerais, Brazil for 2023 ***\n"")
        for holiday in holidays:
            holiday_date = holiday.get(""data"")
            holiday_name = holiday.get(""feriado"")
            holiday_type = holiday.get(""tipo"")
            holiday_type_desc = ""Mandatory Holiday"" if holiday_type == ""feriado"" else ""Optional Holiday""
            print(f""Date: {holiday_date} - Holiday: {holiday_name} ({holiday_type_desc})"")

        print(""\n*** End of Holidays Information ***"")
    else:
        print(""\nBrazil is not in the country list. Unable to retrieve holiday information.\n"")
","*** Holidays in Belo Horizonte, Minas Gerais, Brazil for 2023 ***

Date: 01/01/2023 - Holiday: Ano Novo (Mandatory Holiday)
Date: 07/04/2023 - Holiday: Sexta-Feira Santa (Mandatory Holiday)
Date: 07/04/2023 - Holiday: Sexta-feira Santa (Mandatory Holiday)
Date: 21/04/2023 - Holiday: Dia de Tiradentes (Mandatory Holiday)
Date: 01/05/2023 - Holiday: Dia do Trabalho (Mandatory Holiday)
Date: 08/06/2023 - Holiday: Corpus Christi (Mandatory Holiday)
Date: 15/08/2023 - Holiday: Assuncao de Nossa Senhora (Mandatory Holiday)
Date: 07/09/2023 - Holiday: Independencia do Brasil (Mandatory Holiday)
Date: 12/10/2023 - Holiday: Nossa Senhora Aparecida (Mandatory Holiday)
Date: 02/11/2023 - Holiday: Dia de Finados (Mandatory Holiday)
Date: 15/11/2023 - Holiday: Proclamacao da Republica (Mandatory Holiday)
Date: 08/12/2023 - Holiday: Imaculada Conceicao (Mandatory Holiday)
Date: 25/12/2023 - Holiday: Natal (Mandatory Holiday)
Date: 20/02/2023 - Holiday: Carnaval (Optional Holiday)
Date: 21/02/2023 - Holiday: Carnaval (Optional Holiday)
Date: 21/02/2023 - Holiday: Terca de Carnaval (Optional Holiday)
Date: 22/02/2023 - Holiday: Carnaval (Optional Holiday)
Date: 08/06/2023 - Holiday: Corpus Christi (Optional Holiday)
Date: 15/10/2023 - Holiday: Dia do Professor (Optional Holiday)
Date: 28/10/2023 - Holiday: Dia do Servidor Publico (Optional Holiday)

*** End of Holidays Information ***",1,1,12,33708
I would like to know comprehensive information about airports worldwide and the corresponding airline logos.,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def get_airports_list():
    """"""
    Fetches a comprehensive list of airports worldwide, including details like location, timezone, and country.

    Returns:
        JSON response containing the list of airports and relevant information.
    """"""
    url = ""https://flight-radar1.p.rapidapi.com/airports/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""flight-radar1.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers)
    return response.json()


def get_airline_logos():
    """"""
    Retrieves detailed information about airline logos, including metadata and URLs for accessing the logos.

    Returns:
        JSON response containing airline logos metadata.
    """"""
    url = ""https://flight-radar1.p.rapidapi.com/airlines/get-logos""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""flight-radar1.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers)
    return response.json()


def display_airports_and_logos(airports_data, logos_data):
    """"""
    Displays a user-friendly view of airports and associated airline logos.

    Parameters:
        airports_data (dict): JSON response containing the list of airports.
        logos_data (dict): JSON response containing the list of airline logos.
    """"""
    if 'rows' not in airports_data or not airports_data['rows']:
        print(""No airport data available."")
        return

    print(""\nList of Airports:"")
    print(""-"" * 60)
    for airport in airports_data['rows'][:10]:  # Limit to 10 airports for display
        print(f""Airport: {airport['name']} ({airport['iata']}/{airport['icao']})"")
        print(f""Location: {airport['city']}, {airport['country']}"")
        print(f""Coordinates: ({airport['lat']}, {airport['lon']})"")
        print(f""Timezone: {airport['timezone']['name']} (UTC {airport['timezone']['offsetHours']})"")
        print(""-"" * 60)

    print(""\nAirline Logos:"")
    print(""-"" * 60)
    airlines = logos_data.get('result', {}).get('response', {}).get('airlines', {}).get('logotypes', [])
    for airline in airlines[:10]:  # Limit to 10 logos for display
        logo = airline['file']
        print(f""Airline Logo: {logo['name']}"")
        print(f""Logo URL: {logo['url']}"")
        print(""-"" * 60)


if __name__ == '__main__':
    # Step 1: Retrieve the list of worldwide airports
    print(""Fetching the list of worldwide airports..."")
    airports_data = get_airports_list()

    # Step 2: Retrieve the airline logos information
    print(""Fetching airline logos..."")
    logos_data = get_airline_logos()

    # Step 3: Display the airports and airline logos in a user-friendly way
    display_airports_and_logos(airports_data, logos_data)
","List of Airports:
------------------------------------------------------------
Airport: A Coruna Airport (LCG/LECO)
Location: A Coruna, Spain
Coordinates: (43.302059, -8.37725)
Timezone: Europe/Madrid (UTC 2:00)
------------------------------------------------------------
Airport: Aachen Merzbruck Airport (AAH/EDKA)
Location: Aachen, Germany
Coordinates: (50.821899, 6.184759)
Timezone: Europe/Berlin (UTC 2:00)
------------------------------------------------------------
Airport: Aalborg Airport (AAL/EKYT)
Location: Aalborg, Denmark
Coordinates: (57.095112, 9.855172)
Timezone: Europe/Copenhagen (UTC 2:00)
------------------------------------------------------------
Airport: Aarhus Airport (AAR/EKAH)
Location: Aarhus, Denmark
Coordinates: (56.303295, 10.619129)
Timezone: Europe/Copenhagen (UTC 2:00)
------------------------------------------------------------
Airport: Aarhus Sea Airport (QEA/EKAC)
Location: Aarhus, Denmark
Coordinates: (56.151993, 10.247725)
Timezone: Europe/Copenhagen (UTC 2:00)
------------------------------------------------------------
Airport: Aasiaat Airport (JEG/BGAA)
Location: Aasiaat, Greenland
Coordinates: (68.72184, -52.784698)
Timezone: America/Godthab (UTC -1:00)
------------------------------------------------------------
Airport: Abadan Airport (ABD/OIAA)
Location: Abadan, Iran
Coordinates: (30.371111, 48.228329)
Timezone: Asia/Tehran (UTC 3:30)
------------------------------------------------------------
Airport: Abakan International Airport (ABA/UNAA)
Location: Abakan, Russia
Coordinates: (53.740002, 91.385002)
Timezone: Asia/Krasnoyarsk (UTC 7:00)
------------------------------------------------------------
Airport: Abbotsford International Airport (YXX/CYXX)
Location: Abbotsford, Canada
Coordinates: (49.025269, -122.360001)
Timezone: America/Vancouver (UTC -7:00)
------------------------------------------------------------
Airport: Abemama Airport (AEA/NGTB)
Location: Abemama Atoll, Kiribati
Coordinates: (0.490833, 173.828598)
Timezone: Pacific/Tarawa (UTC 12:00)
------------------------------------------------------------

Airline Logos:
------------------------------------------------------------
Airline Logo: A0_EFW.png
Logo URL: https://s3.eu-central-1.amazonaws.com/images.flightradar24.com/assets/airlines/logotypes/A0_EFW.png?_=1678974401
------------------------------------------------------------
Airline Logo: A3_AEE.png
Logo URL: https://s3.eu-central-1.amazonaws.com/images.flightradar24.com/assets/airlines/logotypes/A3_AEE.png?_=1652880694
------------------------------------------------------------
Airline Logo: A6_OTC.png
Logo URL: https://s3.eu-central-1.amazonaws.com/images.flightradar24.com/assets/airlines/logotypes/A6_OTC.png?_=1699531755
------------------------------------------------------------
Airline Logo: AB_ABC.png
Logo URL: https://s3.eu-central-1.amazonaws.com/images.flightradar24.com/assets/airlines/logotypes/AB_ABC.png?_=1699444240
------------------------------------------------------------
Airline Logo: AB_BNZ.png
Logo URL: https://s3.eu-central-1.amazonaws.com/images.flightradar24.com/assets/airlines/logotypes/AB_BNZ.png?_=1678972803
------------------------------------------------------------
Airline Logo: AE_MDA.png
Logo URL: https://s3.eu-central-1.amazonaws.com/images.flightradar24.com/assets/airlines/logotypes/AE_MDA.png?_=1699644572
------------------------------------------------------------
Airline Logo: AI_AIC.png
Logo URL: https://s3.eu-central-1.amazonaws.com/images.flightradar24.com/assets/airlines/logotypes/AI_AIC.png?_=1699532075
------------------------------------------------------------
Airline Logo: AJ_AZY.png
Logo URL: https://s3.eu-central-1.amazonaws.com/images.flightradar24.com/assets/airlines/logotypes/AJ_AZY.png?_=1653302677
------------------------------------------------------------
Airline Logo: AK_AXM.png
Logo URL: https://s3.eu-central-1.amazonaws.com/images.flightradar24.com/assets/airlines/logotypes/AK_AXM.png?_=1652883979
------------------------------------------------------------
Airline Logo: AL_MAY.png
Logo URL: https://s3.eu-central-1.amazonaws.com/images.flightradar24.com/assets/airlines/logotypes/AL_MAY.png?_=1643833358
------------------------------------------------------------
",1,0,17,51986
I would like to know the vehicle information with trim id 3421.,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def get_trim_details(trim_id):
    """"""
    Retrieves comprehensive vehicle specifications including pricing, mileage, engine details, and make/model information.

    Parameters:
        trim_id (int): The unique identifier for the vehicle trim.

    Returns:
        JSON response containing detailed vehicle specifications.
    """"""
    url = f""https://car-api2.p.rapidapi.com/api/trims/{trim_id}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""car-api2.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers)
    return response.json()


def get_body_details(make_model_trim_id):
    """"""
    Retrieves detailed information about the vehicle body, including dimensions, weight, and cargo capacity.

    Parameters:
        make_model_trim_id (int): The unique identifier for the vehicle make, model, and trim.

    Returns:
        JSON response containing detailed body specifications of the vehicle.
    """"""
    url = ""https://car-api2.p.rapidapi.com/api/bodies""
    querystring = {
        ""make_model_trim_id"": str(make_model_trim_id),
        ""verbose"": ""yes"",
        ""sort"": ""id"",
        ""direction"": ""asc""
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""car-api2.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def display_vehicle_info(trim_data, body_data):
    """"""
    Displays detailed information of the vehicle including general specifications and body details.

    Parameters:
        trim_data (dict): JSON response data containing vehicle trim information.
        body_data (dict): JSON response data containing vehicle body information.
    """"""
    print(""\nVehicle Specifications:"")
    print(""-"" * 60)
    print(f""Vehicle Name: {trim_data.get('name')}"")
    print(f""Year: {trim_data.get('year')}"")
    print(f""MSRP: ${trim_data.get('msrp')}"")
    print(f""Description: {trim_data.get('description')}"")

    engine = trim_data.get('make_model_trim_engine', {})
    if engine:
        print(""\nEngine Specifications:"")
        print(f""Engine Type: {engine.get('engine_type')}"")
        print(f""Fuel Type: {engine.get('fuel_type')}"")
        print(f""Horsepower: {engine.get('horsepower_hp')} HP"")
        print(f""Torque: {engine.get('torque_ft_lbs')} ft-lbs"")

    if body_data and 'data' in body_data and len(body_data['data']) > 0:
        body = body_data['data'][0]
        print(""\nBody Specifications:"")
        print(f""Type: {body.get('type')}"")
        print(f""Doors: {body.get('doors')}"")
        print(f""Length: {body.get('length')}"")
        print(f""Width: {body.get('width')}"")
        print(f""Height: {body.get('height')}"")
        print(f""Seats: {body.get('seats')}"")
        print(f""Cargo Capacity: {body.get('cargo_capacity')}"")
        print(f""Curb Weight: {body.get('curb_weight')} lbs"")
        print(""-"" * 60)
    else:
        print(""No body specifications available for this vehicle."")
    print(""-"" * 60)


if __name__ == '__main__':
    # Step 1: Retrieve general vehicle specifications using the Trim View API
    trim_id = ""3421""

    if trim_id.isdigit():
        trim_data = get_trim_details(int(trim_id))

        # Step 2: Use make_model_trim_id to get detailed body specifications
        make_model_trim_id = trim_data.get('make_model_id')
        if make_model_trim_id:
            body_data = get_body_details(make_model_trim_id)

            # Step 3: Display the vehicle and body information
            display_vehicle_info(trim_data, body_data)
        else:
            print(""Invalid or missing Make/Model/Trim ID."")
    else:
        print(""Invalid Trim ID. Please enter a numeric value."")
","Vehicle Specifications:
------------------------------------------------------------
Vehicle Name: * (hidden)
Year: 2022
MSRP: $27990
Description: ********************************** (hidden)

Engine Specifications:
Engine Type: *** (engine type hidden)
Fuel Type: *** (fuel_type hidden)
Horsepower: 184 HP
Torque: 221 ft-lbs

Body Specifications:
Type: *** (body type hidden)
Doors: 5
Length: 169.7
Width: 71.7
Height: 61.3
Seats: 5
Cargo Capacity: 17.6
Curb Weight: 3618 lbs
------------------------------------------------------------
------------------------------------------------------------",0,0,18,47232
Give me some information about the airport.,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_airports_list():
    """"""
    Fetches a list of airports with detailed information including names, codes, location, altitude, and size.

    Returns:
        JSON response containing the list of airports.
    """"""
    url = ""https://flightradar24-com.p.rapidapi.com/v2/airports/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""flightradar24-com.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers)
    return response.json()

def get_airlines_list():
    """"""
    Retrieves a list of airlines, including their codes, status, and logo information.

    Returns:
        JSON response containing the list of airlines and their metadata.
    """"""
    url = ""https://flightradar24-com.p.rapidapi.com/v2/airlines/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""flightradar24-com.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers)
    return response.json()

def display_airports_and_airlines(airports_data, airlines_data):
    """"""
    Displays a user-friendly list of airports and associated airlines, including airline logos.

    Parameters:
        airports_data (dict): JSON response containing airport information.
        airlines_data (dict): JSON response containing airline information.
    """"""
    if 'rows' not in airports_data or not airports_data['rows']:
        print(""No airport data available."")
        return

    if 'rows' not in airlines_data or not airlines_data['rows']:
        print(""No airline data available."")
        return

    print(""\nList of Airports:"")
    print(""-"" * 60)
    for airport in airports_data['rows'][:10]:  # Displaying the first 10 airports for clarity
        print(f""Airport: {airport['name']} ({airport['iata']}/{airport['icao']})"")
        print(f""Location: {airport['city']}, {airport['country']}"")
        print(f""Coordinates: ({airport['lat']}, {airport['lon']})"")
        print(f""Altitude: {airport['alt']} ft, Size: {airport['size']}"")
        print(""-"" * 60)

    print(""\nList of Airlines with Logos:"")
    print(""-"" * 60)
    for airline in airlines_data['rows'][:10]:  # Displaying the first 10 airlines for clarity
        print(f""Airline: {airline['name']} ({airline['iata']}/{airline['icao']})"")
        if airline['metadata'] and airline['metadata']['images'] and airline['metadata']['images']['logotypes']:
            logo_info = airline['metadata']['images']['logotypes']['default']['file']
            print(f""Logo Name: {logo_info['name']}"")
            print(f""Logo URL: {logo_info['source']}"")
        else:
            print(""No logo information available."")
        print(""-"" * 60)

if __name__ == '__main__':
    # Step 1: Retrieve the list of worldwide airports
    print(""Fetching the list of worldwide airports..."")
    airports_data = get_airports_list()

    # Step 2: Retrieve the list of airlines
    print(""Fetching the list of airlines and their logos..."")
    airlines_data = get_airlines_list()

    # Step 3: Display the airports and airlines with their logos in a user-friendly way
    display_airports_and_airlines(airports_data, airlines_data)
",Fail,-1,-1,29,100253
Give me some information about the airline.,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def get_basic_airlines_list():
    """"""
    Retrieves a basic list of airlines, including their names, codes, and ICAO codes.

    Returns:
        JSON response containing the list of basic airline data.
    """"""
    url = ""https://flight-radar1.p.rapidapi.com/airlines/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""flight-radar1.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers)
    return response.json()


def get_detailed_airlines_list():
    """"""
    Retrieves a comprehensive list of airline data, including activity status, hub, IATA/ICAO codes,
    and additional metadata with logotype information.

    Returns:
        JSON response containing the detailed airline data.
    """"""
    url = ""https://flightradar24-com.p.rapidapi.com/v2/airlines/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""flightradar24-com.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers)
    return response.json()


def display_airlines_information(basic_data, detailed_data):
    """"""
    Displays a user-friendly list of airlines with both basic and detailed information, including logos.

    Parameters:
        basic_data (dict): JSON response containing basic airline data.
        detailed_data (dict): JSON response containing detailed airline data.
    """"""
    if 'rows' not in basic_data or not basic_data['rows']:
        print(""No basic airline data available."")
        return

    if 'rows' not in detailed_data or not detailed_data['rows']:
        print(""No detailed airline data available."")
        return

    print(""\nAirline List (Basic Information):"")
    print(""-"" * 60)
    for airline in basic_data['rows'][:10]:  # Display the first 10 airlines for clarity
        print(f""Airline: {airline['Name']} | Code: {airline['Code']} | ICAO: {airline['ICAO']}"")
    print(""-"" * 60)

    print(""\nAirline List (Detailed Information):"")
    print(""-"" * 60)
    for airline in detailed_data['rows'][:10]:  # Display the first 10 airlines for clarity
        print(f""Airline: {airline['name']} | IATA: {airline['iata']} | ICAO: {airline['icao']}"")
        print(f""Active: {'Yes' if airline['active'] else 'No'} | Hub: {airline['hub'] or 'N/A'}"")
        if airline['metadata'] and airline['metadata']['images'] and airline['metadata']['images']['logotypes']:
            logo_info = airline['metadata']['images']['logotypes']['default']['file']
            print(f""Logo File: {logo_info['name']}, URL: {logo_info['source']}"")
        else:
            print(""No logo information available."")
        print(""-"" * 60)


if __name__ == '__main__':
    # Step 1: Retrieve the basic airline list
    basic_data = get_basic_airlines_list()

    # Step 2: Retrieve the detailed airline list including logos
    detailed_data = get_detailed_airlines_list()

    # Step 3: Display the combined airline information in a user-friendly format
    display_airlines_information(basic_data, detailed_data)
","Airline List (Basic Information):
------------------------------------------------------------
Airline: 21 Air | Code: 2I | ICAO: CSB
Airline: 247 Aviation | Code:  | ICAO: EMC
Airline: 2Excel Aviation | Code:  | ICAO: BRO
Airline: 4 Airways | Code:  | ICAO: DAK
Airline: 40-Mile Air | Code: Q5 | ICAO: MLA
Airline: 748 Air Services | Code: FE | ICAO: IHO
Airline: 9 Air | Code: AQ | ICAO: JYH
Airline: Abakan Air | Code: S5 | ICAO: NKP
Airline: ABS Jets | Code:  | ICAO: ABP
Airline: Abu Dhabi Aviation | Code:  | ICAO: BAR
------------------------------------------------------------

Airline List (Detailed Information):
------------------------------------------------------------
Airline: 21 Air | IATA: 2I | ICAO: CSB
Active: Yes | Hub: N/A
Logo File: 1545.png, URL: https://images.flightradar24.com/assets/airlines/logotypes/1545.png
------------------------------------------------------------
Airline: 247 Aviation | IATA: None | ICAO: EMC
Active: Yes | Hub: N/A
Logo File: 2566.png, URL: https://images.flightradar24.com/assets/airlines/logotypes/2566.png
------------------------------------------------------------
Airline: 2Excel Aviation | IATA: None | ICAO: BRO
Active: Yes | Hub: N/A
Logo File: 1914.png, URL: https://images.flightradar24.com/assets/airlines/logotypes/1914.png
------------------------------------------------------------
Airline: 4 Airways | IATA: None | ICAO: DAK
Active: Yes | Hub: N/A
No logo information available.
------------------------------------------------------------
Airline: 40-Mile Air | IATA: Q5 | ICAO: MLA
Active: Yes | Hub: N/A
Logo File: MLA_logo0.png, URL: https://www.flightradar24.com/static/images/data/operators/MLA_logo0.png
------------------------------------------------------------
Airline: 748 Air Services | IATA: FE | ICAO: IHO
Active: Yes | Hub: N/A
Logo File: 1757.png, URL: https://images.flightradar24.com/assets/airlines/logotypes/1757.png
------------------------------------------------------------
Airline: 9 Air | IATA: AQ | ICAO: JYH
Active: Yes | Hub: N/A
Logo File: 280.png, URL: https://images.flightradar24.com/assets/airlines/logotypes/280.png
------------------------------------------------------------
Airline: Abakan Air | IATA: S5 | ICAO: NKP
Active: Yes | Hub: N/A
No logo information available.
------------------------------------------------------------
Airline: ABS Jets | IATA: None | ICAO: ABP
Active: Yes | Hub: N/A
No logo information available.
------------------------------------------------------------
Airline: Abu Dhabi Aviation | IATA: None | ICAO: BAR
Active: Yes | Hub: N/A
No logo information available.
------------------------------------------------------------",1,1,11,38720
I would like to know more about United States Airlines.,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def search_flights(query, limit=25):
    """"""
    Searches for flights using a query, which can be an aircraft number, flight code, or airline code.

    Parameters:
        query (str): The search query, e.g., flight number or airline code.
        limit (int): The number of items per response, default is 25.

    Returns:
        JSON response containing a list of matching flight details.
    """"""
    url = ""https://flightradar24-com.p.rapidapi.com/v2/flights/search""
    querystring = {""query"": query, ""limit"": limit}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""flightradar24-com.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def get_flight_details(flight_id):
    """"""
    Retrieves detailed information about a specific flight.

    Parameters:
        flight_id (str): The unique identifier of the flight.

    Returns:
        JSON response containing detailed flight information.
    """"""
    url = ""https://flightradar24-com.p.rapidapi.com/v2/flights/detail""
    querystring = {""flight"": flight_id}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""flightradar24-com.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def display_search_results(search_data):
    """"""
    Displays the flight search results in a user-friendly way.

    Parameters:
        search_data (dict): JSON response containing flight search results.
    """"""
    if 'results' not in search_data or not search_data['results']:
        print(""No flight search results available."")
        return

    print(""\nFlight Search Results:"")
    print(""-"" * 60)
    for result in search_data['results'][:10]:  # Display the first 10 results for clarity
        flight_detail = result.get('detail', {})
        print(f""Flight Name: {result.get('name')}"")
        print(f""Flight Number: {flight_detail.get('flight', 'N/A')}"")
        print(f""Operator: {flight_detail.get('operator', 'N/A')}"")
        print(f""Logo URL: {flight_detail.get('logo', 'No logo available')}"")
        print(f""Callsign: {flight_detail.get('callsign', 'N/A')}"")
        print(""-"" * 60)


def display_flight_details(flight_data):
    """"""
    Displays detailed information of a specific flight.

    Parameters:
        flight_data (dict): JSON response containing detailed flight information.
    """"""
    print(""\nDetailed Flight Information:"")
    print(""-"" * 60)
    print(f""Flight ID: {flight_data.get('id')}"")
    print(f""Flight Name: {flight_data.get('name')}"")
    print(f""Created At: {flight_data.get('created_at')}"")
    print(f""Updated At: {flight_data.get('updated_at')}"")

    details = flight_data.get('details', {})
    print(f""Description: {details.get('description', 'No description available')}"")
    print(f""Status: {details.get('status', 'N/A')}"")

    tags = flight_data.get('tags', [])
    if tags:
        print(f""Tags: {', '.join(tags)}"")
    else:
        print(""No tags associated."")
    print(""-"" * 60)


if __name__ == '__main__':
    # Step 1: Search for flights using a specific query
    search_query = ""AA""

    print(f""\nSearching for flights with query '{search_query}'..."")
    search_results = search_flights(search_query)

    # Step 2: Display the search results
    display_search_results(search_results)

    # Step 3: Ask the user to choose a flight for detailed information
    selected_flight_id = ""AA""

    # Step 4: Retrieve and display detailed information for the selected flight
    if selected_flight_id:
        print(f""\nFetching detailed information for Flight ID {selected_flight_id}..."")
        flight_details = get_flight_details(selected_flight_id)
        display_flight_details(flight_details)
    else:
        print(""No Flight ID entered. Exiting."")
",Fail,-1,-1,31,107849
I would like to know New York city and airport information.,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'


def search_airport(query, locale='en-US'):
    """"""
    Searches for airports and related cities based on a specified location query.

    Parameters:
        query (str): The name of the location to search for airports.
        locale (str): The language code for localization, default is 'en-US'.

    Returns:
        JSON response containing a list of airports and related cities.
    """"""
    url = ""https://sky-scrapper.p.rapidapi.com/api/v1/flights/searchAirport""
    querystring = {""query"": query, ""locale"": locale}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""sky-scrapper.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def search_location(query):
    """"""
    Searches for cities and airports based on a specified location query, returning hierarchical information.

    Parameters:
        query (str): The location name to search for, such as a city or airport.

    Returns:
        JSON response containing a list of cities and airports with hierarchical information.
    """"""
    url = ""https://sky-scrapper.p.rapidapi.com/api/v1/cars/searchLocation""
    querystring = {""query"": query}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""sky-scrapper.p.rapidapi.com""
    }

    response = requests.get(url, headers=headers, params=querystring)
    return response.json()


def display_airport_search_results(search_data):
    """"""
    Displays the search results for airports and related cities in a user-friendly format.

    Parameters:
        search_data (dict): JSON response containing search results for airports and cities.
    """"""
    if 'data' not in search_data or not search_data['data']:
        print(""No airport data available."")
        return

    print(""\nAirport and City Search Results:"")
    print(""-"" * 60)
    for entry in search_data['data'][:10]:  # Display the first 10 results for clarity
        presentation = entry.get('presentation', {})
        navigation = entry.get('navigation', {})
        print(f""Title: {presentation.get('title')}"")
        print(f""Suggestion: {presentation.get('suggestionTitle')}"")
        print(f""Subtitle: {presentation.get('subtitle')}"")
        print(f""Entity Type: {navigation.get('entityType')}, Localized Name: {navigation.get('localizedName')}"")
        print(""-"" * 60)


def display_location_search_results(search_data):
    """"""
    Displays the search results for cities and airports with hierarchical information.

    Parameters:
        search_data (dict): JSON response containing search results for cities and airports.
    """"""
    if 'data' not in search_data or not search_data['data']:
        print(""No location data available."")
        return

    print(""\nHierarchical Location Search Results:"")
    print(""-"" * 60)
    for entry in search_data['data'][:10]:  # Display the first 10 results for clarity
        print(f""Entity Name: {entry['entity_name']} | Type: {entry['class']}"")
        print(f""Location Coordinates: {entry['location']}"")
        print(f""Hierarchy: {entry['hierarchy']}"")
        print(f""Entity ID: {entry['entity_id']}"")
        print(""-"" * 60)


if __name__ == '__main__':
    # Step 1: Ask the user for a location to search
    location_query = ""New York""

    # Step 2: Search for airports using the specified query
    airport_search_results = search_airport(location_query)

    # Step 3: Display the search results from the searchAirport API
    display_airport_search_results(airport_search_results)

    # Step 4: Search for cities and airports with hierarchical information using the searchLocation API
    location_search_results = search_location(location_query)

    # Step 5: Display the search results from the searchLocation API
    display_location_search_results(location_search_results)
","Airport and City Search Results:
------------------------------------------------------------
Title: New York
Suggestion: New York (Any)
Subtitle: United States
Entity Type: CITY, Localized Name: New York
------------------------------------------------------------
Title: New York John F. Kennedy
Suggestion: New York John F. Kennedy (JFK)
Subtitle: United States
Entity Type: AIRPORT, Localized Name: New York John F. Kennedy
------------------------------------------------------------
Title: New York Newark
Suggestion: New York Newark (EWR)
Subtitle: United States
Entity Type: AIRPORT, Localized Name: New York Newark
------------------------------------------------------------
Title: New York LaGuardia
Suggestion: New York LaGuardia (LGA)
Subtitle: United States
Entity Type: AIRPORT, Localized Name: New York LaGuardia
------------------------------------------------------------
Title: Stewart International
Suggestion: Stewart International (SWF)
Subtitle: United States
Entity Type: AIRPORT, Localized Name: Stewart International
------------------------------------------------------------
Title: Albany
Suggestion: Albany (ALB)
Subtitle: United States
Entity Type: AIRPORT, Localized Name: Albany
------------------------------------------------------------
Title: Buffalo Niagara
Suggestion: Buffalo Niagara (BUF)
Subtitle: United States
Entity Type: AIRPORT, Localized Name: Buffalo Niagara
------------------------------------------------------------
Title: Syracuse
Suggestion: Syracuse (SYR)
Subtitle: United States
Entity Type: AIRPORT, Localized Name: Syracuse
------------------------------------------------------------

Hierarchical Location Search Results:
------------------------------------------------------------
Entity Name: New York John F. Kennedy (JFK) | Type: Airport
Location Coordinates: 40.6412215, -73.7781176
Hierarchy: New York|New York|United States
Entity ID: 95565058
------------------------------------------------------------
Entity Name: New York | Type: City
Location Coordinates: 40.6940959901, -73.9282670243
Hierarchy: New York|United States
Entity ID: 27537542
------------------------------------------------------------
Entity Name: New York Newark (EWR) | Type: Airport
Location Coordinates: 40.695833, -74.171667
Hierarchy: New York|New York|United States
Entity ID: 95565059
------------------------------------------------------------
Entity Name: New York LaGuardia (LGA) | Type: Airport
Location Coordinates: 40.779444, -73.875833
Hierarchy: New York|New York|United States
Entity ID: 95565057
------------------------------------------------------------
Entity Name: Newyork | Type: City
Location Coordinates: 56.2499999918, -5.2833331469
Hierarchy: Argyll and Bute|Scotland|United Kingdom
Entity ID: 46422841
------------------------------------------------------------
Entity Name: New York | Type: City
Location Coordinates: 32.1679320926, -95.6691276413
Hierarchy: Henderson County|Texas|United States
Entity ID: 46834056
------------------------------------------------------------
Entity Name: New York | Type: City
Location Coordinates: 30.8385201937, -87.2008047958
Hierarchy: Santa Rosa County|Florida|United States
Entity ID: 46772434
------------------------------------------------------------
Entity Name: New York | Type: City
Location Coordinates: 53.0789712548, -0.1400758425
Hierarchy: Lincolnshire|England|United Kingdom
Entity ID: 46422842
------------------------------------------------------------
Entity Name: New York Mills | Type: City
Location Coordinates: 46.5180160916, -95.376146406
Hierarchy: Otter Tail County|Minnesota|United States
Entity ID: 46863744
------------------------------------------------------------
Entity Name: New York Mills | Type: City
Location Coordinates: 43.1053474928, -75.2912777901
Hierarchy: Oneida County|New York|United States
Entity ID: 46872082
------------------------------------------------------------",1,1,8,23360
"I need detailed information about the Paris I'm searching for, including their location hierarchy and geographic coordinates. Also, I'd like to see a randomly selected popular photo related to a trip or destination with metadata such as the photo URL, the username of the uploader, and the hosting platform. Please provide at least three search results for the places I designate and always include a random photo related to your trip for inspiration.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
API_HOST_LOCATIONS = 'skyscanner80.p.rapidapi.com'
API_HOST_PHOTOS = 'nicaraguawallpaper.p.rapidapi.com'


def API_search_location(query, market='US', locale='en-US'):
    url = f""https://{API_HOST_LOCATIONS}/api/v1/cars/search-location""

    querystring = {
        ""query"": query,
        ""market"": market,
        ""locale"": locale
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST_LOCATIONS
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        data = response.json()
        if data.get('status'):
            return data['data'][:3]  # Limit to the first 3 results
        else:
            print(""Error in location search"")
    else:
        print(""Request failed for location search:"", response.status_code)

    return None


def API_get_random_photo():
    url = f""https://{API_HOST_PHOTOS}/api/getdemorandomphoto""

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST_PHOTOS
    }

    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        data = response.json()
        if data.get('success'):
            return data['dataResponse']
        else:
            print(""Error retrieving random photo"")
    else:
        print(""Request failed for random photo:"", response.status_code)

    return None


if __name__ == '__main__':
    # Example location search query
    search_query = ""Paris""

    # Retrieve detailed information for a location
    location_data = API_search_location(search_query)
    if location_data:
        print(""Search Location Results:"")
        for item in location_data:
            print(f""Entity Name: {item['entity_name']}"")
            print(f""Entity ID: {item['entity_id']}"")
            print(f""Location: {item['location']}"")
            print(f""Hierarchy: {item['hierarchy']}"")
            print(f""Class: {item['class']}"")
            print(""\n"")

    # Retrieve a random popular travel-related photo
    random_photo = API_get_random_photo()
    if random_photo:
        print(""Random Travel Photo:"")
        print(f""Photo ID: {random_photo['id']}"")
        print(f""Image URL: {random_photo['imageUrl']}"")
        print(f""Photo Link: {random_photo['photoLink']}"")
        print(f""Uploader Username: {random_photo['username']}"")
        print(f""Platform: {random_photo['platform']}"")
        print(f""Uploader Profile URL: {random_photo['profile_url']}"")
        print(""\n"")
","Search Location Results:
Entity Name: Paris Charles de Gaulle (CDG)
Entity ID: 95565041
Location: 49.0096176, 2.5481698
Hierarchy: Paris|ÃŽle-de-France|France
Class: Airport


Entity Name: Paris
Entity ID: 27539733
Location: 48.85662237511698, 2.3428759930239886
Hierarchy: ÃŽle-de-France|France
Class: City


Entity Name: Paris Orly (ORY)
Entity ID: 95565040
Location: 48.728889, 2.357222
Hierarchy: Paris|ÃŽle-de-France|France
Class: Airport


Random Travel Photo:
Photo ID: popular_02
Image URL: https://live.staticflickr.com/3670/32419322614_499fc708f0_b.jpg
Photo Link: https://flic.kr/p/RoMv9b
Uploader Username: Dieter Vynckier
Platform: Flickr
Uploader Profile URL: https://www.flickr.com/photos/dietervynckier/",1,1,18,65412
"I want to know about different countries, including their market codes, currency details, locale settings, and any associated website URLs. Additionally, I'd like to see some visually appealing photos from a curated list, specifically three promoted and three popular photos, along with information about the photo's uploader and the platform where it's hosted. The results should be presented together, with a focus on countries and captivating photos related to travel.
","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
API_HOST_CONFIG = 'skyscanner80.p.rapidapi.com'
API_HOST_PHOTOS = 'nicaraguawallpaper.p.rapidapi.com'


def API_get_config():
    url = f""https://{API_HOST_CONFIG}/api/v1/get-config""

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST_CONFIG
    }

    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        data = response.json()
        if data.get('status'):
            return data['data']
        else:
            print(""Error retrieving config data:"", data.get('message'))
    else:
        print(""Request failed for config retrieval:"", response.status_code)

    return None


def API_get_demo_all_photos():
    url = f""https://{API_HOST_PHOTOS}/api/getdemoallphotos""

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST_PHOTOS
    }

    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        data = response.json()
        if data.get('success'):
            return data['dataResponse']['dataResponse']
        else:
            print(""Error retrieving photos"")
    else:
        print(""Request failed for photos retrieval:"", response.status_code)

    return None


if __name__ == '__main__':
    # Retrieve country configuration details
    config_data = API_get_config()
    if config_data:
        print(""Country Configuration Data:"")
        for country in config_data[:3]:  # Limit to the first 3 countries
            print(f""Country: {country['country']}"")
            print(f""Market Code: {country['market']}"")
            print(f""Locale: {country['locale']}"")
            print(f""Currency: {country['currency']} ({country['currencySymbol']})"")
            print(f""Website: {country['site']}"")
            print(""\n"")

    # Retrieve a list of curated promoted and popular photos
    photo_data = API_get_demo_all_photos()
    if photo_data:
        print(""Promoted Photos:"")
        for item in photo_data['promotion']['items'][:3]:  # Limit to 3 promoted photos
            print(f""Photo ID: {item['id']}"")
            print(f""Image URL: {item['imageUrl']}"")
            print(f""Photo Link: {item['photoLink']}"")
            print(f""Uploader: {item['username']}"")
            print(f""Platform: {item['platform']}"")
            print(f""Profile URL: {item['profile_url']}"")
            print(""\n"")

        print(""Popular Photos:"")
        for item in photo_data['popular']['items'][:3]:  # Limit to 3 popular photos
            print(f""Photo ID: {item['id']}"")
            print(f""Image URL: {item['imageUrl']}"")
            print(f""Photo Link: {item['photoLink']}"")
            print(f""Uploader: {item['username']}"")
            print(f""Platform: {item['platform']}"")
            print(f""Profile URL: {item['profile_url']}"")
            print(""\n"")
","Country Configuration Data:
Country: Andorra
Market Code: AD
Locale: es-ES
Currency: EUR (â‚¬)
Website: www.skyscanner.net


Country: United Arab Emirates
Market Code: AE
Locale: en-US
Currency: AED (AED)
Website: www.skyscanner.ae


Country: Afghanistan
Market Code: AF
Locale: en-GB
Currency: AFN (AFN)
Website: www.skyscanner.net


Promoted Photos:
Photo ID: promotion_01
Image URL: https://live.staticflickr.com/3681/13654859084_c3ddd8848e_o_d.jpg
Photo Link: https://flic.kr/p/mNCHSU
Uploader: Nava writz
Platform: Flickr
Profile URL: https://www.flickr.com/photos/navawritz/


Popular Photos:
Photo ID: popular_01
Image URL: https://live.staticflickr.com/4069/4467659984_c68c183360_b.jpg
Photo Link: https://flic.kr/p/7NMVEq
Uploader: Shannon
Platform: Flickr
Profile URL: https://www.flickr.com/photos/trailofdead/


Photo ID: popular_02
Image URL: https://live.staticflickr.com/3670/32419322614_499fc708f0_b.jpg
Photo Link: https://flic.kr/p/RoMv9b
Uploader: Dieter Vynckier
Platform: Flickr
Profile URL: https://www.flickr.com/photos/dietervynckier/


Photo ID: popular_03
Image URL: https://live.staticflickr.com/49/129153533_4bb1dc653b_b.jpg
Photo Link: https://flic.kr/p/cpWR4
Uploader: Daniela Cordero
Platform: Flickr
Profile URL: https://www.flickr.com/photos/danielacordero/",1,1,10,25060
"I need a detailed overview of New York's airports and related cities, including their titles and navigation details such as localized names and identifiers. Also, I want to know about its available rental car options, focusing on at least three available offers with information on car types, prices, and supplier ratings. Responses should provide concise and comprehensive information to help inform travel plans, combining airport data with car rental options in the same area.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
API_HOST_AIRPORTS = 'sky-scrapper.p.rapidapi.com'
API_HOST_CARS = 'skyscanner80.p.rapidapi.com'


def API_search_airport(query, locale='en-US'):
    url = f""https://{API_HOST_AIRPORTS}/api/v1/flights/searchAirport""

    querystring = {
        ""query"": query,
        ""locale"": locale
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST_AIRPORTS
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        data = response.json()
        if data.get('status'):
            return data['data'][:3]  # Limit to the first 3 results
        else:
            print(""Error retrieving airport data"")
    else:
        print(""Request failed for airport search:"", response.status_code)

    return None


def API_search_cars(pick_up_entity_id, pick_up_date='2024-03-12', pick_up_time='10:00', currency='USD', market='US',
                    locale='en-US'):
    url = f""https://{API_HOST_CARS}/api/v1/cars/search-cars""

    querystring = {
        ""pickUpEntityId"": pick_up_entity_id,
        ""pickUpDate"": pick_up_date,
        ""pickUpTime"": pick_up_time,
        ""currency"": currency,
        ""market"": market,
        ""locale"": locale
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST_CARS
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        data = response.json()
        if data.get('status'):
            return data['data']['quotes'][:3]  # Limit to the first 3 quotes
        else:
            print(""Error retrieving car rental data"")
    else:
        print(""Request failed for car rental search:"", response.status_code)

    return None


if __name__ == '__main__':
    # Search for airports in a specific location
    search_location = ""New York""
    airport_data = API_search_airport(search_location)

    if airport_data:
        print(""Airports and Related Cities:"")
        for airport in airport_data:
            print(f""Title: {airport['presentation']['title']}"")
            print(f""Suggestion Title: {airport['presentation']['suggestionTitle']}"")
            print(f""Country: {airport['presentation']['subtitle']}"")
            print(f""Entity ID: {airport['navigation']['entityId']}"")
            print(f""Localized Name: {airport['navigation']['localizedName']}"")
            print(f""Entity Type: {airport['navigation']['entityType']}"")
            print(""\n"")

        # Assume the first airport entity for car rental search
        first_airport_id = airport_data[0]['navigation']['entityId']

        # Search for car rentals at the first found airport location
        car_quotes = API_search_cars(pick_up_entity_id=first_airport_id)

        if car_quotes:
            print(""Car Rental Options:"")
            for quote in car_quotes:
                print(f""Car Name: {quote['car_name']}"")
                print(f""Vendor: {quote['vndr']}"")
                print(f""Price: ${quote['price']}"")
                print(f""Fuel Type: {quote['fuel_type']}"")
                print(f""Unlimited Mileage: {'Yes' if quote['adds']['unlim_mlg'] else 'No'}"")
                print(f""Free Cancellation: {'Yes' if quote['adds']['free_cancel'] else 'No'}"")
                print(f""Vendor Rating: {quote['vndr_rating']['overall_rating']} / 5"")
                print(f""Deep Link: {quote['dplnk']}"")
                print(""\n"")
",Fail,-1,-1,30,101910
"I want a comprehensive weather and air quality report for a specific location in the National Capital Region of the Philippines. I need to know the detailed weather forecast for the next few days, including temperature, humidity, and wind conditions, as well as the air quality, with information on pollutants like carbon monoxide, nitrogen dioxide, and particulate matter. The report should cover at least three upcoming forecast intervals and provide the current air quality index (AQI) and pollutant concentrations.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
API_HOST_WEATHER = 'weather-data-api1.p.rapidapi.com'

def API_check_forecast(lat='14.5995', lon='120.9842'):
    url = f""https://{API_HOST_WEATHER}/check-forecast""

    querystring = {
        ""lat"": lat,
        ""lon"": lon
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST_WEATHER
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        data = response.json()
        if data.get('list'):
            return data['list'][:3]  # Limit to the first 3 forecast intervals
        else:
            print(""Error retrieving weather forecast"")
    else:
        print(""Request failed for weather forecast:"", response.status_code)

    return None

def API_check_air_pollution(lat='14.5995', lon='120.9842'):
    url = f""https://{API_HOST_WEATHER}/air_pollution""

    querystring = {
        ""lat"": lat,
        ""lon"": lon
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST_WEATHER
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        data = response.json()
        if data.get('list'):
            return data['list'][0]  # Get the most recent air quality data
        else:
            print(""Error retrieving air pollution data"")
    else:
        print(""Request failed for air pollution data:"", response.status_code)

    return None

if __name__ == '__main__':
    # Retrieve weather forecast data for the National Capital Region
    forecast_data = API_check_forecast()
    if forecast_data:
        print(""Weather Forecast:"")
        for forecast in forecast_data:
            print(f""Date & Time: {forecast['dt_txt']}"")
            print(f""Temperature: {forecast['main']['temp']}Â°C"")
            print(f""Feels Like: {forecast['main']['feels_like']}Â°C"")
            print(f""Humidity: {forecast['main']['humidity']}%"")
            print(f""Weather: {forecast['weather'][0]['description'].capitalize()}"")
            print(f""Wind Speed: {forecast['wind']['speed']} m/s"")
            print(f""Cloudiness: {forecast['clouds']['all']}%"")
            if 'rain' in forecast and '3h' in forecast['rain']:
                print(f""Rain Volume (last 3 hrs): {forecast['rain']['3h']} mm"")
            print(""\n"")

    # Retrieve air quality data for the National Capital Region
    air_quality_data = API_check_air_pollution()
    if air_quality_data:
        print(""Air Quality Report:"")
        print(f""Air Quality Index (AQI): {air_quality_data['main']['aqi']}"")
        print(f""Carbon Monoxide (CO): {air_quality_data['components']['co']} Âµg/mÂ³"")
        print(f""Nitric Oxide (NO): {air_quality_data['components']['no']} Âµg/mÂ³"")
        print(f""Nitrogen Dioxide (NO2): {air_quality_data['components']['no2']} Âµg/mÂ³"")
        print(f""Ozone (O3): {air_quality_data['components']['o3']} Âµg/mÂ³"")
        print(f""Sulfur Dioxide (SO2): {air_quality_data['components']['so2']} Âµg/mÂ³"")
        print(f""PM2.5: {air_quality_data['components']['pm2_5']} Âµg/mÂ³"")
        print(f""PM10: {air_quality_data['components']['pm10']} Âµg/mÂ³"")
        print(f""Ammonia (NH3): {air_quality_data['components']['nh3']} Âµg/mÂ³"")
","Weather Forecast:
Date & Time: 2024-10-26 21:00:00
Temperature: 299Â°C
Feels Like: 299.99Â°C
Humidity: 90%
Weather: Moderate rain
Wind Speed: 5.93 m/s
Cloudiness: 89%
Rain Volume (last 3 hrs): 8.27 mm


Date & Time: 2024-10-27 00:00:00
Temperature: 299.31Â°C
Feels Like: 299.31Â°C
Humidity: 86%
Weather: Moderate rain
Wind Speed: 6.49 m/s
Cloudiness: 93%
Rain Volume (last 3 hrs): 3.69 mm


Date & Time: 2024-10-27 03:00:00
Temperature: 300.72Â°C
Feels Like: 303.7Â°C
Humidity: 76%
Weather: Light rain
Wind Speed: 7.5 m/s
Cloudiness: 100%
Rain Volume (last 3 hrs): 1.26 mm


Air Quality Report:
Air Quality Index (AQI): 2
Carbon Monoxide (CO): 347.14 Âµg/mÂ³
Nitric Oxide (NO): 0 Âµg/mÂ³
Nitrogen Dioxide (NO2): 4.54 Âµg/mÂ³
Ozone (O3): 70.1 Âµg/mÂ³
Sulfur Dioxide (SO2): 2.86 Âµg/mÂ³
PM2.5: 9.8 Âµg/mÂ³
PM10: 16.31 Âµg/mÂ³
Ammonia (NH3): 1.11 Âµg/mÂ³
",1,1,15,53190
"I would like to know the exact location of Manila including latitude, longitude, country code and region details. After getting the geographic information, I want to know the air quality of the place, providing the air quality index (AQI) and the concentration of major pollutants such as carbon monoxide, nitrogen dioxide, and particulate matter. Make sure the response covers at least one specific location and includes air quality details.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
API_HOST_LOCATION = 'weather-data-api1.p.rapidapi.com'
API_HOST_AIR = 'weather-data-api1.p.rapidapi.com'


def API_find_location(location_name='Manila', limit=1):
    url = f""https://{API_HOST_LOCATION}/find-location""

    querystring = {
        ""q"": location_name,
        ""limit"": str(limit)
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST_LOCATION
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        data = response.json()
        if data:
            return data[0]  # Get the first matching location
        else:
            print(""Error retrieving location data"")
    else:
        print(""Request failed for location data:"", response.status_code)

    return None


def API_check_air_pollution(lat, lon):
    url = f""https://{API_HOST_AIR}/air_pollution""

    querystring = {
        ""lat"": str(lat),
        ""lon"": str(lon)
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST_AIR
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        data = response.json()
        if data.get('list'):
            return data['list'][0]  # Get the most recent air quality data
        else:
            print(""Error retrieving air pollution data"")
    else:
        print(""Request failed for air pollution data:"", response.status_code)

    return None


if __name__ == '__main__':
    # Retrieve geographical information for a specific location
    location_name = 'Manila'  # Example location
    location_data = API_find_location(location_name=location_name)

    if location_data:
        print(""Location Information:"")
        print(f""Location Name: {location_data['name']}"")
        print(f""Latitude: {location_data['lat']}"")
        print(f""Longitude: {location_data['lon']}"")
        print(f""Country Code: {location_data['country']}"")
        if 'state' in location_data:
            print(f""State: {location_data['state']}"")
        print(""\n"")

        # Retrieve air quality data using the found latitude and longitude
        air_quality_data = API_check_air_pollution(lat=location_data['lat'], lon=location_data['lon'])

        if air_quality_data:
            print(""Air Quality Report:"")
            print(f""Air Quality Index (AQI): {air_quality_data['main']['aqi']}"")
            print(f""Carbon Monoxide (CO): {air_quality_data['components']['co']} Âµg/mÂ³"")
            print(f""Nitric Oxide (NO): {air_quality_data['components']['no']} Âµg/mÂ³"")
            print(f""Nitrogen Dioxide (NO2): {air_quality_data['components']['no2']} Âµg/mÂ³"")
            print(f""Ozone (O3): {air_quality_data['components']['o3']} Âµg/mÂ³"")
            print(f""Sulfur Dioxide (SO2): {air_quality_data['components']['so2']} Âµg/mÂ³"")
            print(f""PM2.5: {air_quality_data['components']['pm2_5']} Âµg/mÂ³"")
            print(f""PM10: {air_quality_data['components']['pm10']} Âµg/mÂ³"")
            print(f""Ammonia (NH3): {air_quality_data['components']['nh3']} Âµg/mÂ³"")
",Fail,-1,-1,30,101160
"I want information about cars for specific years, including key details about models available during those years. Please provide data for the most recent three years and include details such as model names, manufacturers, and any other relevant specifications.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
API_HOST = 'car-api2.p.rapidapi.com'


def get_years_data():
    url = f""https://{API_HOST}/api/years""

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST
    }

    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        data = response.json()
        if isinstance(data, list):
            # Assume the data is a list of year objects, get the most recent 3 years
            sorted_years = sorted(data, reverse=True)[:3]  # Sort in descending order and take the top 3
            return sorted_years
        else:
            print(""Unexpected data format received from Years API"")
    else:
        print(f""Failed to fetch years data: {response.status_code}"")

    return None


def get_models_for_year(year):
    url = f""https://{API_HOST}/api/models""

    querystring = {
        ""year"": str(year),  # Ensure the year is passed as a string
        ""sort"": ""id"",
        ""direction"": ""asc"",
        ""verbose"": ""yes""
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        data = response.json()
        return data['data'][:3]  # Limit to the first 3 models for simplicity
    else:
        print(f""Failed to fetch models for year {year}: {response.status_code}"")

    return None


if __name__ == '__main__':
    # Step 1: Get the most recent 3 years
    years = get_years_data()

    if years:
        for year in years:
            print(f""\nFetching data for cars from the year {year}...\n"")

            # Step 2: Get car models for each of the recent 3 years
            models_data = get_models_for_year(year)

            if models_data:
                print(f""Top 3 Car Models for {year}:"")
                for model in models_data:
                    print(f""Model Name: {model['name']}"")
                    print(f""Manufacturer: {model['make']['name']}"")
                    print(f""Model ID: {model['id']}"")
                    print(f""Make ID: {model['make']['id']}"")
                    print(f""Link: {model.get('collection', {}).get('url', 'N/A')}\n"")
","Fetching data for cars from the year 2025...

Top 3 Car Models for 2025:
Model Name: ******* (hidden)
Manufacturer: Acura
Model ID: 1
Make ID: 1
Link: N/A

Model Name: *** (hidden)
Manufacturer: Acura
Model ID: 2
Make ID: 1
Link: N/A

Model Name: ******** (hidden)
Manufacturer: BMW
Model ID: 9
Make ID: 3
Link: N/A


Fetching data for cars from the year 2024...

Top 3 Car Models for 2024:
Model Name: ******* (hidden)
Manufacturer: Acura
Model ID: 1
Make ID: 1
Link: N/A

Model Name: *** (hidden)
Manufacturer: Acura
Model ID: 2
Make ID: 1
Link: N/A

Model Name: ** (hidden)
Manufacturer: Audi
Model ID: 3
Make ID: 2
Link: N/A


Fetching data for cars from the year 2023...

Top 3 Car Models for 2023:
Model Name: ******* (hidden)
Manufacturer: Acura
Model ID: 1
Make ID: 1
Link: N/A

Model Name: *** (hidden)
Manufacturer: Acura
Model ID: 2
Make ID: 1
Link: N/A

Model Name: ** (hidden)
Manufacturer: Audi
Model ID: 3
Make ID: 2
Link: N/A
",0,0,13,39429
"I need detailed information about automobile data for the latest three years, including specific manufacturers and their details. ","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
API_HOST = 'car-api2.p.rapidapi.com'


def get_recent_years():
    url = f""https://{API_HOST}/api/years""

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST
    }

    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        data = response.json()
        if isinstance(data, list):
            # Assume the response is a list of years, get the most recent 3
            sorted_years = sorted(data, reverse=True)[:3]
            return sorted_years
        else:
            print(""Unexpected data format received from Years API"")
    else:
        print(f""Failed to fetch years data: {response.status_code}"")

    return None


def get_makes_for_year(year):
    url = f""https://{API_HOST}/api/makes""

    querystring = {
        ""year"": str(year),  # Ensure the year is passed as a string
        ""sort"": ""id"",
        ""direction"": ""asc"",
        ""limit"": ""3""  # Limit to the first 3 makes for each year
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": API_HOST
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        data = response.json()
        return data['data'][:3]  # Take the first 3 manufacturers
    else:
        print(f""Failed to fetch makes for year {year}: {response.status_code}"")

    return None


if __name__ == '__main__':
    # Step 1: Get the most recent 3 years
    recent_years = get_recent_years()

    if recent_years:
        for year in recent_years:
            print(f""\nFetching car makes for the year {year}...\n"")

            # Step 2: Get car makes for each of the recent 3 years
            makes_data = get_makes_for_year(year)

            if makes_data:
                print(f""Top 3 Car Makes for {year}:"")
                for make in makes_data:
                    print(f""Make Name: {make['name']}"")
                    print(f""Make ID: {make['id']}\n"")
","Fetching car makes for the year 2025...

Top 3 Car Makes for 2025:
Make Name: Acura
Make ID: 1

Make Name: Audi
Make ID: 2

Make Name: BMW
Make ID: 3


Fetching car makes for the year 2024...

Top 3 Car Makes for 2024:
Make Name: Acura
Make ID: 1

Make Name: Audi
Make ID: 2

Make Name: BMW
Make ID: 3


Fetching car makes for the year 2023...

Top 3 Car Makes for 2023:
Make Name: Acura
Make ID: 1

Make Name: Audi
Make ID: 2

Make Name: BMW
Make ID: 3
",0,0,14,43232
I need a summary of recent soccer matches and the profiles of three players from the La Liga league. ,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
ARCHIVE_API_HOST = 'pinnacle-odds.p.rapidapi.com'
PLAYER_API_HOST = 'la-liga2.p.rapidapi.com'

def get_recent_soccer_events():
    url = f""https://{ARCHIVE_API_HOST}/kit/v1/archive""

    querystring = {
        ""sport_id"": ""1"",  # Assuming 1 is the ID for soccer
        ""page_num"": ""1""   # Get the first page of events
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ARCHIVE_API_HOST
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        data = response.json()
        if 'events' in data:
            # Return the first three events
            return data['events'][:3]
        else:
            print(""Unexpected data format received from Archive Events API"")
    else:
        print(f""Failed to fetch events data: {response.status_code}"")

    return None

def get_player_details(player_id):
    url = f""https://{PLAYER_API_HOST}/players/{player_id}""

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": PLAYER_API_HOST
    }

    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        return response.json()
    else:
        print(f""Failed to fetch player data for ID {player_id}: {response.status_code}"")

    return None

if __name__ == '__main__':
    # Step 1: Get recent soccer events
    recent_events = get_recent_soccer_events()

    if recent_events:
        print(""\nRecent Soccer Events:"")
        player_ids = []

        for event in recent_events:
            print(f""Match: {event['home']} vs {event['away']}"")
            print(f""League: {event['league_name']}"")
            print(f""Start Time: {event['starts']}"")
            print(f""Home Score: {event['period_results'][0]['team_1_score']}, Away Score: {event['period_results'][0]['team_2_score']}\n"")

            # Collect player IDs if available
            if 'parent_id' in event and event['parent_id']:
                player_ids.append(event['parent_id'])

        # Ensure we have exactly 3 player IDs
        if len(player_ids) >= 3:
            player_ids = player_ids[:3]

            print(""\nDetailed Player Information:"")
            for player_id in player_ids:
                player_info = get_player_details(player_id)
                if player_info:
                    print(f""Player Name: {player_info['name']}"")
                    print(f""Team: {player_info['team']['name']}"")
                    print(f""Position: {player_info['position']}"")
                    print(f""Age: {player_info['age']}"")
                    print(f""Goals: {player_info['goals']}"")
                    print(f""Assists: {player_info['assists']}"")
                    print(f""Yellow Cards: {player_info['yellowCards']}"")
                    print(f""Red Cards: {player_info['redCards']}\n"")
",Fail,-1,-1,31,93403
I need to find detailed geographical information and the local time for three specific cities. ,"import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
LOCATION_API_HOST = 'weather-data-api1.p.rapidapi.com'
TIMEZONE_API_HOST = 'weatherapi-com.p.rapidapi.com'


def get_location_data(city_name):
    url = f""https://{LOCATION_API_HOST}/find-location""

    querystring = {
        ""q"": city_name,
        ""limit"": ""1""  # Limit to the first matching result
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": LOCATION_API_HOST
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        data = response.json()
        if data:
            # Return the first match's latitude and longitude
            return data[0]  # Assume the first item is the desired result
        else:
            print(""No location data found."")
    else:
        print(f""Failed to fetch location data for {city_name}: {response.status_code}"")

    return None


def get_time_zone(lat, lon):
    url = f""https://{TIMEZONE_API_HOST}/timezone.json""

    querystring = {
        ""q"": f""{lat},{lon}""  # Use latitude and longitude for the query
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": TIMEZONE_API_HOST
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()
    else:
        print(f""Failed to fetch timezone data for coordinates ({lat}, {lon}): {response.status_code}"")

    return None


if __name__ == '__main__':
    # Define the cities you want to get data for
    cities = [""New York"", ""Tokyo"", ""London""]

    for city in cities:
        # Step 1: Get geographical data for the city
        location_data = get_location_data(city)

        if location_data:
            print(f""\nLocation Data for {city}:"")
            print(f""Name: {location_data['name']}"")
            print(f""Country: {location_data['country']}"")
            print(f""Latitude: {location_data['lat']}, Longitude: {location_data['lon']}"")

            # Step 2: Use latitude and longitude to get local time
            timezone_data = get_time_zone(location_data['lat'], location_data['lon'])

            if timezone_data:
                print(f""Local Time Information for {city}:"")
                print(f""Time Zone ID: {timezone_data['location']['tz_id']}"")
                print(f""Local Time: {timezone_data['location']['localtime']}"")
","Location Data for New York:
Name: New York
Country: US
Latitude: 40.7127281, Longitude: -74.0060152
Local Time Information for New York:
Time Zone ID: America/New_York
Local Time: 2024-10-26 14:24

Location Data for Tokyo:
Name: Tokyo
Country: JP
Latitude: 35.6828387, Longitude: 139.7594549
Local Time Information for Tokyo:
Time Zone ID: Asia/Tokyo
Local Time: 2024-10-27 03:24

Location Data for London:
Name: London
Country: GB
Latitude: 51.5073219, Longitude: -0.1276474
Local Time Information for London:
Time Zone ID: Europe/London
Local Time: 2024-10-26 19:24",1,1,10,30540
"I need to get information about NBA teams, including their players and details. First, gather a list of all NBA teams.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
NBA_API_HOST = 'nba-api-free-data.p.rapidapi.com'

def get_all_nba_teams():
    url = f""https://{NBA_API_HOST}/nba-team-list""

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": NBA_API_HOST
    }

    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        return response.json()  # Return list of NBA teams
    else:
        print(f""Failed to fetch NBA teams: {response.status_code}"")
        return None

def get_nba_players_by_team_id(team_id):
    url = f""https://{NBA_API_HOST}/nba-player-listing/v1/data""

    querystring = {""id"": team_id}

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": NBA_API_HOST
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()  # Return players data for a specific team
    else:
        print(f""Failed to fetch players for team ID {team_id}: {response.status_code}"")
        return None

def get_nba_team_details(team_id):
    url = f""https://{NBA_API_HOST}/nba-team-info/v1/data""

    querystring = {""id"": team_id}

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": NBA_API_HOST
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()  # Return detailed info for a specific team
    else:
        print(f""Failed to fetch team details for team ID {team_id}: {response.status_code}"")
        return None

if __name__ == '__main__':
    # Step 1: Retrieve the list of all NBA teams
    nba_teams = get_all_nba_teams()

    if nba_teams and 'teams' in nba_teams:
        print(""\nList of NBA Teams:"")
        selected_teams = nba_teams['teams'][:3]  # Select the first 3 teams for detailed analysis

        for team in selected_teams:
            print(f""Team ID: {team['id']}, Name: {team['displayName']}"")

            # Step 2: Fetch players for each selected team
            players = get_nba_players_by_team_id(team['id'])

            if players and 'athletes' in players:
                print(f""\nPlayers for {team['displayName']}:"")
                for player in players['athletes'][:3]:  # Display first 3 players
                    print(f""Player Name: {player['fullName']}, Position: {player['position']['displayName']}"")

            # Step 3: Fetch detailed team info for each selected team
            team_details = get_nba_team_details(team['id'])

            if team_details and 'team' in team_details:
                team_info = team_details['team']
                print(f""\nDetailed Info for {team_info['displayName']}:"")
                print(f""Location: {team_info['location']}, Abbreviation: {team_info['abbreviation']}"")
                print(f""Record: {team_info['recordSummary']}, Standing: {team_info['standingSummary']}"")
    else:
        print(""No NBA teams found."")
","List of NBA Teams:
Team ID: 1, Name: Atlanta Hawks

Players for Atlanta Hawks:
Player Name: Dominick Barlow, Position: Forward
Player Name: Bogdan Bogdanovic, Position: Shooting Guard
Player Name: Kobe Bufkin, Position: Guard

Detailed Info for Atlanta Hawks:
Location: Atlanta, Abbreviation: ATL
Record: 2-0, Standing: 1st in Southeast Division
Team ID: 2, Name: Boston Celtics

Players for Boston Celtics:
Player Name: Jaylen Brown, Position: Shooting Guard
Player Name: JD Davison, Position: Shooting Guard
Player Name: Sam Hauser, Position: Small Forward

Detailed Info for Boston Celtics:
Location: Boston, Abbreviation: BOS
Record: 2-0, Standing: 1st in Atlantic Division
Team ID: 17, Name: Brooklyn Nets

Players for Brooklyn Nets:
Player Name: Bojan Bogdanovic, Position: Small Forward
Player Name: Nic Claxton, Position: Center
Player Name: Noah Clowney, Position: Forward

Detailed Info for Brooklyn Nets:
Location: Brooklyn, Abbreviation: BKN
Record: 0-2, Standing: 4th in Atlantic Division",1,1,17,46631
" I want to gather detailed information about a specific movie and also retrieve related anime titles. First, provide comprehensive details of a movie including its synopsis, genres, release date, streaming sources, and recommendations for similar movies. Then, fetch anime titles with similar genres and content from a large anime database, with at least three results from both categories.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'
MOVIE_API_HOST = 'movies-api14.p.rapidapi.com'
ANIME_API_HOST = 'anime-db.p.rapidapi.com'

def get_movie_details(movie_id):
    url = f""https://{MOVIE_API_HOST}/movie/{movie_id}""

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": MOVIE_API_HOST
    }

    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        return response.json()  # Return detailed information of a movie
    else:
        print(f""Failed to fetch movie details for movie ID {movie_id}: {response.status_code}"")
        return None

def get_anime_titles(page, size, genres):
    url = f""https://{ANIME_API_HOST}/anime""

    querystring = {
        ""page"": page,
        ""size"": size,
        ""genres"": genres,
        ""sortBy"": ""ranking"",
        ""sortOrder"": ""asc""
    }

    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ANIME_API_HOST
    }

    response = requests.get(url, headers=headers, params=querystring)

    if response.status_code == 200:
        return response.json()  # Return anime titles based on criteria
    else:
        print(f""Failed to fetch anime titles: {response.status_code}"")
        return None

if __name__ == '__main__':
    # Step 1: Fetch detailed movie information
    movie_id = ""27205""  # Example movie ID
    movie_details = get_movie_details(movie_id)

    if movie_details and 'movie' in movie_details:
        movie_info = movie_details['movie']
        print(""\nMovie Information:"")
        print(f""Title: {movie_info['title']}"")
        print(f""Overview: {movie_info['overview']}"")
        print(f""Release Date: {movie_info['release_date']}"")
        print(f""Genres: {', '.join(movie_info['genres'])}"")
        print(f""Rating: {movie_info['vote_average']} ({movie_info['vote_count']} votes)"")
        print(""\nStreaming Sources:"")
        for source in movie_info['sources'][:3]:  # Display the first 3 streaming sources
            print(f""Platform: {source['display_name']}, Type: {source['type']}, URL: {source['link']}"")

        print(""\nSimilar Movies:"")
        for similar in movie_details['similarMovies'][:3]:  # Display first 3 similar movies
            print(f""Title: {similar['title']}, Poster URL: {similar['poster_path']}"")

        # Step 2: Use movie genres to fetch related anime titles
        movie_genres = ','.join(movie_info['genres'])
        anime_data = get_anime_titles(page=""1"", size=""3"", genres=movie_genres)

        if anime_data and 'data' in anime_data:
            print(""\nRelated Anime Titles:"")
            for anime in anime_data['data']:
                print(f""Title: {anime['title']}, Genre: {', '.join(anime['genres'])}"")
                print(f""Synopsis: {anime['synopsis']}"")
                print(f""Link: {anime['link']}"")
    else:
        print(""No movie details found."")
","Movie Information:
Title: Inception
Overview: Cobb, a skilled thief who commits corporate espionage by infiltrating the subconscious of his targets is offered a chance to regain his old life as payment for a task considered to be impossible: ""inception"", the implantation of another person's idea into a target's subconscious.
Release Date: 2010-07-15
Genres: Action & Adventure, Sci-Fi & Fantasy, Action & Adventure
Rating: 8.362 (33655 votes)

Streaming Sources:
Platform: HBO Max, Type: subscription, URL: https://play.hbomax.com/feature/urn:hbo:feature:GYDbfcQGssU-dEAEAAAAI
Platform: Netflix, Type: subscription, URL: https://www.netflix.com/watch/70131314
Platform: Apple TV, Type: purchase, URL: https://tv.apple.com/us/movie/inception/umc.cmc.6loas01ow0w4lkatxxloz7a6e

Similar Movies:
Title: Strange Days, Poster URL: https://image.tmdb.org/t/p/original/dwstlGA74LujvsLpGGaxtBvgx67.jpg
Title: Beverly Hills Cop III, Poster URL: https://image.tmdb.org/t/p/original/7vIpCkgGEfk5LZwm78nMxswLJRH.jpg
Title: The Matrix, Poster URL: https://image.tmdb.org/t/p/original/f89U3ADr1oiB1s9GkdPOEpXUk5H.jpg

Related Anime Titles:",1,1,10,29350
"I want retrieve details of three trending TV shows and three popular movies, including their titles, genres, release dates, and short descriptions. Display the information separately for movies and TV shows, along with the poster URLs","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

# Fetch trending TV shows
def get_shows():
    url = ""https://movies-api14.p.rapidapi.com/shows""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""movies-api14.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    data = response.json()

    # Retrieve top 3 TV shows
    shows = data.get(""movies"", [])[:3]
    result = []
    for show in shows:
        result.append({
            ""title"": show.get(""title""),
            ""genres"": show.get(""genres""),
            ""first_aired"": show.get(""first_aired""),
            ""overview"": show.get(""overview""),
            ""poster_path"": show.get(""poster_path"")
        })
    return result

# Fetch popular movies
def get_movies():
    url = ""https://movies-api14.p.rapidapi.com/movies""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""movies-api14.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    data = response.json()

    # Retrieve top 3 movies
    movies = data.get(""movies"", [])[:3]
    result = []
    for movie in movies:
        result.append({
            ""title"": movie.get(""title""),
            ""genres"": movie.get(""genres""),
            ""release_date"": movie.get(""release_date""),
            ""overview"": movie.get(""overview""),
            ""poster_path"": movie.get(""poster_path"")
        })
    return result

# Main function to output results
if __name__ == '__main__':
    shows = get_shows()
    movies = get_movies()

    # Display TV show information
    print(""TV Shows:"")
    for show in shows:
        print(f""Title: {show['title']}"")
        print(f""Genres: {', '.join(show['genres'])}"")
        print(f""First Aired: {show['first_aired']}"")
        print(f""Overview: {show['overview']}"")
        print(f""Poster URL: {show['poster_path']}"")
        print(""\n"" + ""=""*40 + ""\n"")

    # Display movie information
    print(""Movies:"")
    for movie in movies:
        print(f""Title: {movie['title']}"")
        print(f""Genres: {', '.join(movie['genres'])}"")
        print(f""Release Date: {movie['release_date']}"")
        print(f""Overview: {movie['overview']}"")
        print(f""Poster URL: {movie['poster_path']}"")
        print(""\n"" + ""=""*40 + ""\n"")
","TV Shows:
Title: Law & Order: Special Victims Unit
Genres: Crime, Drama, Mystery
First Aired: 1999-09-20
Overview: In the criminal justice system, sexually-based offenses are considered especially heinous. In New York City, the dedicated detectives who investigate these vicious felonies are members of an elite squad known as the Special Victims Unit. These are their stories.
Poster URL: https://image.tmdb.org/t/p/original/ywBt4WKADdMVgxTR1rS2uFwMYTH.jpg

========================================

Title: The Simpsons
Genres: Family, Animation, Comedy
First Aired: 1989-12-17
Overview: Set in Springfield, the average American town, the show focuses on the antics and everyday adventures of the Simpson family; Homer, Marge, Bart, Lisa and Maggie, as well as a virtual cast of thousands. Since the beginning, the series has been a pop culture icon, attracting hundreds of celebrities to guest star. The show has also made name for itself in its fearless satirical take on politics, media and American life in general.
Poster URL: https://image.tmdb.org/t/p/original/zI3E2a3WYma5w8emI35mgq5Iurx.jpg

========================================

Title: The Last of Us
Genres: Drama
First Aired: 2023-01-15
Overview: Twenty years after modern civilization has been destroyed, Joel, a hardened survivor, is hired to smuggle Ellie, a 14-year-old girl, out of an oppressive quarantine zone. What starts as a small job soon becomes a brutal, heartbreaking journey, as they both must traverse the United States and depend on each other for survival.
Poster URL: https://image.tmdb.org/t/p/original/uKvVjHNqB5VmOrdxqAt2F7J78ED.jpg

========================================

Movies:
Title: Ghosted
Genres: Action & Adventure, Comedy, Romance
Release Date: 2023-04-18
Overview: Salt-of-the-earth Cole falls head over heels for enigmatic Sadie â€” but then makes the shocking discovery that sheâ€™s a secret agent. Before they can decide on a second date, Cole and Sadie are swept away on an international adventure to save the world.
Poster URL: https://image.tmdb.org/t/p/original/liLN69YgoovHVgmlHJ876PKi5Yi.jpg

========================================

Title: AKA
Genres: Thriller, Action & Adventure, Crime
Release Date: 2023-04-28
Overview: A steely special ops agent finds his morality put to the test when he infiltrates a crime syndicate and unexpectedly bonds with the boss' young son.
Poster URL: https://image.tmdb.org/t/p/original/3BSxAjiporlwQTWzaHZ9Yrl5C9D.jpg

========================================

Title: Scream VI
Genres: Horror, Mystery, Thriller
Release Date: 2023-03-08
Overview: Following the latest Ghostface killings, the four survivors leave Woodsboro behind and start a fresh chapter.
Poster URL: https://image.tmdb.org/t/p/original/wDWwtvkRRlgTiUr6TyLSMX8FCuZ.jpg

========================================

",1,1,11,37323
"I want find three popular movies and then perform a search for these movies based on their titles to retrieve additional details. Display each movie's title, genre, release date, synopsis, and available streaming platforms. Provide a poster link for each movie as well.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

# Fetch list of popular movies
def get_movies():
    url = ""https://movies-api14.p.rapidapi.com/movies""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""movies-api14.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    data = response.json()

    # Return top 3 movies with basic info
    movies = data.get(""movies"", [])[:3]
    return [{""title"": movie.get(""title""), ""id"": movie.get(""_id"")} for movie in movies]

# Perform a search for additional details on each movie
def search_movie(query):
    url = ""https://movies-api14.p.rapidapi.com/search""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""movies-api14.p.rapidapi.com""
    }
    params = {""query"": query}
    response = requests.get(url, headers=headers, params=params)
    return response.json()

# Main function to retrieve and display movie details
if __name__ == '__main__':
    # Fetch top 3 movies
    movies = get_movies()

    # Fetch additional details for each movie
    for movie in movies:
        details = search_movie(movie[""title""])

        # Extract and display movie details
        if details.get(""contents""):
            movie_detail = details[""contents""][0]
            title = movie_detail.get(""title"")
            genres = movie_detail.get(""genres"", [])
            release_date = movie_detail.get(""release_date"")
            overview = movie_detail.get(""overview"")
            poster_path = movie_detail.get(""poster_path"")
            streaming_sources = [
                f""{source['display_name']} - {source['link']}""
                for source in movie_detail.get(""sources"", [])
            ]

            # Output movie information
            print(f""Title: {title}"")
            print(f""Genres: {', '.join(genres)}"")
            print(f""Release Date: {release_date}"")
            print(f""Overview: {overview}"")
            print(f""Poster URL: {poster_path}"")
            print(""Available on:"")
            for source in streaming_sources:
                print(f"" - {source}"")
            print(""\n"" + ""=""*40 + ""\n"")
",Fail,-1,-1,30,105300
"I want retrieve a list of available movie genres and use them to filter trending movies from different categories. For each genre, get three trending or new release movies, showing the title, genre, release date, and a short description, along with poster images.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

# Retrieve available genres for filtering
def get_genres():
    url = ""https://ott-details.p.rapidapi.com/getParams""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""ott-details.p.rapidapi.com""
    }
    params = {""param"": ""genre""}
    response = requests.get(url, headers=headers, params=params)
    return response.json()

# Retrieve trending and new release movies in specific categories
def get_trending_movies():
    url = ""https://movies-api14.p.rapidapi.com/home""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""movies-api14.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    data = response.json()

    # Limit to top 3 movies per category
    movies_by_category = []
    for category in data[:3]:  # Limit to 3 categories for simplicity
        category_movies = category[""movies""][:3]
        movies_by_category.append({
            ""category_title"": category[""title""],
            ""movies"": [{
                ""title"": movie[""title""],
                ""genres"": movie[""genres""],
                ""release_date"": movie[""release_date""],
                ""overview"": movie[""overview""],
                ""poster_path"": movie[""poster_path""]
            } for movie in category_movies]
        })
    return movies_by_category

# Main function to display genres and movies
if __name__ == '__main__':
    genres = get_genres()
    trending_movies = get_trending_movies()

    # Display available genres
    print(""Available Genres:"")
    for genre in genres:
        print(f""- {genre}"")
    print(""\n"" + ""=""*40 + ""\n"")

    # Display trending movies by category
    for category in trending_movies:
        print(f""Category: {category['category_title']}"")
        for movie in category[""movies""]:
            print(f""Title: {movie['title']}"")
            print(f""Genres: {', '.join(movie['genres'])}"")
            print(f""Release Date: {movie['release_date']}"")
            print(f""Overview: {movie['overview']}"")
            print(f""Poster URL: {movie['poster_path']}"")
            print(""\n"" + ""-""*20 + ""\n"")
        print(""\n"" + ""=""*40 + ""\n"")
",Fail,-1,-1,31,81065
"I want retrieve a list of three recent action movies with an IMDb rating between 6 and 8. For each movie, get additional details, including the plot summary, release date, genres, and available streaming sources. Display each movieâ€™s title, genres, release date, IMDb rating, plot, streaming platform information, and a poster image","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

# Perform an advanced search for action movies with specific IMDb rating range
def advanced_search():
    url = ""https://ott-details.p.rapidapi.com/advancedsearch""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""ott-details.p.rapidapi.com""
    }
    params = {
        ""start_year"": ""2010"",
        ""end_year"": ""2022"",
        ""min_imdb"": ""6"",
        ""max_imdb"": ""8"",
        ""genre"": ""action"",
        ""language"": ""english"",
        ""type"": ""movie"",
        ""sort"": ""latest"",
        ""page"": ""1""
    }
    response = requests.get(url, headers=headers, params=params)
    data = response.json()

    # Retrieve top 3 movies based on the search
    results = data.get(""results"", [])[:3]
    return [{""title"": movie[""title""], ""id"": movie[""imdbid""]} for movie in results]

# Fetch detailed information for a specific movie
def get_movie_details(movie_id):
    url = f""https://movies-api14.p.rapidapi.com/movie/{movie_id}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""movies-api14.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

# Main function to get and display movie details
if __name__ == '__main__':
    # Get the list of action movies from the search
    movies = advanced_search()

    # Fetch and display details for each movie
    for movie in movies:
        details = get_movie_details(movie[""id""])
        movie_data = details.get(""movie"", {})

        title = movie_data.get(""title"")
        genres = movie_data.get(""genres"", [])
        release_date = movie_data.get(""release_date"")
        overview = movie_data.get(""overview"")
        imdb_rating = movie_data.get(""vote_average"")
        poster_path = movie_data.get(""poster_path"")
        streaming_sources = [
            f""{source['display_name']} - {source['link']}""
            for source in movie_data.get(""sources"", [])
        ]

        # Output movie information
        print(f""Title: {title}"")
        print(f""Genres: {', '.join(genres)}"")
        print(f""Release Date: {release_date}"")
        print(f""IMDb Rating: {imdb_rating}"")
        print(f""Overview: {overview}"")
        print(f""Poster URL: {poster_path}"")
        print(""Available on:"")
        for source in streaming_sources:
            print(f"" - {source}"")
        print(""\n"" + ""=""*40 + ""\n"")
",Fail,-1,-1,32,90080
"I want retrieve a list of available genres and use it to search for top-ranked anime from selected genres. Retrieve three anime titles per genre, showing details including title, ranking, episode count, synopsis, status, and poster image. Display the information sorted by ranking for each genre.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

# Retrieve a list of genres
def get_genres():
    url = ""https://advanced-movie-search.p.rapidapi.com/genre/movie/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""advanced-movie-search.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    data = response.json()

    # Select top 3 genres to use in search (for demonstration purposes)
    genres = [genre[""name""] for genre in data.get(""genres"", [])[:3]]
    return genres

# Search for top-ranked anime in selected genres
def get_anime_by_genre(genre):
    url = ""https://anime-db.p.rapidapi.com/anime""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""anime-db.p.rapidapi.com""
    }
    params = {
        ""page"": ""1"",
        ""size"": ""3"",  # Limit to top 3 anime for each genre
        ""genres"": genre,
        ""sortBy"": ""ranking"",
        ""sortOrder"": ""asc""
    }
    response = requests.get(url, headers=headers, params=params)
    return response.json()

# Main function to get and display anime details by genre
if __name__ == '__main__':
    # Get list of genres
    genres = get_genres()

    # Retrieve and display top anime for each genre
    for genre in genres:
        print(f""Genre: {genre}"")
        anime_data = get_anime_by_genre(genre)

        for anime in anime_data.get(""data"", []):
            title = anime.get(""title"")
            ranking = anime.get(""ranking"")
            episodes = anime.get(""episodes"")
            synopsis = anime.get(""synopsis"")
            status = anime.get(""status"")
            poster_path = anime.get(""image"")

            # Display anime details
            print(f""Title: {title}"")
            print(f""Ranking: {ranking}"")
            print(f""Episodes: {episodes}"")
            print(f""Status: {status}"")
            print(f""Synopsis: {synopsis}"")
            print(f""Poster URL: {poster_path}"")
            print(""\n"" + ""-""*20 + ""\n"")
        print(""\n"" + ""=""*40 + ""\n"")
","Genre: Action
Title: Fullmetal Alchemist: Brotherhood
Ranking: 2
Episodes: 64
Status: Finished Airing
Synopsis: After a horrific alchemy experiment goes wrong in the Elric household, brothers Edward and Alphonse are left in a catastrophic new reality. Ignoring the alchemical principle banning human transmutation, the boys attempted to bring their recently deceased mother back to life. Instead, they suffered brutal personal loss: Alphonse's body disintegrated while Edward lost a leg and then sacrificed an arm to keep Alphonse's soul in the physical realm by binding it to a hulking suit of armor.

The brothers are rescued by their neighbor Pinako Rockbell and her granddaughter Winry. Known as a bio-mechanical engineering prodigy, Winry creates prosthetic limbs for Edward by utilizing ""automail,"" a tough, versatile metal used in robots and combat armor. After years of training, the Elric brothers set off on a quest to restore their bodies by locating the Philosopher's Stoneâ€”a powerful gem that allows an alchemist to defy the traditional laws of Equivalent Exchange.

As Edward becomes an infamous alchemist and gains the nickname ""Fullmetal,"" the boys' journey embroils them in a growing conspiracy that threatens the fate of the world.

[Written by MAL Rewrite]
Poster URL: https://cdn.myanimelist.net/images/anime/1208/94745.webp

--------------------

Title: GintamaÂ°
Ranking: 4
Episodes: 51
Status: Finished Airing
Synopsis: Gintoki, Shinpachi, and Kagura return as the fun-loving but broke members of the Yorozuya team! Living in an alternate-reality Edo, where swords are prohibited and alien overlords have conquered Japan, they try to thrive on doing whatever work they can get their hands on. However, Shinpachi and Kagura still haven't been paid... Does Gin-chan really spend all that cash playing pachinko?

Meanwhile, when Gintoki drunkenly staggers home one night, an alien spaceship crashes nearby. A fatally injured crew member emerges from the ship and gives Gintoki a strange, clock-shaped device, warning him that it is incredibly powerful and must be safeguarded. Mistaking it for his alarm clock, Gintoki proceeds to smash the device the next morning and suddenly discovers that the world outside his apartment has come to a standstill. With Kagura and Shinpachi at his side, he sets off to get the device fixed; though, as usual, nothing is ever that simple for the Yorozuya team.

Filled with tongue-in-cheek humor and moments of heartfelt emotion, Gintama's fourth season finds Gintoki and his friends facing both their most hilarious misadventures and most dangerous crises yet.

[Written by MAL Rewrite]
Poster URL: https://cdn.myanimelist.net/images/anime/3/72078.webp

--------------------

Title: Shingeki no Kyojin Season 3 Part 2
Ranking: 5
Episodes: 10
Status: Finished Airing
Synopsis: Seeking to restore humanity's diminishing hope, the Survey Corps embark on a mission to retake Wall Maria, where the battle against the merciless ""Titans"" takes the stage once again.

Returning to the tattered Shiganshina District that was once his home, Eren Yeager and the Corps find the town oddly unoccupied by Titans. Even after the outer gate is plugged, they strangely encounter no opposition. The mission progresses smoothly until Armin Arlert, highly suspicious of the enemy's absence, discovers distressing signs of a potential scheme against them.

Shingeki no Kyojin Season 3 Part 2 follows Eren as he vows to take back everything that was once his. Alongside him, the Survey Corps striveâ€”through countless sacrificesâ€”to carve a path towards victory and uncover the secrets locked away in the Yeager family's basement.

[Written by MAL Rewrite]
Poster URL: https://cdn.myanimelist.net/images/anime/1517/100633.webp

--------------------


========================================

Genre: Adventure
Title: Sousou no Frieren
Ranking: 1
Episodes: 0
Status: Not yet aired
Synopsis: The demon king has been defeated, and the victorious hero party returns home before disbanding. The fourâ€”mage Frieren, hero Himmel, priest Heiter, and warrior Eisenâ€”reminisce about their decade-long journey as the moment to bid each other farewell arrives. But the passing of time is different for elves, thus Frieren witnesses her companions slowly pass away one by one.

Before his death, Heiter manages to foist a young human apprentice called Fern onto Frieren. Driven by the elf's passion for collecting a myriad of magic spells, the pair embarks on a seemingly aimless journey, revisiting the places that the heroes of yore had visited. Along their travels, Frieren slowly confronts her regrets of missed opportunities to form deeper bonds with her now-deceased comrades.

[Written by MAL Rewrite]
Poster URL: https://cdn.myanimelist.net/images/anime/1015/138006.webp

--------------------

Title: Fullmetal Alchemist: Brotherhood
Ranking: 2
Episodes: 64
Status: Finished Airing
Synopsis: After a horrific alchemy experiment goes wrong in the Elric household, brothers Edward and Alphonse are left in a catastrophic new reality. Ignoring the alchemical principle banning human transmutation, the boys attempted to bring their recently deceased mother back to life. Instead, they suffered brutal personal loss: Alphonse's body disintegrated while Edward lost a leg and then sacrificed an arm to keep Alphonse's soul in the physical realm by binding it to a hulking suit of armor.

The brothers are rescued by their neighbor Pinako Rockbell and her granddaughter Winry. Known as a bio-mechanical engineering prodigy, Winry creates prosthetic limbs for Edward by utilizing ""automail,"" a tough, versatile metal used in robots and combat armor. After years of training, the Elric brothers set off on a quest to restore their bodies by locating the Philosopher's Stoneâ€”a powerful gem that allows an alchemist to defy the traditional laws of Equivalent Exchange.

As Edward becomes an infamous alchemist and gains the nickname ""Fullmetal,"" the boys' journey embroils them in a growing conspiracy that threatens the fate of the world.

[Written by MAL Rewrite]
Poster URL: https://cdn.myanimelist.net/images/anime/1208/94745.webp

--------------------

Title: Hunter x Hunter (2011)
Ranking: 8
Episodes: 148
Status: Finished Airing
Synopsis: Hunters devote themselves to accomplishing hazardous tasks, all from traversing the world's uncharted territories to locating rare items and monsters. Before becoming a Hunter, one must pass the Hunter Examinationâ€”a high-risk selection process in which most applicants end up handicapped or worse, deceased.

Ambitious participants who challenge the notorious exam carry their own reason. What drives 12-year-old Gon Freecss is finding Ging, his father and a Hunter himself. Believing that he will meet his father by becoming a Hunter, Gon takes the first step to walk the same path.

During the Hunter Examination, Gon befriends the medical student Leorio Paladiknight, the vindictive Kurapika, and ex-assassin Killua Zoldyck. While their motives vastly differ from each other, they band together for a common goal and begin to venture into a perilous world.

[Written by MAL Rewrite]
Poster URL: https://cdn.myanimelist.net/images/anime/1337/99013.webp

--------------------


========================================

Genre: Animation

========================================
",1,1,14,44086
"I want retrieve information about recently added movies available for streaming in the US, including the title, genre, IMDb rating, release year, synopsis, and streaming platforms. Additionally, fetch basic biographical details about Robert Downey Jr., including his birth name, birth date, and notable works. Finally, get details of three top-ranked anime titles, including their title, genre, ranking, and a brief synopsis.etrieve information about recently added movies available for streaming in the US, including the title, genre, IMDb rating, release year, synopsis, and streaming platforms. Additionally, fetch basic biographical details about Robert Downey Jr., including his birth name, birth date, and notable works. Finally, get details of three top-ranked anime titles, including their title, genre, ranking, and a brief synopsis.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

# Retrieve recently added movies with US streaming availability
def get_new_arrivals():
    url = ""https://ott-details.p.rapidapi.com/getnew""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""ott-details.p.rapidapi.com""
    }
    params = {
        ""region"": ""US"",
        ""page"": ""1""
    }
    response = requests.get(url, headers=headers, params=params)
    return response.json()

# Retrieve basic information about Robert Downey Jr.
def get_actor_info():
    url = ""https://ott-details.p.rapidapi.com/getcastDetails""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""ott-details.p.rapidapi.com""
    }
    params = {""peopleid"": ""nm0000375""}  # IMDb ID for Robert Downey Jr.
    response = requests.get(url, headers=headers, params=params)
    return response.json()

# Retrieve top-ranked anime titles
def get_top_anime():
    url = ""https://anime-db.p.rapidapi.com/anime""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""anime-db.p.rapidapi.com""
    }
    params = {
        ""page"": ""1"",
        ""size"": ""3"",  # Retrieve top 3 anime
        ""sortBy"": ""ranking"",
        ""sortOrder"": ""asc""
    }
    response = requests.get(url, headers=headers, params=params)
    return response.json()

# Main function to get and display all required details
if __name__ == '__main__':
    # Step 1: Get new movie arrivals
    new_arrivals = get_new_arrivals()
    print(""Recently Added Movies:"")
    for movie in new_arrivals.get(""results"", [])[:3]:  # Limiting to 3 movies
        title = movie.get(""title"")
        genres = movie.get(""genre"", [])
        imdb_rating = movie.get(""imdbrating"")
        release_year = movie.get(""released"")
        synopsis = movie.get(""synopsis"")
        streaming_info = movie.get(""streamingAvailability"", {}).get(""country"", {}).get(""US"", [])

        print(f""Title: {title}"")
        print(f""Genres: {', '.join(genres)}"")
        print(f""IMDb Rating: {imdb_rating}"")
        print(f""Release Year: {release_year}"")
        print(f""Synopsis: {synopsis}"")
        print(""Available on:"")
        for platform in streaming_info:
            print(f"" - {platform['platform']}: {platform['url']}"")
        print(""\n"" + ""-""*20 + ""\n"")
    print(""\n"" + ""=""*40 + ""\n"")

    # Step 2: Get Robert Downey Jr.'s details
    actor_info = get_actor_info()
    print(""Actor Bio:"")
    print(f""Name: {actor_info.get('name')}"")
    print(f""Birth Name: {actor_info.get('birthName')}"")
    print(f""Born: {actor_info.get('born')}"")
    print(""Notable Works:"")
    for title_id in actor_info.get(""knownForTitles"", []):
        print(f"" - {title_id}"")
    print(""\n"" + ""=""*40 + ""\n"")

    # Step 3: Retrieve top anime
    anime_list = get_top_anime().get(""data"", [])
    print(""Top-Ranked Anime:"")
    for anime in anime_list:
        title = anime.get(""title"")
        ranking = anime.get(""ranking"")
        genres = anime.get(""genres"", [])
        synopsis = anime.get(""synopsis"")

        print(f""Title: {title}"")
        print(f""Ranking: {ranking}"")
        print(f""Genres: {', '.join(genres)}"")
        print(f""Synopsis: {synopsis}"")
        print(""\n"" + ""-""*20 + ""\n"")
","Recently Added Movies:

========================================

Actor Bio:
Name: Robert Downey Jr.
Birth Name: Robert John Downey Jr
Born: April 4,      1965      inÂ Manhattan, New York City, New York, USA
Notable Works:
 - tt0371746
 - tt1300854
 - tt4154796
 - tt0988045

========================================

Top-Ranked Anime:
Title: Sousou no Frieren
Ranking: 1
Genres: Adventure, Drama, Fantasy
Synopsis: The demon king has been defeated, and the victorious hero party returns home before disbanding. The fourâ€”mage Frieren, hero Himmel, priest Heiter, and warrior Eisenâ€”reminisce about their decade-long journey as the moment to bid each other farewell arrives. But the passing of time is different for elves, thus Frieren witnesses her companions slowly pass away one by one.

Before his death, Heiter manages to foist a young human apprentice called Fern onto Frieren. Driven by the elf's passion for collecting a myriad of magic spells, the pair embarks on a seemingly aimless journey, revisiting the places that the heroes of yore had visited. Along their travels, Frieren slowly confronts her regrets of missed opportunities to form deeper bonds with her now-deceased comrades.

[Written by MAL Rewrite]

--------------------

Title: Fullmetal Alchemist: Brotherhood
Ranking: 2
Genres: Action, Adventure, Drama, Fantasy
Synopsis: After a horrific alchemy experiment goes wrong in the Elric household, brothers Edward and Alphonse are left in a catastrophic new reality. Ignoring the alchemical principle banning human transmutation, the boys attempted to bring their recently deceased mother back to life. Instead, they suffered brutal personal loss: Alphonse's body disintegrated while Edward lost a leg and then sacrificed an arm to keep Alphonse's soul in the physical realm by binding it to a hulking suit of armor.

The brothers are rescued by their neighbor Pinako Rockbell and her granddaughter Winry. Known as a bio-mechanical engineering prodigy, Winry creates prosthetic limbs for Edward by utilizing ""automail,"" a tough, versatile metal used in robots and combat armor. After years of training, the Elric brothers set off on a quest to restore their bodies by locating the Philosopher's Stoneâ€”a powerful gem that allows an alchemist to defy the traditional laws of Equivalent Exchange.

As Edward becomes an infamous alchemist and gains the nickname ""Fullmetal,"" the boys' journey embroils them in a growing conspiracy that threatens the fate of the world.

[Written by MAL Rewrite]

--------------------

Title: Steins;Gate
Ranking: 3
Genres: Drama, Sci-Fi, Suspense
Synopsis: Eccentric scientist Rintarou Okabe has a never-ending thirst for scientific exploration. Together with his ditzy but well-meaning friend Mayuri Shiina and his roommate Itaru Hashida, Rintarou founds the Future Gadget Laboratory in the hopes of creating technological innovations that baffle the human psyche. Despite claims of grandeur, the only notable ""gadget"" the trio have created is a microwave that has the mystifying power to turn bananas into green goo.

However, when Rintarou decides to attend neuroscientist Kurisu Makise's conference on time travel, he experiences a series of strange events that lead him to believe that there is more to the ""Phone Microwave"" gadget than meets the eye. Apparently able to send text messages into the past using the microwave, Rintarou dabbles further with the ""time machine,"" attracting the ire and attention of the mysterious organization SERN.

Due to the novel discovery, Rintarou and his friends find themselves in an ever-present danger. As he works to mitigate the damage his invention has caused to the timeline, he is not only fighting a battle to save his loved ones, but also one against his degrading sanity.

[Written by MAL Rewrite]

--------------------",1,1,19,57760
"I need to get a list of countries and, based on that, retrieve customer reviews for a specific product. Show me the product's top three reviews, including user details, review date, rating, content, and any images provided by the reviewer. Include the name of the country where each review was submitted if available.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

# Retrieve the list of countries
def get_countries():
    url = ""https://unofficial-shein.p.rapidapi.com/countries/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""unofficial-shein.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

# Retrieve customer reviews for a specific product
def get_reviews(goods_spu):
    url = ""https://unofficial-shein.p.rapidapi.com/reviews/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""unofficial-shein.p.rapidapi.com""
    }
    params = {
        ""goods_spu"": goods_spu,
        ""language"": ""en"",
        ""country"": ""US"",
        ""limit"": ""3"",
        ""page"": ""1""
    }
    response = requests.get(url, headers=headers, params=params)
    return response.json()

# Main function to retrieve countries and display product reviews
if __name__ == '__main__':
    # Step 1: Get the list of countries
    countries_data = get_countries()
    countries = countries_data.get(""info"", {}).get(""country"", {}).get(""item_cates"", [])
    print(""Countries List:"")
    for country in countries:
        print(f""{country['value']} - {country['country']}"")
    print(""\n"" + ""=""*40 + ""\n"")

    # Step 2: Retrieve reviews for a specific product
    reviews_data = get_reviews(""m22022854841"")  # Example goods_spu
    print(""Customer Reviews:"")
    for review in reviews_data.get(""info"", {}).get(""comment_info"", []):
        user_name = review.get(""user_name"")
        review_date = review.get(""commentTimeNewStyle"")
        rating = review.get(""comment_rank"")
        content = review.get(""content"")
        country_code = review.get(""language_flag"")
        images = [img.get(""member_image_small"") for img in review.get(""comment_image"", [])]

        # Display review information
        print(f""User: {user_name}"")
        print(f""Date: {review_date}"")
        print(f""Rating: {rating}"")
        print(f""Review: {content}"")
        if country_code:
            print(f""Country Code: {country_code}"")
        if images:
            print(""Images:"")
            for img in images:
                print(f"" - {img}"")
        print(""\n"" + ""-""*20 + ""\n"")
","Countries List:
AL - Albania
AD - Andorra
AO - Angola
AI - Anguilla
AG - Antigua and Barbuda
AR - Argentina
AM - Armenia
AW - Aruba
AU - Australia
AT - Austria
AZ - Azerbaijan
BS - Bahamas
BH - Bahrain
BB - Barbados
BE - Belgium
BZ - Belize
BJ - Benin
BM - Bermuda
BT - Bhutan
BO - Bolivia
BW - Botswana
BR - Brazil
BN - Brunei Darussalam
BG - Bulgaria
BF - Burkina Faso
BI - Burundi
KH - Cambodia
CM - Cameroon
CA - Canada
CV - Cape Verde
KY - Cayman Islands
CF - Central African Republic
TD - Chad
CL - Chile
CO - Colombia
KM - Comoros
CG - Congo
CK - Cook Islands
CR - Costa Rica
HR - Croatia (local name: Hrvatska)
CY - Cyprus
CZ - Czech Republic
DK - Denmark
DJ - Djibouti
DM - Dominica
DO - Dominican Republic
TL - East Timor
EC - Ecuador
EG - Egypt
SV - El Salvador
GQ - Equatorial Guinea
ER - Eritrea
EE - Estonia
ET - Ethiopia
FJ - Fiji
FI - Finland
FR - France
GA - Gabon
GM - Gambia
GE - Georgia
DE - Germany
GH - Ghana
GI - Gibraltar
GR - Greece
GL - Greenland
GD - Grenada
GP - Guadeloupe
GU - Guam
GN - Guinea
GW - Guinea-Bissau
GY - Guyana
HT - Haiti
HN - Honduras
HU - Hungary
IS - Iceland
ID - Indonesia
IE - Ireland
IL - Israel
IT - Italy
JM - Jamaica
JP - Japan
JO - Jordan
KZ - Kazakhstan
KE - Kenya
KW - Kuwait
KG - Kyrgyzstan
LA - Lao People's Democratic Republic
LV - Latvia
LB - Lebanon
LS - Lesotho
LR - Liberia
LI - Liechtenstein
LT - Lithuania
LU - Luxembourg
MO - Macau
MG - Madagascar
MW - Malawi
MY - Malaysia
MV - Maldives
ML - Mali
MT - Malta
MQ - Martinique
MU - Mauritius
YT - Mayotte
MX - Mexico
MD - Moldova
MN - Mongolia
ME - Montenegro
MA - Morocco
MZ - Mozambique
MM - Myanmar
NA - Namibia
NP - Nepal
NL - Netherlands
NC - New Caledonia
NZ - New Zealand
NG - Nigeria
NF - Norfolk Island
NO - Norway
OM - Oman
PK - Pakistan
PS - Palestine
PA - Panama
PG - Papua New Guinea
PY - Paraguay
PE - Peru
PH - Philippines
PL - Poland
PT - Portugal
PR - Puerto Rico
QA - Qatar
RE - Reunion
RO - Romania
RW - Rwanda
LC - Saint Lucia
SM - San Marino
SA - Saudi Arabia
RS - Serbia
SN - Senegal
SC - Seychelles
SL - Sierra Leone
SG - Singapore
SK - Slovakia (Slovak Republic)
SI - Slovenia
SO - Somalia
ZA - South Africa
KR - South Korea
ES - Spain
LK - Sri Lanka
SR - Suriname
SZ - Swaziland
SE - Sweden
CH - Switzerland
TW - Taiwan
TJ - Tajikistan
TZ - Tanzania
TH - Thailand
TG - Togo
TT - Trinidad and Tobago
TR - Turkey
TC - Turks and Caicos Islands
UG - Uganda
UA - Ukraine
AE - United Arab Emirates
GB - United Kingdom
US - United States
UZ - Uzbekistan
VU - Vanuatu
VA - Vatican City State (Holy See)
VN - Vietnam
VI - Virgin Islands (U.S.)
ZM - Zambia
ZW - Zimbabwe

========================================

Customer Reviews:
User: A***y
Date: 1 year ago
Rating: 5
Review:
Country Code: en
Images:
 - images3/2023/03/28/16799400683fd3568c434d692b7b43d73654558792_thumbnail_80x120.webp
 - images3/2023/03/28/167994007052833fb422a619aec64a2dcb5ee5635c_thumbnail_80x120.webp
 - images3/2023/03/28/1679940073e3e75b58fb7dfec40b2e42725ce7f87a_thumbnail_80x120.webp

--------------------

User: n***0
Date: 1 year ago
Rating: 5
Review: Love the shirt, very funny. Good material
Country Code: en
Images:
 - images3/2023/03/28/16799350097f7670592a47de5874da4f9ef287e901_thumbnail_80x120.webp

--------------------

User: T***a
Date: 1 year ago
Rating: 5
Review:
Country Code: en
Images:
 - images3/2023/03/22/1679450297277064458a47827a110c115354016325_thumbnail_80x120.webp

--------------------
",1,1,10,30330
"I need to view a list of countries and select specific ones to retrieve detailed information, including supported languages and currencies. Display the country's full name, supported languages, and currency details for each selected country.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

# Retrieve the list of countries
def get_countries():
    url = ""https://unofficial-shein.p.rapidapi.com/countries/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""unofficial-shein.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

# Retrieve detailed information about a specific country
def get_country_details(region):
    url = ""https://unofficial-shein.p.rapidapi.com/countries/detail""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""unofficial-shein.p.rapidapi.com""
    }
    params = {""region"": region}
    response = requests.get(url, headers=headers, params=params)
    return response.json()

# Main function to retrieve countries and display details for selected ones
if __name__ == '__main__':
    # Step 1: Get the list of countries
    countries_data = get_countries()
    countries = countries_data.get(""info"", {}).get(""country"", {}).get(""item_cates"", [])

    print(""Countries List:"")
    for country in countries[:3]:  # Limiting to the first three countries for demonstration
        print(f""{country['value']} - {country['country']}"")

    print(""\n"" + ""=""*40 + ""\n"")

    # Step 2: Retrieve and display details for selected countries
    for country in countries[:3]:  # Retrieve details for the first three countries
        region_code = country[""value""]
        details = get_country_details(region_code)

        print(f""Country: {details.get('info', {}).get('current_country_full_name')}"")

        # Display supported languages
        print(""Supported Languages:"")
        for lang in details.get(""info"", {}).get(""support_language_list"", []):
            print(f"" - {lang['languageTip']} ({lang['language']})"")

        # Display currency details
        print(""Supported Currencies:"")
        for currency in details.get(""info"", {}).get(""suppport_currency_list"", []):
            print(f"" - {currency['code']} ({currency['symbol_left']}{currency['symbol_right']}), Exchange Rate: {currency['value']}"")

        print(""\n"" + ""-""*20 + ""\n"")
",Fail,-1,-1,31,85529
"I need to view a list of navigation tabs with detailed information, including each tabâ€™s ID, name, associated channel, and category IDs. Then, based on a specified tabâ€™s channel ID, retrieve the root categories within that channel to see each categoryâ€™s name, ID, and sub-categories.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

# Task 1: Retrieve navigation tabs
def get_navigation_tabs():
    url = ""https://unofficial-shein.p.rapidapi.com/navigations/get-tabs""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""unofficial-shein.p.rapidapi.com""
    }
    params = {
        ""language"": ""en"",
        ""country"": ""US"",
        ""currency"": ""USD""
    }
    response = requests.get(url, headers=headers, params=params)
    return response.json()

# Task 2: Retrieve root categories based on a specific channel type
def get_root_categories(channel_type):
    url = ""https://unofficial-shein.p.rapidapi.com/navigations/get-root""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""unofficial-shein.p.rapidapi.com""
    }
    params = {
        ""language"": ""en"",
        ""country"": ""US"",
        ""channelType"": channel_type,
        ""currency"": ""USD""
    }
    response = requests.get(url, headers=headers, params=params)
    return response.json()

if __name__ == '__main__':
    # Step 1: Get and display navigation tabs
    navigation_data = get_navigation_tabs()
    tabs = navigation_data.get(""info"", {}).get(""tabs"", [])

    print(""Navigation Tabs List:"")
    for tab in tabs[:3]:  # Displaying the first three tabs for demonstration
        print(f""Tab ID: {tab['id']}"")
        print(f""Tab Name: {tab['name']}"")
        print(f""Channel Name: {tab['channelName']}"")
        print(f""Category IDs: {tab['cat_id']}"")
        print(""\n"" + ""-""*20 + ""\n"")

    # Example: Using the first tabâ€™s ID as the channelType parameter for the next API call
    channel_type = tabs[0]['id'] if tabs else None  # Checking if there's at least one tab

    if channel_type:
        # Step 2: Get and display root categories for the specified channel type
        root_data = get_root_categories(channel_type)
        categories = root_data.get(""info"", {}).get(""content"", [])

        print(""Root Categories List:"")
        for category in categories[:3]:  # Displaying the first three root categories
            print(f""Category Name: {category.get('name', 'N/A')}"")
            print(f""Category ID: {category.get('id', 'N/A')}"")
            print(""Sub-categories:"")
            for child in category.get(""child"", [])[:3]:  # Limiting to 3 sub-categories
                print(f"" - {child.get('name', 'Unnamed')} (ID: {child.get('navNodeId', 'N/A')})"")
            print(""\n"" + ""-""*20 + ""\n"")
",Fail,-1,-1,28,91336
"I need a list of all available countries, categorized into featured and standard countries, and their identifiers. Additionally, I want the navigation tabs with details for each tab, including their names, associated channels, and category IDs. Please provide results for three countries and three tabs.","API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

import requests

def get_countries_list():
    url = ""https://unofficial-shein.p.rapidapi.com/countries/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""unofficial-shein.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json().get(""info"", {}).get(""country"", {})

def get_navigation_tabs(language=""en"", country=""US"", currency=""USD""):
    url = ""https://unofficial-shein.p.rapidapi.com/navigations/get-tabs""
    querystring = {""language"": language, ""country"": country, ""currency"": currency}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""unofficial-shein.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json().get(""info"", {}).get(""tabs"", [])

if __name__ == '__main__':
    # Step 1: Retrieve the list of countries
    countries_info = get_countries_list()

    # Display first three results from featured and all countries
    featured_countries = countries_info.get(""hotcountry"", [])[:3]
    all_countries = countries_info.get(""item_cates"", [])[:3]

    print(""Featured Countries:"")
    for country in featured_countries:
        print(f""- {country['country']} (Code: {country['value']}, ID: {country['id']})"")

    print(""\nAll Countries:"")
    for country in all_countries:
        print(f""- {country['country']} (Code: {country['value']}, ID: {country['id']})"")

    # Step 2: Retrieve navigation tabs
    navigation_tabs = get_navigation_tabs()

    print(""\nNavigation Tabs:"")
    for tab in navigation_tabs[:3]:  # Display first three tabs
        print(f""- Tab Name: {tab['name']}, Channel: {tab['channelName']}, Category IDs: {tab['cat_id']}"")
","Featured Countries:
- United States (Code: US, ID: 226)

All Countries:
- Albania (Code: AL, ID: 2)
- Andorra (Code: AD, ID: 5)
- Angola (Code: AO, ID: 6)

Navigation Tabs:
- Tab Name: All, Channel: All, Category IDs: 1864,2026,2031,2032,2038,2273,2297,2400,3195,3224,3631,3634,3636,3637,3650,3657,4327,4328,4436,4083
- Tab Name: Women, Channel: Women, Category IDs: 4436,2041,2042,3644,7469,4816,4815,4813,4811,4808,4806,4804,3863,3694,3686,2475,4451,4801,4802,2031,2032,4314,4295,4293,4292,3812,3627,3626,3625,2347,2346,2208,2273,2297,2400,1894,2491,4329,4331,4335,4350,4366,4379,4396,4409,4413,4422,4424,5106,5247,5290,5301,5348,5796,5797,6376,7152,3224,1765,5806,5826,5906,5907,6219,6217,4656,4438,4219,3632,3035,3029,1745,4803,5928,2043,4099,4258,4268,4458,3650,3657,4083,4327,4328,2462,2463,2464,2465
- Tab Name: Curve, Channel: PlusSize, Category IDs: 1888,3734,2346,2347,2491,3613
",1,1,15,42015
"I need a list of countries, including their featured and full lists with each countryâ€™s unique identifiers and short codes. Then, for three specific countries from this list, I need to retrieve additional details, including supported languages and currency settings. Finally, I want to view the top reviews for a product based on available product identifiers, including user information, product details, and review comments. Please provide at least three reviews for context.","API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

import requests

def get_countries_list():
    url = ""https://unofficial-shein.p.rapidapi.com/countries/list""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""unofficial-shein.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json().get(""info"", {}).get(""country"", {})

def get_country_detail(region):
    url = ""https://unofficial-shein.p.rapidapi.com/countries/detail""
    querystring = {""region"": region}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""unofficial-shein.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json().get(""info"", {})

def get_reviews_list(goods_spu, cat_id, limit=3):
    url = ""https://unofficial-shein.p.rapidapi.com/reviews/list""
    querystring = {
        ""language"": ""en"",
        ""country"": ""US"",
        ""currency"": ""USD"",
        ""goods_spu"": goods_spu,
        ""cat_id"": cat_id,
        ""sort"": ""default"",
        ""limit"": str(limit),
        ""page"": ""1""
    }
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""unofficial-shein.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json().get(""info"", {}).get(""comment_info"", [])

if __name__ == '__main__':
    # Step 1: Retrieve the list of countries
    countries_info = get_countries_list()

    # Display featured and all countries
    featured_countries = countries_info.get(""hotcountry"", [])[:3]
    all_countries = countries_info.get(""item_cates"", [])[:3]

    print(""Featured Countries:"")
    for country in featured_countries:
        print(f""- {country['country']} (Code: {country['value']}, ID: {country['id']})"")

    print(""\nAll Countries:"")
    for country in all_countries:
        print(f""- {country['country']} (Code: {country['value']}, ID: {country['id']})"")

    # Step 2: Retrieve details for each featured country
    print(""\nCountry Details:"")
    for country in featured_countries:
        country_detail = get_country_detail(country['value'])
        print(f""\nDetails for {country['country']}:"")
        print(f""  - Supported Languages: {[lang['languageTip'] for lang in country_detail.get('support_language_list', [])]}"")
        print(f""  - Currencies: {[currency['code'] for currency in country_detail.get('suppport_currency_list', [])]}"")
        print(f""  - Full Name: {country_detail.get('current_country_full_name')}"")

    # Step 3: Retrieve and display reviews for a specific product
    goods_spu = ""m22022854841""  # Example product ID
    cat_id = ""1980""             # Example category ID
    reviews = get_reviews_list(goods_spu, cat_id)

    print(""\nProduct Reviews:"")
    for review in reviews[:3]:  # Displaying the top 3 reviews
        print(f""\nReview by {review['user_name']}:"")
        print(f""  - Rating: {review['comment_rank']}"")
        print(f""  - Comment: {review['content']}"")
        if review.get('comment_image'):
            print(""  - Images:"")
            for img in review['comment_image']:
                print(f""    - {img['member_image_small']}"")
","Featured Countries:
- United States (Code: US, ID: 226)

All Countries:
- Albania (Code: AL, ID: 2)
- Andorra (Code: AD, ID: 5)
- Angola (Code: AO, ID: 6)

Country Details:

Details for United States:
  - Supported Languages: ['English', 'EspaÃ±ol']
  - Currencies: ['USD', 'EUR', 'GBP', 'CHF', 'CAD', 'MXN']
  - Full Name: United States

Product Reviews:

Review by A***y:
  - Rating: 5
  - Comment:
  - Images:
    - images3/2023/03/28/16799400683fd3568c434d692b7b43d73654558792_thumbnail_80x120.webp
    - images3/2023/03/28/167994007052833fb422a619aec64a2dcb5ee5635c_thumbnail_80x120.webp
    - images3/2023/03/28/1679940073e3e75b58fb7dfec40b2e42725ce7f87a_thumbnail_80x120.webp

Review by n***0:
  - Rating: 5
  - Comment: Love the shirt, very funny. Good material
  - Images:
    - images3/2023/03/28/16799350097f7670592a47de5874da4f9ef287e901_thumbnail_80x120.webp

Review by T***a:
  - Rating: 5
  - Comment:
  - Images:
    - images3/2023/03/22/1679450297277064458a47827a110c115354016325_thumbnail_80x120.webp
",1,1,12,32652
"I need to retrieve the latest news articles from a specified source along with their publication details, images, and URLs. Additionally, I want a list of general recent news articles from various sources, including titles, publication dates, and brief previews. Please provide as many results as possible, with at least three articles from each category.","API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

import requests

def get_news_from_source(source_id=""bbc""):
    url = f""https://biztoc.p.rapidapi.com/news/source/{source_id}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""biztoc.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()[:3]  # Retrieve the first 3 articles

def get_latest_news():
    url = ""https://biztoc.p.rapidapi.com/news/latest""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""biztoc.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()[:3]  # Retrieve the first 3 articles

if __name__ == '__main__':
    # Step 1: Retrieve the latest news from a specific source (e.g., BBC)
    source_news = get_news_from_source(""bbc"")
    print(""Latest News from Source (BBC):"")
    for article in source_news:
        print(f""\nTitle: {article['title']}"")
        print(f""Published Date: {article['published']}"")
        print(f""URL: {article['url']}"")
        print(f""Image (Small): {article['img']['s']}"")
        print(f""Image (Square): {article['img']['sq']}"")
        print(f""Summary: {article['body']}"")

    # Step 2: Retrieve the latest general news
    latest_news = get_latest_news()
    print(""\nLatest General News:"")
    for article in latest_news:
        print(f""\nTitle: {article['title']}"")
        print(f""Source: {article['domain']}"")
        print(f""Published Date: {article['published']}"")
        print(f""URL: {article['url']}"")
        if article.get('img'):
            print(f""Image (Small): {article['img']['s']}"")
            print(f""Image (Square): {article['img']['sq']}"")
        print(f""Preview: {article['body_preview'] if 'body_preview' in article else article['body']}"")
","Latest News from Source (BBC):

Title: Firms must prevent sexual harassment under new law
Published Date: Fri, 25 Oct 2024 23:43:11 GMT
URL: https://www.bbc.co.uk/news/business-65311346?ref=biztoc.com
Image (Small): https://cdn.biztoc.com/1a0da8ad8878aa79_s.webp
Image (Square): https://cdn.biztoc.com/1a0da8ad8878aa79_sq.webp
Summary: Employers must take steps to prevent sexual harassment in the workplace, under a new law.
The Worker Protection Act also states employers must discipline or hold accountable those who are found guilty of sexual harassment.
The new rules from 26 October include harassment via social media and alsoâ€¦

Title: Employers' National Insurance hike to raise Â£20bn
Published Date: Fri, 25 Oct 2024 21:57:05 GMT
URL: https://www.bbc.com/news/articles/c9wrkngvyx4o?ref=biztoc.com
Image (Small): https://cdn.biztoc.com/3263d1f20185ed35_s.webp
Image (Square): https://cdn.biztoc.com/3263d1f20185ed35_sq.webp
Summary: Employers' National Insurance hike to raise Â£20bn
The chancellor is set to increase the amount employers in the UK pay in National Insurance to raise Â£20bn for public services at the Budget.
Rachel Reeves is also expected to lower the threshold for when employers start paying the tax, but is notâ€¦

Title: 'I can't run a business like this': Why the WordPress row matters
Published Date: Fri, 25 Oct 2024 21:55:34 GMT
URL: https://www.bbc.com/news/articles/cm2y7eyp3zpo?ref=biztoc.com
Image (Small): https://cdn.biztoc.com/48d5475d43274c4e_s.webp
Image (Square): https://cdn.biztoc.com/48d5475d43274c4e_sq.webp
Summary: 'I can't run a business like this': Why the WordPress row matters
One of the world's biggest web publishing platforms - used by a large chunk of the internet - is locked in a spat which is affecting thousands of businesses worldwide.
While most of the work WordPress does is not seen by internetâ€¦

Latest General News:

Title: Capcom Co., Ltd. (OTCMKTS:CCOEY) Short Interest Down 63.1% in October
Source: marketbeat.com
Published Date: 2024-10-26 17:27:17
URL: https://www.marketbeat.com/instant-alerts/otcmkts-ccoey-options-data-report-2024-10-26/?ref=biztoc.com
Preview: Capcom Co., Ltd. (OTCMKTS:CCOEY - Get Free Report) was the target of a large decline in short interest in October. As of October 15th, there was short interest totalling 10,300 shares, a decline of 63.1% from the September 30th total of 27,900 shares. Based on an average trading volume of 31,400â€¦

Title: CBB Bancorp, Inc. (OTCMKTS:CBBI) Short Interest Update
Source: marketbeat.com
Published Date: 2024-10-26 17:27:16
URL: https://www.marketbeat.com/instant-alerts/otcmkts-cbbi-options-data-report-2024-10-26/?ref=biztoc.com
Preview: CBB Bancorp, Inc. (OTCMKTS:CBBI - Get Free Report) was the target of a large drop in short interest in October. As of October 15th, there was short interest totalling 100 shares, a drop of 50.0% from the September 30th total of 200 shares. Based on an average daily volume of 6,200 shares, theâ€¦

Title: China Resources Building Materials Technology Holdings Limited (OTCMKTS:CARCY) Short Interest Up 125.0% in October
Source: marketbeat.com
Published Date: 2024-10-26 17:27:15
URL: https://www.marketbeat.com/instant-alerts/otcmkts-carcy-options-data-report-2024-10-26/?ref=biztoc.com
Preview: China Resources Building Materials Technology Holdings Limited (OTCMKTS:CARCY - Get Free Report) saw a significant growth in short interest in the month of October. As of October 15th, there was short interest totalling 900 shares, a growth of 125.0% from the September 30th total of 400 shares.â€¦
",1,1,13,47918
"I need a list of available news sources, including each source's title and website URL, excluding social media and ticker sources. Additionally, I want detailed information on movies or TV shows that match a specific search term, including details such as images, genre, release date, streaming sources, and average rating. Provide as many as three items for each request.","API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

import requests

def get_news_sources():
    url = ""https://biztoc.p.rapidapi.com/sources""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""biztoc.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()[:3]  # Retrieve the first 3 sources

def search_movies_or_tv(query=""breaking bad""):
    url = ""https://movies-api14.p.rapidapi.com/search""
    querystring = {""query"": query}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""movies-api14.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json().get(""contents"", [])[:3]  # Retrieve the first 3 search results

if __name__ == '__main__':
    # Step 1: Retrieve the list of news sources
    news_sources = get_news_sources()
    print(""Available News Sources:"")
    for source in news_sources:
        print(f""\nSource Title: {source['title']}"")
        print(f""Website: {source['web']}"")

    # Step 2: Search for a specific movie or TV show (e.g., ""Breaking Bad"")
    search_results = search_movies_or_tv(""breaking bad"")
    print(""\nMovie or TV Show Search Results:"")
    for item in search_results:
        print(f""\nTitle: {item['title']}"")
        print(f""Original Title: {item['original_title']}"")
        print(f""Release Date: {item['release_date']}"")
        print(f""Genres: {', '.join(item['genres'])}"")
        print(f""Overview: {item['overview']}"")
        print(f""Rating: {item['vote_average']} / 10"")
        print(f""Number of Votes: {item['vote_count']}"")
        print(f""Poster: {item['poster_path']}"")
        print(f""Backdrop: {item['backdrop_path']}"")
        print(f""YouTube Trailer: {item['youtube_trailer']}"")

        print(""Available on:"")
        for source in item.get(""sources"", []):
            display_name = source.get(""display_name"", ""Unknown Platform"")
            access_type = source.get(""type"", ""Unknown Access Type"")
            link = source.get(""link"", ""No link available"")
            print(f""  - {display_name} ({access_type}): {link}"")
",Fail,-1,-1,27,74709
"I need a summary of the latest news articles across different topics, including their titles, sources, publication dates, and links. Also, please provide a list of trending news topics, each with the most popular articles related to the topic. Additionally, I want a list of movies or TV shows based on a specific keyword or title, along with details like release date, rating, genre, trailer link, and streaming platforms.","API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

import requests

def fetch_latest_news():
    url = ""https://biztoc.p.rapidapi.com/news/latest""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""biztoc.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def fetch_trending_news_topics():
    url = ""https://biztoc.p.rapidapi.com/news/topics""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""biztoc.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

def search_movie_or_tv_show(query):
    url = ""https://movies-api14.p.rapidapi.com/search""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""movies-api14.p.rapidapi.com""
    }
    querystring = {""query"": query}
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

if __name__ == '__main__':
    # Fetch the latest news articles
    latest_news = fetch_latest_news()
    print(""Latest News Articles:"")
    for news in latest_news[:3]:  # Limiting to the first three articles for brevity
        print(f""Title: {news['title']}\nSource: {news['domain']}\nPublished: {news['published']}\nURL: {news['url']}\n"")

    # Fetch trending news topics and related articles
    trending_topics = fetch_trending_news_topics()
    print(""\nTrending News Topics:"")
    for topic in trending_topics[:3]:  # Limiting to first three topics for brevity
        print(f""Topic: {', '.join(topic['topic'])}"")
        for story in topic['stories'][:3]:  # Limiting to first three stories per topic
            print(f""  Title: {story['title']}\n  Source: {story['domain']}\n  Published: {story['published']}\n  URL: {story['url']}\n"")

    # Search for movies or TV shows based on a keyword
    query = ""breaking bad""  # Replace with your desired search term
    movies_or_shows = search_movie_or_tv_show(query)
    print(""\nMovies or TV Shows Search Results:"")
    for content in movies_or_shows['contents'][:3]:  # Limiting to the first three results for brevity
        print(f""Title: {content['title']}\nRelease Date: {content['release_date']}\nRating: {content['vote_average']}\nGenres: {', '.join(content['genres'])}\nTrailer: {content.get('youtube_trailer', 'N/A')}\n"")
",Fail,-1,-1,33,109461
"I need to gather the latest news articles across various sections, such as general news, technology, culture, and sports, from global and BBC-exclusive sources, as well as social media in Chinese. The summary should include titles, brief descriptions, image links, and URLs where available. Additionally, I want a selection of trending news in Chinese from popular social media platforms, with details on publication times and direct links","API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

import requests

def fetch_news(lang=""en""):
    url = ""https://bbc-api2.p.rapidapi.com/news""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""bbc-api2.p.rapidapi.com""
    }
    params = {""lang"": lang}
    response = requests.get(url, headers=headers, params=params)
    return response.json()

def fetch_latest_news(lang=""en""):
    url = ""https://bbc-api2.p.rapidapi.com/latest""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""bbc-api2.p.rapidapi.com""
    }
    params = {""lang"": lang}
    response = requests.get(url, headers=headers, params=params)
    return response.json()

def fetch_social_media_news_chinese():
    url = ""https://onelike1.p.rapidapi.com/servicech/news""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""onelike1.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    return response.json()

if __name__ == '__main__':
    # Fetch a brief of categorized news articles
    categorized_news = fetch_news(lang=""en"")
    for section in [""Latest"", ""Only from the BBC"", ""Technology""]:
        articles = categorized_news.get(section, [])
        for article in articles[:1]:  # Limit to the first article
            print(f""{section} Article:\nTitle: {article['title']}\nURL: {article['news_link']}\n"")

    # Fetch a single latest news article
    latest_news = fetch_latest_news(lang=""en"")
    if 'Latest' in latest_news:
        article = latest_news['Latest'][0]
        print(f""Latest News:\nTitle: {article['title']}\nURL: {article['news_link']}\n"")

    # Fetch a single trending news from Chinese social media
    social_media_news = fetch_social_media_news_chinese()
    if 'socialmedias' in social_media_news:
        first_news = social_media_news['socialmedias'][0]['news'][0]
        print(f""Chinese Social Media News:\nTitle: {first_news['title']}\nURL: {first_news['url']}\n"")
",Fail,-1,-1,32,96768
"I need a brief overview of a specific joke by its unique identifier, including the setup and punchline, as well as essential details about the planets in our Solar System, with a summary of each planetâ€™s attributes like volume, mass, a brief description, and a link to an image.","API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

import requests

def fetch_joke_by_id(joke_id=10):
    url = f""https://manatee-jokes.p.rapidapi.com/manatees/{joke_id}""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""manatee-jokes.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    joke_data = response.json()
    print(""Joke Details:"")
    print(f""Setup: {joke_data['setup']}"")
    print(f""Punchline: {joke_data['punchline']}\n"")
    return joke_data

def fetch_all_planets():
    url = ""https://planets-info-by-newbapi.p.rapidapi.com/api/v1/planets/""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""planets-info-by-newbapi.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    planets_data = response.json()
    print(""Planet Information:"")
    for planet in planets_data[:3]:  # Limiting to the first three planets
        print(f""Name: {planet['name']}"")
        print(f""Description: {planet['description']}"")
        print(f""Volume: {planet['basicDetails']['volume']}"")
        print(f""Mass: {planet['basicDetails']['mass']}"")
        print(f""Image: {planet['imgSrc']['img']}\n"")
    return planets_data

if __name__ == '__main__':
    # Fetch and display a specific joke
    fetch_joke_by_id(joke_id=10)

    # Fetch and display information about the planets
    fetch_all_planets()
",Fail,-1,-1,29,81548
"I need information on a specific plant, including its scientific name, habitat, growth details, and visual characteristics. Additionally, I want details about three movies or TV shows that match a specific keyword, providing information like title, genre, rating, release date, and streaming sources.","API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

import requests

def fetch_plant_details():
    url = ""https://daily-knowledge.p.rapidapi.com/plants-de.json""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""daily-knowledge.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    plant_data = response.json()
    print(""Plant Information:"")
    print(f""Name: {plant_data['name']}"")
    print(f""Scientific Name: {plant_data['basicData']['scName']}"")
    print(f""Habitat: {plant_data['basicData']['home']}"")
    print(f""Climate: {plant_data['basicData']['climate']}"")
    print(f""Appearance: {plant_data['descrData']['infoAppear']}\n"")
    return plant_data

def fetch_movie_details(query=""inception""):
    url = ""https://movies-api14.p.rapidapi.com/search""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""movies-api14.p.rapidapi.com""
    }
    querystring = {""query"": query}
    response = requests.get(url, headers=headers, params=querystring)
    movies_data = response.json()
    print(""Movies Information:"")
    for movie in movies_data['contents'][:3]:  # Limiting to the first three results
        print(f""Title: {movie['title']}"")
        print(f""Genre: {', '.join(movie['genres'])}"")
        print(f""Rating: {movie['vote_average']}"")
        print(f""Release Date: {movie['release_date']}"")
        print(f""Streaming Sources: {[source['display_name'] for source in movie.get('sources', [])]}\n"")
    return movies_data

if __name__ == '__main__':
    # Fetch and display details about a specific plant
    fetch_plant_details()

    # Fetch and display details about movies matching a specific keyword
    fetch_movie_details(query=""inception"")
","Plant Information:
Name: Alpen-Fettkraut
Scientific Name: PinguÃ­cula alpÃ­na
Habitat: 17
Climate: 4
Appearance: Die Wurzeln der Pflanze sind lediglich 1 bis 2 cm lang. Die fleischigen LaubblÃ¤tter bilden am Boden eine Rosette. Die BlattoberflÃ¤che ist klebrig, wodurch Insekten gefangen werden. Die Verdauung der Insekten erfolgt mittels eines Enzyms, das aus DrÃ¼sen an der BlattoberflÃ¤che ausgeschieden wird. Die BlÃ¤tter kÃ¶nnen sich bis zur Mitte einrollen.

Movies Information:
Title: Inception
Genre: Action & Adventure, Sci-Fi & Fantasy, Action & Adventure
Rating: 8.362
Release Date: 2010-07-15
Streaming Sources: ['HBO Max', 'Netflix', 'Vudu', 'Apple TV']

Title: The Crack: Inception
Genre: Drama, Thriller
Rating: 6.897
Release Date: 2019-10-04
Streaming Sources: []
",0,0,12,42348
"I need to calculate the power in watts for both a single-phase and a three-phase system. For the single-phase calculation, use a current of 13 amperes, with the default voltage (230 volts) and power factor (0.95). For the three-phase calculation, also use a current of 13 amperes, with the default three-phase voltage (400 volts) and power factor (0.95).","API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

import requests

def convert_single_phase_current_to_power(current=13, voltage=230, powerfactor=0.95):
    url = ""https://electrical-units.p.rapidapi.com/current_to_power/single_phase""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""electrical-units.p.rapidapi.com""
    }
    querystring = {""current"": str(current), ""voltage"": str(voltage), ""powerfactor"": str(powerfactor)}
    response = requests.get(url, headers=headers, params=querystring)
    single_phase_power = response.json()
    print(""Single-Phase Power Calculation:"")
    print(f""Power: {single_phase_power['power']} watts\n"")
    return single_phase_power

def convert_three_phase_current_to_power(current=13, voltage=400, powerfactor=0.95):
    url = ""https://electrical-units.p.rapidapi.com/current_to_power/three_phase""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""electrical-units.p.rapidapi.com""
    }
    querystring = {""current"": str(current), ""voltage"": str(voltage), ""powerfactor"": str(powerfactor)}
    response = requests.get(url, headers=headers, params=querystring)
    three_phase_power = response.json()
    print(""Three-Phase Power Calculation:"")
    print(f""Power: {three_phase_power['power']} watts\n"")
    return three_phase_power

if __name__ == '__main__':
    # Calculate and display power for single-phase system
    convert_single_phase_current_to_power()

    # Calculate and display power for three-phase system
    convert_three_phase_current_to_power()
","Single-Phase Power Calculation:
Power: 2840.5 watts

Three-Phase Power Calculation:
Power: 8556.33 watts
",1,1,18,55134
"I need detailed information about a chemical element, including its atomic number, atomic mass, electron configuration, and year of discovery. Additionally, I want to understand the objectives and targets related to the ""Life on Land"" sustainable development goal, with actionable tips for promoting sustainable land use and biodiversity conservation.","API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

import requests

def get_chemical_element_details():
    url = ""https://daily-knowledge.p.rapidapi.com/chemical-elements-en.json""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""daily-knowledge.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    element_data = response.json()

    print(""Chemical Element Details:"")
    print(f""Name: {element_data['name']} ({element_data['symbol']})"")
    print(f""Atomic Number: {element_data['atomicNumber']}"")
    print(f""Atomic Mass: {element_data['atomicMass']}"")
    print(f""Electrons per Shell: {element_data['electronsPerShell']}"")
    print(f""Discovery Date: {element_data['discoveryDate']}\n"")
    return element_data

def get_sustainable_development_goal():
    url = ""https://daily-knowledge.p.rapidapi.com/sustainable-development-goals-en.json""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""daily-knowledge.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    goal_data = response.json()

    print(""Sustainable Development Goal - Life on Land:"")
    print(f""Title: {goal_data['title']}"")
    print(f""Introduction: {goal_data['introduction']}\n"")
    print(""Key Targets:"")
    for target in goal_data['targets'][:3]:  # Limiting to the first three targets
        print(f"" - {target['title']}: {target['body']}"")

    print(""\nActionable Tips:"")
    for tip in goal_data['tips'][:3]:  # Limiting to the first three tips
        print(f"" - {tip}"")
    return goal_data

if __name__ == '__main__':
    # Retrieve and display chemical element details
    get_chemical_element_details()

    # Retrieve and display sustainable development goal information
    get_sustainable_development_goal()
",Fail,-1,-1,32,88032
"I need the current weather and a 3-day forecast for a specified location. Please provide detailed information on the current conditions, including temperature, wind speed, and humidity, along with a 3-day forecast of daily temperature ranges, precipitation chances, and wind speeds. Include hourly details for each day, and ensure the data includes sunrise, sunset, and moon phase information.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_realtime_weather(location):
    url = ""https://weatherapi-com.p.rapidapi.com/current.json""
    querystring = {""q"": location}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_weather_forecast(location):
    url = ""https://weatherapi-com.p.rapidapi.com/forecast.json""
    querystring = {""q"": location, ""days"": ""3""}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

if __name__ == '__main__':
    # Specify the location, for example, New York
    location = ""New York""

    # Fetch current weather data
    current_weather = get_realtime_weather(location)
    print(""Current Weather Data:"", current_weather)

    # Fetch 3-day weather forecast
    weather_forecast = get_weather_forecast(location)
    print(""3-Day Weather Forecast:"", weather_forecast)
",Fail,-1,-1,33,98604
"I need geolocation and timezone details based on a specific IP address. This includes detailed information about the location, such as the continent, country, city, latitude, and longitude, as well as the timezone, local time, and other related data. Additionally, I need the time zone information verified and detailed through the timezone API.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_ip_geolocation(ip_address):
    url = ""https://weatherapi-com.p.rapidapi.com/ip.json""
    querystring = {""q"": ip_address}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_timezone(location):
    url = ""https://weatherapi-com.p.rapidapi.com/timezone.json""
    querystring = {""q"": location}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

if __name__ == '__main__':
    # Specify the IP address
    ip_address = ""8.8.8.8""

    # Fetch geolocation data based on IP address
    geolocation_data = get_ip_geolocation(ip_address)
    print(""Geolocation Data:"", geolocation_data)

    # Fetch timezone information based on location details from geolocation response
    location = f""{geolocation_data['lat']},{geolocation_data['lon']}""
    timezone_data = get_timezone(location)
    print(""Timezone Information:"", timezone_data)
",Fail,-1,-1,27,81297
"I need the current weather and astronomical data for a specified location, including temperature, wind speed, humidity, sunrise, sunset, moonrise, moonset, and moon phase. Ensure the weather and astronomical details are specific to this location and include as much information as possible for both conditions in a single retrieval.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_realtime_weather(location):
    url = ""https://weatherapi-com.p.rapidapi.com/current.json""
    querystring = {""q"": location}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_astronomy_data(location, date=None):
    url = ""https://weatherapi-com.p.rapidapi.com/astronomy.json""
    querystring = {""q"": location}
    if date:
        querystring[""dt""] = date
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

if __name__ == '__main__':
    # Specify the location (e.g., ""New York"")
    location = ""New York""
    date = ""2024-10-27""  # Optional: specify a date for astronomy data if needed

    # Fetch current weather data
    current_weather = get_realtime_weather(location)
    print(""Current Weather Data:"", current_weather)

    # Fetch astronomical data for the specified date
    astronomy_data = get_astronomy_data(location, date)
    print(""Astronomy Data:"", astronomy_data)
","Current Weather Data: {'location': {'name': 'New York', 'region': 'New York', 'country': 'United States of America', 'lat': 40.7142, 'lon': -74.0064, 'tz_id': 'America/New_York', 'localtime_epoch': 1729965837, 'localtime': '2024-10-26 14:03'}, 'current': {'last_updated_epoch': 1729965600, 'last_updated': '2024-10-26 14:00', 'temp_c': 17.2, 'temp_f': 63.0, 'is_day': 1, 'condition': {'text': 'Sunny', 'icon': '//cdn.weatherapi.com/weather/64x64/day/113.png', 'code': 1000}, 'wind_mph': 15.4, 'wind_kph': 24.8, 'wind_degree': 316, 'wind_dir': 'NW', 'pressure_mb': 1020.0, 'pressure_in': 30.12, 'precip_mm': 0.0, 'precip_in': 0.0, 'humidity': 29, 'cloud': 0, 'feelslike_c': 17.2, 'feelslike_f': 63.0, 'windchill_c': 18.3, 'windchill_f': 64.9, 'heatindex_c': 18.3, 'heatindex_f': 64.9, 'dewpoint_c': 9.7, 'dewpoint_f': 49.4, 'vis_km': 16.0, 'vis_miles': 9.0, 'uv': 5.0, 'gust_mph': 18.9, 'gust_kph': 30.3}}
Astronomy Data: {'location': {'name': 'New York', 'region': 'New York', 'country': 'United States of America', 'lat': 40.7142, 'lon': -74.0064, 'tz_id': 'America/New_York', 'localtime_epoch': 1729965835, 'localtime': '2024-10-26 14:03'}, 'astronomy': {'astro': {'sunrise': '07:21 AM', 'sunset': '05:58 PM', 'moonrise': '02:33 AM', 'moonset': '04:01 PM', 'moon_phase': 'Waning Crescent', 'moon_illumination': 25, 'is_moon_up': 0, 'is_sun_up': 0}}}
",1,1,18,53622
"I need the current weather, timezone, and historical weather data for a specific location. Please include current temperature, wind speed, and humidity, timezone details with local time, and historical weather data for a specific date, including daily high and low temperatures, precipitation, and significant events such as sunrise, sunset, and moon phase.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_realtime_weather(location):
    url = ""https://weatherapi-com.p.rapidapi.com/current.json""
    querystring = {""q"": location}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_timezone_info(location):
    url = ""https://weatherapi-com.p.rapidapi.com/timezone.json""
    querystring = {""q"": location}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_historical_weather(location, date):
    url = ""https://weatherapi-com.p.rapidapi.com/history.json""
    querystring = {""q"": location, ""dt"": date}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

if __name__ == '__main__':
    # Specify the location and date for historical data
    location = ""New York""
    date = ""2023-10-01""  # Example historical date

    # Fetch current weather data
    current_weather = get_realtime_weather(location)
    print(""Current Weather Data:"", current_weather)

    # Fetch timezone information
    timezone_info = get_timezone_info(location)
    print(""Timezone Information:"", timezone_info)

    # Fetch historical weather data for the specified date
    historical_weather = get_historical_weather(location, date)
    print(""Historical Weather Data:"", historical_weather)
",Fail,-1,-1,29,79460
"I need to get the forecasted weather, geolocation based on an IP address, and autocomplete suggestions for a location name. Specifically, I want the 3-day weather forecast for a specified location with hourly details, geolocation data for a given IP, and detailed autocomplete information (including geographical coordinates and unique identifiers) for a specified place.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_forecast_weather(location, days=3):
    url = ""https://weatherapi-com.p.rapidapi.com/forecast.json""
    querystring = {""q"": location, ""days"": str(days)}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_ip_geolocation(ip_address):
    url = ""https://weatherapi-com.p.rapidapi.com/ip.json""
    querystring = {""q"": ip_address}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_location_autocomplete(query):
    url = ""https://weatherapi-com.p.rapidapi.com/search.json""
    querystring = {""q"": query}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

if __name__ == '__main__':
    # Specify inputs for each function
    location = ""New York""
    ip_address = ""8.8.8.8""
    autocomplete_query = ""Los Angeles""

    # Fetch the 3-day weather forecast
    forecast_weather = get_forecast_weather(location)
    print(""3-Day Weather Forecast:"", forecast_weather)

    # Fetch geolocation data based on IP address
    ip_geolocation = get_ip_geolocation(ip_address)
    print(""Geolocation Data:"", ip_geolocation)

    # Fetch autocomplete suggestions for location
    autocomplete_data = get_location_autocomplete(autocomplete_query)
    print(""Location Autocomplete Suggestions:"", autocomplete_data)
",Fail,-1,-1,29,72500
"I need detailed weather data, including both historical hourly and monthly records, for a specified weather station. Additionally, I want to retrieve location details and identifiers for a city using an autocomplete search. Please provide hourly historical data with details such as temperature, humidity, and wind, and monthly data including average, minimum, and maximum temperatures, along with precipitation and sunshine duration. For location details, retrieve the city name, region, country, and coordinates.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_location_autocomplete(query):
    url = ""https://weatherapi-com.p.rapidapi.com/search.json""
    querystring = {""q"": query}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""weatherapi-com.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_hourly_station_data(station_id, start_date, end_date):
    url = ""https://meteostat.p.rapidapi.com/stations/hourly""
    querystring = {""station"": station_id, ""start"": start_date, ""end"": end_date, ""tz"": ""UTC""}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""meteostat.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_monthly_station_data(station_id, start_date, end_date):
    url = ""https://meteostat.p.rapidapi.com/stations/monthly""
    querystring = {""station"": station_id, ""start"": start_date, ""end"": end_date}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""meteostat.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

if __name__ == '__main__':
    # Example inputs
    location_query = ""Los Angeles""
    station_id = ""10637""  # Example station ID
    start_date = ""2023-01-01""
    end_date = ""2023-12-31""

    # Fetch location details through autocomplete
    location_data = get_location_autocomplete(location_query)
    print(""Location Autocomplete Data:"", location_data)

    # Fetch hourly historical data for the station
    hourly_data = get_hourly_station_data(station_id, start_date, start_date)  # One day of hourly data
    print(""Hourly Station Data:"", hourly_data)

    # Fetch monthly historical data for the station
    monthly_data = get_monthly_station_data(station_id, start_date, end_date)
    print(""Monthly Station Data:"", monthly_data)
","Location Autocomplete Data: [{'id': 2548773, 'name': 'Los Angeles', 'region': 'California', 'country': 'United States of America', 'lat': 34.05, 'lon': -118.24, 'url': 'los-angeles-california-united-states-of-america'}, {'id': 371191, 'name': 'Los Angeles', 'region': 'Bio-Bio', 'country': 'Chile', 'lat': -37.47, 'lon': -72.35, 'url': 'los-angeles-bio-bio-chile'}, {'id': 1849673, 'name': 'Los Angeles', 'region': 'Butuan', 'country': 'Philippines', 'lat': 9.01, 'lon': 125.61, 'url': 'los-angeles-butuan-philippines'}, {'id': 3265054, 'name': 'Los Angeles', 'region': 'Guanajuato', 'country': 'Mexico', 'lat': 20.55, 'lon': -100.94, 'url': 'los-angeles-guanajuato-mexico'}, {'id': 3217043, 'name': 'Los Angeles', 'region': 'Oaxaca', 'country': 'Mexico', 'lat': 17.16, 'lon': -97.8, 'url': 'los-angeles-oaxaca-mexico'}]
Hourly Station Data: {'meta': {'generated': '2024-10-26 18:10:22'}, 'data': [{'time': '2023-01-01 00:00:00', 'temp': 10.9, 'dwpt': 6.8, 'rhum': 76.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 220.0, 'wspd': 10.1, 'wpgt': 17.0, 'pres': 1017.4, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 01:00:00', 'temp': 12.4, 'dwpt': 7.7, 'rhum': 73.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 230.0, 'wspd': 15.5, 'wpgt': 22.0, 'pres': 1017.2, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 02:00:00', 'temp': 11.9, 'dwpt': 7.2, 'rhum': 73.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 190.0, 'wspd': 10.8, 'wpgt': 19.0, 'pres': 1017.2, 'tsun': 0, 'coco': 1}, {'time': '2023-01-01 03:00:00', 'temp': 11.2, 'dwpt': 7.5, 'rhum': 78.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 200.0, 'wspd': 5.4, 'wpgt': 10.0, 'pres': 1017.8, 'tsun': 0, 'coco': 2}, {'time': '2023-01-01 04:00:00', 'temp': 11.7, 'dwpt': 7.6, 'rhum': 76.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 110.0, 'wspd': 4.0, 'wpgt': 9.0, 'pres': 1017.9, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 05:00:00', 'temp': 8.8, 'dwpt': 6.9, 'rhum': 88.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 200.0, 'wspd': 8.6, 'wpgt': 17.0, 'pres': 1018.1, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 06:00:00', 'temp': 11.1, 'dwpt': 7.6, 'rhum': 79.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 200.0, 'wspd': 14.0, 'wpgt': 33.0, 'pres': 1018.5, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 07:00:00', 'temp': 10.9, 'dwpt': 7.2, 'rhum': 78.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 210.0, 'wspd': 12.6, 'wpgt': 24.0, 'pres': 1019.0, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 08:00:00', 'temp': 11.5, 'dwpt': 7.2, 'rhum': 75.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 220.0, 'wspd': 15.8, 'wpgt': 29.0, 'pres': 1019.7, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 09:00:00', 'temp': 11.1, 'dwpt': 7.4, 'rhum': 78.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 220.0, 'wspd': 15.1, 'wpgt': 32.0, 'pres': 1020.0, 'tsun': 5, 'coco': 4}, {'time': '2023-01-01 10:00:00', 'temp': 13.3, 'dwpt': 7.9, 'rhum': 70.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 220.0, 'wspd': 14.8, 'wpgt': 26.0, 'pres': 1020.3, 'tsun': 1, 'coco': 4}, {'time': '2023-01-01 11:00:00', 'temp': 14.4, 'dwpt': 7.7, 'rhum': 64.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 220.0, 'wspd': 20.5, 'wpgt': 31.0, 'pres': 1020.3, 'tsun': 4, 'coco': 4}, {'time': '2023-01-01 12:00:00', 'temp': 14.8, 'dwpt': 8.3, 'rhum': 65.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 220.0, 'wspd': 22.3, 'wpgt': 35.0, 'pres': 1019.4, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 13:00:00', 'temp': 15.0, 'dwpt': 7.8, 'rhum': 62.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 220.0, 'wspd': 22.0, 'wpgt': 37.0, 'pres': 1019.5, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 14:00:00', 'temp': 14.7, 'dwpt': 7.7, 'rhum': 63.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 220.0, 'wspd': 20.2, 'wpgt': 31.0, 'pres': 1019.6, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 15:00:00', 'temp': 14.2, 'dwpt': 8.4, 'rhum': 68.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 210.0, 'wspd': 15.1, 'wpgt': 26.0, 'pres': 1019.7, 'tsun': 0, 'coco': 7}, {'time': '2023-01-01 16:00:00', 'temp': 14.3, 'dwpt': 7.8, 'rhum': 65.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 210.0, 'wspd': 11.2, 'wpgt': 23.0, 'pres': 1019.4, 'tsun': 0, 'coco': 7}, {'time': '2023-01-01 17:00:00', 'temp': 12.1, 'dwpt': 8.2, 'rhum': 77.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 190.0, 'wspd': 6.1, 'wpgt': 14.0, 'pres': 1019.4, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 18:00:00', 'temp': 11.5, 'dwpt': 8.2, 'rhum': 80.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 160.0, 'wspd': 6.8, 'wpgt': 14.0, 'pres': 1018.8, 'tsun': 0, 'coco': 8}, {'time': '2023-01-01 19:00:00', 'temp': 11.2, 'dwpt': 8.2, 'rhum': 82.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 140.0, 'wspd': 6.5, 'wpgt': 15.0, 'pres': 1018.2, 'tsun': 0, 'coco': 8}, {'time': '2023-01-01 20:00:00', 'temp': 10.7, 'dwpt': 8.1, 'rhum': 84.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 90.0, 'wspd': 2.5, 'wpgt': 11.0, 'pres': 1017.7, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 21:00:00', 'temp': 10.3, 'dwpt': 8.2, 'rhum': 87.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 310.0, 'wspd': 4.3, 'wpgt': 15.0, 'pres': 1017.4, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 22:00:00', 'temp': 10.3, 'dwpt': 7.9, 'rhum': 85.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 210.0, 'wspd': 4.7, 'wpgt': 14.0, 'pres': 1017.7, 'tsun': 0, 'coco': 4}, {'time': '2023-01-01 23:00:00', 'temp': 9.2, 'dwpt': 7.5, 'rhum': 89.0, 'prcp': 0.0, 'snow': 0.0, 'wdir': 160.0, 'wspd': 2.9, 'wpgt': 5.0, 'pres': 1017.9, 'tsun': 0, 'coco': 4}]}
Monthly Station Data: {'meta': {'generated': '2024-10-26 18:10:23'}, 'data': [{'date': '2023-01-01', 'tavg': 5.0, 'tmin': 2.5, 'tmax': 7.1, 'prcp': 62.9, 'wspd': 16.8, 'pres': 1018.7, 'tsun': 2052}, {'date': '2023-02-01', 'tavg': 4.9, 'tmin': 1.3, 'tmax': 8.4, 'prcp': 9.3, 'wspd': 13.7, 'pres': 1028.9, 'tsun': 5808}, {'date': '2023-03-01', 'tavg': 7.6, 'tmin': 3.1, 'tmax': 12.2, 'prcp': 98.1, 'wspd': 16.1, 'pres': 1012.6, 'tsun': 6792}, {'date': '2023-04-01', 'tavg': 9.6, 'tmin': 4.7, 'tmax': 14.3, 'prcp': 43.9, 'wspd': 12.9, 'pres': 1017.3, 'tsun': 9372}, {'date': '2023-05-01', 'tavg': 15.8, 'tmin': 10.2, 'tmax': 21.2, 'prcp': 53.3, 'wspd': 13.2, 'pres': 1019.9, 'tsun': 16212}, {'date': '2023-06-01', 'tavg': 21.7, 'tmin': 14.5, 'tmax': 28.2, 'prcp': 37.6, 'wspd': 11.5, 'pres': 1017.1, 'tsun': 20304}, {'date': '2023-07-01', 'tavg': 20.8, 'tmin': 14.2, 'tmax': 26.7, 'prcp': 107.5, 'wspd': 14.6, 'pres': 1014.4, 'tsun': 13914}, {'date': '2023-08-01', 'tavg': 19.7, 'tmin': 14.8, 'tmax': 25.2, 'prcp': 137.2, 'wspd': 11.8, 'pres': 1015.0, 'tsun': 11076}, {'date': '2023-09-01', 'tavg': 18.8, 'tmin': 12.7, 'tmax': 25.5, 'prcp': 20.8, 'wspd': 9.5, 'pres': 1018.5, 'tsun': 15804}, {'date': '2023-10-01', 'tavg': 12.9, 'tmin': 8.0, 'tmax': 18.0, 'prcp': 80.8, 'wspd': 11.9, 'pres': 1012.9, 'tsun': 6894}, {'date': '2023-11-01', 'tavg': 7.3, 'tmin': 4.4, 'tmax': 10.1, 'prcp': 91.1, 'wspd': 17.0, 'pres': 1009.1, 'tsun': 1908}, {'date': '2023-12-01', 'tavg': 5.4, 'tmin': 2.6, 'tmax': 7.6, 'prcp': 66.2, 'wspd': 17.2, 'pres': 1016.4, 'tsun': 1548}]}
",0,0,11,31075
"I need detailed address information based on a query search, including postal codes, street names, and geographical coordinates. Additionally, I want detailed album information, covering artists, tracks, and release details for specified album IDs. Please provide address details with as many results as possible (up to a limit of 10) and detailed album information including tracks and artists.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_address_details(query, limit=10):
    url = ""https://uk-postcode.p.rapidapi.com/search""
    querystring = {""q"": query, ""limit"": str(limit)}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""uk-postcode.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

def get_album_details(album_ids):
    url = ""https://spotify23.p.rapidapi.com/albums/""
    querystring = {""ids"": album_ids}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""spotify23.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    return response.json()

if __name__ == '__main__':
    # Specify inputs for each function
    address_query = ""Downing Street London""
    album_ids = ""3IBcauSj5M2A6lTeffJzdv,7Ey4PD4MYsKc5I2DolUwbH""  # Example album IDs

    # Fetch address details
    address_data = get_address_details(address_query)
    print(""Address Details:"", address_data)

    # Fetch album details
    album_data = get_album_details(album_ids)
    print(""Album Details:"", album_data)
","Address Details: {'success': True, 'results': [{'postCode': 'SW1A 2AA', 'postCodeTrimmed': 'SW1A2AA', 'streetName': 'Downing Street', 'longitude': -0.127695, 'latitude': 51.50354, 'plusCode': '9C3XGV3C+CW', 'region': 'Greater London', 'locality': 'London', 'area': 'England', 'numUPRNs': 2}, {'postCode': 'SW1A 2AB', 'postCodeTrimmed': 'SW1A2AB', 'streetName': 'Downing Street', 'longitude': -0.127874, 'latitude': 51.503399, 'plusCode': '9C3XGV3C+9V', 'region': 'Greater London', 'locality': 'London', 'area': 'England', 'numUPRNs': 1}, {'postCode': 'SW1A 2AD', 'postCodeTrimmed': 'SW1A2AD', 'streetName': 'Downing Street', 'longitude': -0.128105, 'latitude': 51.503384, 'plusCode': '9C3XGV3C+9Q', 'region': 'Greater London', 'locality': 'London', 'area': 'England', 'numUPRNs': 2}, {'postCode': 'SW1A 2AG', 'postCodeTrimmed': 'SW1A2AG', 'streetName': 'Downing Street', 'longitude': -0.126639, 'latitude': 51.503649, 'plusCode': '9C3XGV3F+F8', 'region': 'Greater London', 'locality': 'London', 'area': 'England', 'numUPRNs': 3}, {'postCode': 'SW1A 2AL', 'postCodeTrimmed': 'SW1A2AL', 'streetName': 'Downing Street', 'longitude': -0.126639, 'latitude': 51.503649, 'plusCode': '9C3XGV3F+F8', 'region': 'Greater London', 'locality': 'London', 'area': 'England', 'numUPRNs': 1}]}
Album Details: {'albums': [{'album_type': 'album', 'total_tracks': 10, 'is_playable': True, 'external_urls': {'spotify': 'https://open.spotify.com/album/3IBcauSj5M2A6lTeffJzdv'}, 'id': '3IBcauSj5M2A6lTeffJzdv', 'images': [{'url': 'https://i.scdn.co/image/ab67616d0000b27367c738a703dc979f5c3c52ef', 'height': 640, 'width': 640}, {'url': 'https://i.scdn.co/image/ab67616d00001e0267c738a703dc979f5c3c52ef', 'height': 300, 'width': 300}, {'url': 'https://i.scdn.co/image/ab67616d0000485167c738a703dc979f5c3c52ef', 'height': 64, 'width': 64}], 'name': 'Kan', 'release_date': '2021-03-19', 'release_date_precision': 'day', 'type': 'album', 'uri': 'spotify:album:3IBcauSj5M2A6lTeffJzdv', 'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/51DevdOxIJin6DB1FXJpD1'}, 'id': '51DevdOxIJin6DB1FXJpD1', 'name': 'UZI', 'type': 'artist', 'uri': 'spotify:artist:51DevdOxIJin6DB1FXJpD1'}], 'tracks': {'limit': 50, 'next': None, 'offset': 0, 'previous': None, 'total': 10, 'items': [{'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/51DevdOxIJin6DB1FXJpD1'}, 'id': '51DevdOxIJin6DB1FXJpD1', 'name': 'UZI', 'type': 'artist', 'uri': 'spotify:artist:51DevdOxIJin6DB1FXJpD1'}], 'disc_number': 1, 'duration_ms': 211016, 'explicit': True, 'external_urls': {'spotify': 'https://open.spotify.com/track/5jvhTc0g18kwYQNUJM5C4e'}, 'id': '5jvhTc0g18kwYQNUJM5C4e', 'is_playable': True, 'name': 'Makina', 'preview_url': 'https://p.scdn.co/mp3-preview/27ab6f738e8bbb6be4225f2e10886883ef760381?cid=d8a5ed958d274c2e8ee717e6a4b0971d', 'track_number': 1, 'type': 'track', 'uri': 'spotify:track:5jvhTc0g18kwYQNUJM5C4e', 'is_local': False}, {'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/51DevdOxIJin6DB1FXJpD1'}, 'id': '51DevdOxIJin6DB1FXJpD1', 'name': 'UZI', 'type': 'artist', 'uri': 'spotify:artist:51DevdOxIJin6DB1FXJpD1'}], 'disc_number': 1, 'duration_ms': 185458, 'explicit': True, 'external_urls': {'spotify': 'https://open.spotify.com/track/72t3CRd8YEFrlc3x0OVaob'}, 'id': '72t3CRd8YEFrlc3x0OVaob', 'is_playable': True, 'name': 'Umrumda DeÄŸil', 'preview_url': 'https://p.scdn.co/mp3-preview/ca2df079cdae1386e8b2899951cadf93c335018c?cid=d8a5ed958d274c2e8ee717e6a4b0971d', 'track_number': 2, 'type': 'track', 'uri': 'spotify:track:72t3CRd8YEFrlc3x0OVaob', 'is_local': False}, {'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/51DevdOxIJin6DB1FXJpD1'}, 'id': '51DevdOxIJin6DB1FXJpD1', 'name': 'UZI', 'type': 'artist', 'uri': 'spotify:artist:51DevdOxIJin6DB1FXJpD1'}, {'external_urls': {'spotify': 'https://open.spotify.com/artist/3BVPc9s4JXzM6O1InlLxED'}, 'id': '3BVPc9s4JXzM6O1InlLxED', 'name': 'Mavi', 'type': 'artist', 'uri': 'spotify:artist:3BVPc9s4JXzM6O1InlLxED'}], 'disc_number': 1, 'duration_ms': 200000, 'explicit': False, 'external_urls': {'spotify': 'https://open.spotify.com/track/5TkQatzJqKafPgHQerZ0dL'}, 'id': '5TkQatzJqKafPgHQerZ0dL', 'is_playable': True, 'name': 'Gecenin IÃ§ine Gir', 'preview_url': 'https://p.scdn.co/mp3-preview/dad0bc5179800da1fe3d5a7c8ca5d9c8563b786b?cid=d8a5ed958d274c2e8ee717e6a4b0971d', 'track_number': 3, 'type': 'track', 'uri': 'spotify:track:5TkQatzJqKafPgHQerZ0dL', 'is_local': False}, {'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/51DevdOxIJin6DB1FXJpD1'}, 'id': '51DevdOxIJin6DB1FXJpD1', 'name': 'UZI', 'type': 'artist', 'uri': 'spotify:artist:51DevdOxIJin6DB1FXJpD1'}], 'disc_number': 1, 'duration_ms': 243205, 'explicit': True, 'external_urls': {'spotify': 'https://open.spotify.com/track/6IW5ocUH5DRWagxkLTlbUS'}, 'id': '6IW5ocUH5DRWagxkLTlbUS', 'is_playable': True, 'name': 'Nedenini Sorma', 'preview_url': 'https://p.scdn.co/mp3-preview/3b60dc4dd3c57960b5e5309f069709034d5e6888?cid=d8a5ed958d274c2e8ee717e6a4b0971d', 'track_number': 4, 'type': 'track', 'uri': 'spotify:track:6IW5ocUH5DRWagxkLTlbUS', 'is_local': False}, {'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/51DevdOxIJin6DB1FXJpD1'}, 'id': '51DevdOxIJin6DB1FXJpD1', 'name': 'UZI', 'type': 'artist', 'uri': 'spotify:artist:51DevdOxIJin6DB1FXJpD1'}, {'external_urls': {'spotify': 'https://open.spotify.com/artist/7GaMopkesD4KK9dNbgyO5D'}, 'id': '7GaMopkesD4KK9dNbgyO5D', 'name': 'Eko Fresh', 'type': 'artist', 'uri': 'spotify:artist:7GaMopkesD4KK9dNbgyO5D'}], 'disc_number': 1, 'duration_ms': 152301, 'explicit': True, 'external_urls': {'spotify': 'https://open.spotify.com/track/1ijjjMFlM3Pe8t3ykXBzxk'}, 'id': '1ijjjMFlM3Pe8t3ykXBzxk', 'is_playable': True, 'name': 'Mahalle', 'preview_url': 'https://p.scdn.co/mp3-preview/355ebff1c469cda6bc280f59cf4546c4a8c0e13a?cid=d8a5ed958d274c2e8ee717e6a4b0971d', 'track_number': 5, 'type': 'track', 'uri': 'spotify:track:1ijjjMFlM3Pe8t3ykXBzxk', 'is_local': False}, {'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/51DevdOxIJin6DB1FXJpD1'}, 'id': '51DevdOxIJin6DB1FXJpD1', 'name': 'UZI', 'type': 'artist', 'uri': 'spotify:artist:51DevdOxIJin6DB1FXJpD1'}], 'disc_number': 1, 'duration_ms': 171880, 'explicit': True, 'external_urls': {'spotify': 'https://open.spotify.com/track/4a1WLOoydq7u011UG9jjC9'}, 'id': '4a1WLOoydq7u011UG9jjC9', 'is_playable': True, 'name': 'Krvn', 'preview_url': 'https://p.scdn.co/mp3-preview/c595da41713f76fe3935b4314acda5b4b54a82c2?cid=d8a5ed958d274c2e8ee717e6a4b0971d', 'track_number': 6, 'type': 'track', 'uri': 'spotify:track:4a1WLOoydq7u011UG9jjC9', 'is_local': False}, {'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/51DevdOxIJin6DB1FXJpD1'}, 'id': '51DevdOxIJin6DB1FXJpD1', 'name': 'UZI', 'type': 'artist', 'uri': 'spotify:artist:51DevdOxIJin6DB1FXJpD1'}], 'disc_number': 1, 'duration_ms': 155010, 'explicit': True, 'external_urls': {'spotify': 'https://open.spotify.com/track/4hy4nY2PiYWx8qVXjpky3P'}, 'id': '4hy4nY2PiYWx8qVXjpky3P', 'is_playable': True, 'name': 'Vur', 'preview_url': 'https://p.scdn.co/mp3-preview/5b57e9e80e8dc443ad508fa794686e731a3e92ef?cid=d8a5ed958d274c2e8ee717e6a4b0971d', 'track_number': 7, 'type': 'track', 'uri': 'spotify:track:4hy4nY2PiYWx8qVXjpky3P', 'is_local': False}, {'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/51DevdOxIJin6DB1FXJpD1'}, 'id': '51DevdOxIJin6DB1FXJpD1', 'name': 'UZI', 'type': 'artist', 'uri': 'spotify:artist:51DevdOxIJin6DB1FXJpD1'}, {'external_urls': {'spotify': 'https://open.spotify.com/artist/6dOAGo4z0syiCjbnlh4VSO'}, 'id': '6dOAGo4z0syiCjbnlh4VSO', 'name': 'Critical', 'type': 'artist', 'uri': 'spotify:artist:6dOAGo4z0syiCjbnlh4VSO'}], 'disc_number': 1, 'duration_ms': 223608, 'explicit': True, 'external_urls': {'spotify': 'https://open.spotify.com/track/4PpYi6USHlY7OhOcDASnD3'}, 'id': '4PpYi6USHlY7OhOcDASnD3', 'is_playable': True, 'name': 'Davetiye', 'preview_url': 'https://p.scdn.co/mp3-preview/f162cdd28694f1289d689294a35937d51255e333?cid=d8a5ed958d274c2e8ee717e6a4b0971d', 'track_number': 8, 'type': 'track', 'uri': 'spotify:track:4PpYi6USHlY7OhOcDASnD3', 'is_local': False}, {'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/51DevdOxIJin6DB1FXJpD1'}, 'id': '51DevdOxIJin6DB1FXJpD1', 'name': 'UZI', 'type': 'artist', 'uri': 'spotify:artist:51DevdOxIJin6DB1FXJpD1'}, {'external_urls': {'spotify': 'https://open.spotify.com/artist/3R27mVPp04i87RNmvysZfY'}, 'id': '3R27mVPp04i87RNmvysZfY', 'name': 'Stap', 'type': 'artist', 'uri': 'spotify:artist:3R27mVPp04i87RNmvysZfY'}], 'disc_number': 1, 'duration_ms': 196682, 'explicit': True, 'external_urls': {'spotify': 'https://open.spotify.com/track/5gx3dMxQGJ1JDw5qHarRqp'}, 'id': '5gx3dMxQGJ1JDw5qHarRqp', 'is_playable': True, 'name': 'Elhamdulillah', 'preview_url': 'https://p.scdn.co/mp3-preview/4b926e5459e68e85007155518bd13dc42eff3b46?cid=d8a5ed958d274c2e8ee717e6a4b0971d', 'track_number': 9, 'type': 'track', 'uri': 'spotify:track:5gx3dMxQGJ1JDw5qHarRqp', 'is_local': False}, {'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/51DevdOxIJin6DB1FXJpD1'}, 'id': '51DevdOxIJin6DB1FXJpD1', 'name': 'UZI', 'type': 'artist', 'uri': 'spotify:artist:51DevdOxIJin6DB1FXJpD1'}], 'disc_number': 1, 'duration_ms': 192694, 'explicit': False, 'external_urls': {'spotify': 'https://open.spotify.com/track/4PUniKS3Cywu23xjdtoji5'}, 'id': '4PUniKS3Cywu23xjdtoji5', 'is_playable': True, 'name': 'Outro', 'preview_url': 'https://p.scdn.co/mp3-preview/daa66ec952bed822128228eb026861b970dc4b8f?cid=d8a5ed958d274c2e8ee717e6a4b0971d', 'track_number': 10, 'type': 'track', 'uri': 'spotify:track:4PUniKS3Cywu23xjdtoji5', 'is_local': False}]}, 'copyrights': [{'text': '2021 M.O.B Entertainment Associated Label Of Govinet', 'type': 'C'}, {'text': '2021 M.O.B Entertainment Associated Label Of Govinet', 'type': 'P'}], 'external_ids': {'upc': '3616553578384'}, 'genres': [], 'label': 'M.O.B. Entertainment', 'popularity': 69}, None]}
",0,1,13,41834
"I need an overview of available content categories, including Music, Podcasts, and Audiobooks, each represented with a title, artwork, and URI details. Additionally, I want to retrieve a specific album's tracklist with metadata such as track names, play counts, and duration for up to 300 tracks, starting from the first track.","import requests

API_KEY = 'e308f6a1f4msh1a3643540014badp1b9f6cjsn93a28ffe97fa'

def get_explore_content():
    url = ""https://spotify23.p.rapidapi.com/browse_all/""
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""spotify23.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers)
    response.raise_for_status()  # Raises an error for HTTP issues
    return response.json()

def get_album_tracks(album_id, offset=0, limit=300):
    url = ""https://spotify23.p.rapidapi.com/album_tracks/""
    querystring = {""id"": album_id, ""offset"": str(offset), ""limit"": str(limit)}
    headers = {
        ""x-rapidapi-key"": API_KEY,
        ""x-rapidapi-host"": ""spotify23.p.rapidapi.com""
    }
    response = requests.get(url, headers=headers, params=querystring)
    response.raise_for_status()  # Raises an error for HTTP issues
    return response.json()

if __name__ == '__main__':
    # Fetch explore content details
    explore_content_data = get_explore_content()

    # Extract a limited number of Explore Categories
    explore_data = [
        {
            ""title"": item.get('content', {}).get('data', {}).get('data', {}).get('cardRepresentation', {}).get('title', {}).get('transformedLabel', ""No Title""),
            ""artwork"": item.get('content', {}).get('data', {}).get('data', {}).get('cardRepresentation', {}).get('artwork', {}).get('sources', [{}])[0].get('url', ""No Artwork""),
            ""uri"": item.get('uri', ""No URI"")
        }
        for item in explore_content_data.get('data', {}).get('browseStart', {}).get('sections', {}).get('items', [])[0].get('sectionItems', {}).get('items', [])
    ][:3]  # Limiting to first 3 categories for display

    print(""Explore Categories:"")
    for category in explore_data:
        print(f""Title: {category['title']}"")
        print(f""Artwork URL: {category['artwork']}"")
        print(f""URI: {category['uri']}\n"")

    # Example Album ID to fetch tracks (replace 'your_album_id' with an actual album ID)
    album_tracks_data = get_album_tracks(""your_album_id"")

    # Improved check for structure of album_tracks_data
    album_data = album_tracks_data.get('data', {}).get('album', {})
    if album_data and 'tracks' in album_data and 'items' in album_data['tracks']:
        # Extract a limited number of Album Track Details
        album_tracks = [
            {
                ""track_name"": track.get('track', {}).get('name', ""No Track Name""),
                ""play_count"": track.get('track', {}).get('playcount', ""No Play Count""),
                ""duration_ms"": track.get('track', {}).get('duration', {}).get('totalMilliseconds', ""No Duration""),
                ""artists"": [artist.get('profile', {}).get('name', ""Unknown Artist"") for artist in track.get('track', {}).get('artists', {}).get('items', [])]
            }
            for track in album_data['tracks'].get('items', [])
        ][:3]  # Limiting to first 3 tracks for display

        print(""\nAlbum Tracks:"")
        for track in album_tracks:
            print(f""Track Name: {track['track_name']}"")
            print(f""Play Count: {track['play_count']}"")
            print(f""Duration (ms): {track['duration_ms']}"")
            print(f""Artists: {', '.join(track['artists'])}\n"")
    else:
        print(""Error: Album track data is missing or not in the expected format."")
","Explore Categories:
Title: Music
Artwork URL: https://i.scdn.co/image/ab67fb8200005caf474a477debc822a3a45c5acb
URI: spotify:page:0JQ5DAqbMKFSi39LMRT0Cy

Title: Podcasts
Artwork URL: https://i.scdn.co/image/ab6765630000ba8a81f07e1ead0317ee3c285bfa
URI: spotify:page:0JQ5DArNBzkmxXHCqFLx2J

Title: Audiobooks
Artwork URL: https://i.scdn.co/image/ab67fb8200005cafa6152f62518b4c3251858b21
URI: spotify:page:0JQ5DAqbMKFETqK4t8f1n3

Error: Album track data is missing or not in the expected format.
",1,1,9,32229
